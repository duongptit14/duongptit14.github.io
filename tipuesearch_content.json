{"pages":[{"title":"About Chris Albon","text":"I am a data scientist and machine learning engineer formally trained as a quantitative political scientist. Currently I am Chief Data Scientist at the Kenyan startup BRCK . Previously, I founded New Knowledge and the data science podcast, Partially Derivative . Prior to New Knowledge, I led Ushahidi's work on crisis and humanitarian data and launched CrisisNET . I was also the Director of the low-resource technology Governance Project at FrontlineSMS . I earned a Ph.D. in Political Science from the University of California, Davis researching the quantitative impact of civil wars on health care systems. I earned a B.A. from the University of Miami, where I triple majored in political science, international studies, and religious studies. Email: cralbon@gmail.com Twitter: @chrisalbon Curriculum Vitae Education Ph.D., Political Science , University of California, Davis. 2012 Dissertation: \"Civil Wars And Health Systems\", a quantitative analysis of the determinants of rebel and government behavior towards health system destruction and reconstruction using original data Fields: International Relations, Quantitative Methodology, and Epidemiology M.A., Political Science , University of California, Davis. 2010 Thesis: \"U.N. Peace Operations And Public Health After Civil War\" B.A. , University of Miami, Miami, FL. 2006 Triple majored in political science, international studies, and religious studies Work Experience Chief Data Scientist , BRCK , 2017 - Present Creator , Machine Learning Flashcards , 2017 - Present Author , Python Machine Learning Cookbook, O'Reilly Media , Forthcoming Co-founder & Co-host , Partially Derivative , 2014 - 2017 Co-founded a podcast on data and data science. Co-founder & Chief Science Officer , New Knowledge , 2015 - 2016 In charge of everything data science and product. Director of CrisisNET , Ushahidi , 2014 - 2015 Launched a pipeline for global humanitarian crisis data. Director of Data Projects , Ushahidi , 2013 - 2014 The non-profit's first data science hire; led all data science efforts. Project Director , FrontlineSMS , 2012 - 2013 Led FrontlineSMS's Governance Project, an effort improve the transparency and accountability of governments through low resource mobile technology Other Experience Moderator , /r/DataScience, Reddit , 2015 - Present Volunteer Data Scientist , DataKind , 2015 Contributor , Daily Dot , 2012 - 2013 Write opinion pieces on the politics of data and the internet Contributor , United Nations Dispatch , 2011 - 2013 Write news, opinion, and analysis on global affairs, particularly relating to health during conflict, global health politics, and the role of social media Blogger , Conflict Health , 2008 - 2012 Designed and launched blog on defending health and health workers against persecution, violence, and armed conflict Wrote 485 posts over four years Cited by major publications including The Atlantic, Harpers, Wired, The Economist, Time, The Guardian, and The American Prospect Contributor , United States Naval Institute Blog , 2009 - 2011 Wrote posts on the U.S. Navy's role in disaster relief, humanitarian assistance, and health diplomacy for one of America's most prestigious professional military associations Research Assistant , U.C. Davis Department Of Political Science , 2008 - 2009 Researched the effect of U.S. defense policy on military suicides Founder , Serve Your World 2002 - 2006 An information site on overseas volunteering","tags":"pages","url":"http://chrisalbon.com/pages/about.html","loc":"http://chrisalbon.com/pages/about.html"},{"title":"Adding Dropout","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np . random . seed ( 0 ) Load IMDB Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Construct Neural Network Architecture With Dropout Layer In Keras, we can implement dropout by added Dropout layers into our network architecture. Each Dropout layer will drop a user-defined hyperparameter of units in the previous layer every batch. Remember in Keras the input layer is assumed to be the first layer and not added using the add . Therefore, if we want to add dropout to the input layer, the layer we add in our is a dropout layer. This layer contains both the proportion of the input layer's units to drop 0.2 and input_shape defining the shape of the observation data. Next, after we add a dropout layer with 0.5 after each of the hidden layers. # Start neural network network = models . Sequential () # Add a dropout layer for input layer network . add ( layers . Dropout ( 0.2 , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add a dropout layer for previous hidden layer network . add ( layers . Dropout ( 0.5 )) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add a dropout layer for previous hidden layer network . add ( layers . Dropout ( 0.5 )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 3 , # Number of epochs verbose = 0 , # No output batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/adding_dropout.html","loc":"http://chrisalbon.com/deep-learning-keras/adding_dropout.html"},{"title":"Convolutional Neural Network","text":"Preliminaries import numpy as np from keras.datasets import mnist from keras.models import Sequential from keras.layers import Dense , Dropout , Flatten from keras.layers.convolutional import Conv2D , MaxPooling2D from keras.utils import np_utils from keras import backend as K # Set that the color channel value will be first K . set_image_data_format ( 'channels_first' ) # Set seed np . random . seed ( 0 ) Load MNIST Image Data # Set image information channels = 1 height = 28 width = 28 # Load data and target from MNIST data ( train_data , train_target ), ( test_data , test_target ) = mnist . load_data () # Reshape training image data into features train_data = train_data . reshape ( train_data . shape [ 0 ], channels , height , width ) # Reshape test image data into features test_data = test_data . reshape ( test_data . shape [ 0 ], channels , height , width ) # Rescale pixel intensity to between 0 and 1 train_features = train_data / 255 test_features = test_data / 255 # One-hot encode target train_target = np_utils . to_categorical ( train_target ) test_target = np_utils . to_categorical ( test_target ) number_of_classes = test_target . shape [ 1 ] Create Convolutional Neural Network Architecture Convolutional neural networks (also called ConvNets) are a popular type of network that has proven very effective at computer vision (e.g. recognizing cats, dogs, planes, and even hot dogs). It is completely possible to use feedforward neural networks on images, where each pixel is a feature. However, when doing so we run into two major problems. First, a feedforward neural networks do not take into account the spatial structure of the pixels. For example, in a 10x10 pixel image we might convert it into a vector of 100 pixel features, and in this case feedforward would consider the first feature (e.g. pixel value) to have the same relationship with the 10th feature as the 11th feature. However, in reality the 10th feature represents a pixel on the far side of the image as the first feature, while the 11th feature represents the pixel immediately below the first pixel. Second, and relatedly, feedforward neural networks learn global relationships in the features instead of local patterns. In more practical terms, this means that feedforward neural networks are not able to detect an object regardless of where it appears in an image. For example, imagine we are training a neural network to recognize faces, these faces might appear anywhere in the image from the upper right to the middle to the lower left. The power of convolutional neural networks is their ability handle both of these issues (and others). # Start neural network network = Sequential () # Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function network . add ( Conv2D ( filters = 64 , kernel_size = ( 5 , 5 ), input_shape = ( channels , width , height ), activation = 'relu' )) # Add max pooling layer with a 2x2 window network . add ( MaxPooling2D ( pool_size = ( 2 , 2 ))) # Add dropout layer network . add ( Dropout ( 0.5 )) # Add layer to flatten input network . add ( Flatten ()) # # Add fully connected layer of 128 units with a ReLU activation function network . add ( Dense ( 128 , activation = 'relu' )) # Add dropout layer network . add ( Dropout ( 0.5 )) # Add fully connected layer with a softmax activation function network . add ( Dense ( number_of_classes , activation = 'softmax' )) Compile Convolutional Neural Network # Compile neural network network . compile ( loss = 'categorical_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Convolutional Neural Network # Train neural network network . fit ( train_features , # Features train_target , # Target epochs = 2 , # Number of epochs verbose = 0 , # Don't print description after each epoch batch_size = 1000 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation <keras.callbacks.History at 0x129039748>","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/convolutional_neural_network.html","loc":"http://chrisalbon.com/deep-learning-keras/convolutional_neural_network.html"},{"title":"Feedforward Neural Network For Binary Classification","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Construct Neural Network Architecture Because this is a binary classification problem, one common choice is to use the sigmoid activation function in a one-unit output layer. # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Feedforward Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Feedforward Neural Network In Keras, we train our neural network using the fit method. There are six significant parameters to define. The first two parameters are the features and target vector of the training data. The epochs parameter defines how many epochs to use when training the data. verbose determines how much information is outputted during the training process, with 0 being no out, 1 outputting a progress bar, and 2 one log line per epoch. batch_size sets the number of observations to propagate through the network before updating the parameters. Finally, we held out a test set of data to use to evaluate the model. These test features and test target vector can be arguments of the validation_data , which will use them for evaluation. Alternatively, we could have used validation_split to define what fraction of the training data we want to hold out for evaluation. In scikit-learn fit method returned a trained model, however in Keras the fit method returns a History object containing the loss values and performance metrics at each epoch. # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 3 , # Number of epochs verbose = 1 , # Print description after each epoch batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation Train on 25000 samples, validate on 25000 samples Epoch 1/3 25000/25000 [==============================] - 1s - loss: 0.4215 - acc: 0.8107 - val_loss: 0.3388 - val_acc: 0.8556 Epoch 2/3 25000/25000 [==============================] - 1s - loss: 0.3240 - acc: 0.8646 - val_loss: 0.3256 - val_acc: 0.8630 Epoch 3/3 25000/25000 [==============================] - 1s - loss: 0.3117 - acc: 0.8702 - val_loss: 0.3264 - val_acc: 0.8601","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_binary_classification.html","loc":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_binary_classification.html"},{"title":"Feedforward Neural Network For Multiclass Classification","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import reuters from keras.utils.np_utils import to_categorical from keras.preprocessing.text import Tokenizer from keras import models from keras import layers # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Data # Set the number of features we want number_of_features = 5000 # Load feature and target data ( train_data , train_target_vector ), ( test_data , test_target_vector ) = reuters . load_data ( num_words = number_of_features ) # Convert feature data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) # One-hot encode target vector to create a target matrix train_target = to_categorical ( train_target_vector ) test_target = to_categorical ( test_target_vector ) Construct Neural Network Architecture In this example we use a loss function suited to multi-class classification, the categorical cross-entropy loss function, categorical_crossentropy . # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 100 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 100 , activation = 'relu' )) # Add fully connected layer with a softmax activation function network . add ( layers . Dense ( units = 46 , activation = 'softmax' )) Compile Feedforward Neural Network # Compile neural network network . compile ( loss = 'categorical_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Feedforward Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 3 , # Three epochs verbose = 0 , # No output batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data to use for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_multiclass_classification.html","loc":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_multiclass_classification.html"},{"title":"Feedforward Neural Networks For Regression","text":"Preliminaries # Load libraries import numpy as np from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn import preprocessing # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Generate Training Data # Generate features matrix and target vector features , target = make_regression ( n_samples = 10000 , n_features = 3 , n_informative = 3 , n_targets = 1 , noise = 0.0 , random_state = 0 ) # Divide our data into training and test sets train_features , test_features , train_target , test_target = train_test_split ( features , target , test_size = 0.33 , random_state = 0 ) Create Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 32 , activation = 'relu' , input_shape = ( train_features . shape [ 1 ],))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 32 , activation = 'relu' )) # Add fully connected layer with no activation function network . add ( layers . Dense ( units = 1 )) Compile Neural Network Because we are training a regression, we should use an appropriate loss function and evaluation metric, in our case the mean square error: $$\\operatorname {MSE}={\\frac {1}{n}}\\sum {{i=1}}&#94;{n}({\\hat {y }}-y_{i})&#94;{2}$$ where $n$ is the number of observations, $y_{i}$ is the true value of the target we are trying to predict, $y$, for observation $i$, and ${\\hat {y_{i}}}$ is the model's predicted value for $y_{i}$. # Compile neural network network . compile ( loss = 'mse' , # Mean squared error optimizer = 'RMSprop' , # Optimization algorithm metrics = [ 'mse' ]) # Mean squared error Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 10 , # Number of epochs verbose = 0 , # No output batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_regression.html","loc":"http://chrisalbon.com/deep-learning-keras/feedforward_neural_network_for_regression.html"},{"title":"k-Fold Cross-Validating Neural Networks","text":"If we have smaller data it can be useful to benefit from k-fold cross-validation to maximize our ability to evaluate the neural network's performance. This is possible in Keras because we can \"wrap\" any neural network such that it can use the evaluation features available in scikit-learn, including k-fold cross-validation. To accomplish this, we first have to create a function that returns a compiled neural network. Next we use KerasClassifier (if we have a classifier, if we have a regressor we can use KerasRegressor ) to wrap the model so it can be used by scikit-learn. After this, we can use our neural network like any other scikit-learn learning algorithm (e.g. random forests, logistic regression). In our solution, we used cross_val_score to run a 3-fold cross-validation on our neural network. Preliminaries # Load libraries import numpy as np from keras import models from keras import layers from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import cross_val_score from sklearn.datasets import make_classification # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Create Feature And Target Data # Number of features number_of_features = 100 # Generate features matrix and target vector features , target = make_classification ( n_samples = 10000 , n_features = number_of_features , n_informative = 3 , n_redundant = 0 , n_classes = 2 , weights = [ . 5 , . 5 ], random_state = 0 ) Create Function That Constructs Neural Network # Create function returning a compiled network def create_network (): # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric # Return compiled network return network Wrap Function In KerasClassifier # Wrap Keras model so it can be used by scikit-learn neural_network = KerasClassifier ( build_fn = create_network , epochs = 10 , batch_size = 100 , verbose = 0 ) Conduct k-Fold Cross-Validation Using scikit-learn # Evaluate neural network using three-fold cross-validation cross_val_score ( neural_network , features , target , cv = 3 ) array([ 0.90491901, 0.7770777 , 0.87068707])","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/k-fold_cross-validating_neural_networks.html","loc":"http://chrisalbon.com/deep-learning-keras/k-fold_cross-validating_neural_networks.html"},{"title":"LSTM Recurrent Neural Network","text":"Oftentimes we have text data that we want to classify. While it is possible to use a type of convolutional network, we are going to focus on a more popular option: the recurrent neural network. The key feature of recurrent neural networks is that information loops back in the network. This gives recurrent neural networks a type of memory it can use to better understand sequential data. A popular choice type of recurrent neural network is the long short-term memory (LSTM) network which allows for information to loop backwards in the network. Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing import sequence from keras import models from keras import layers # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Dataset On Movie Review Text # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Use padding or truncation to make each observation have 400 features train_features = sequence . pad_sequences ( train_data , maxlen = 400 ) test_features = sequence . pad_sequences ( test_data , maxlen = 400 ) View First Observation's Raw Data # View first observation print ( train_data [ 0 ]) [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32] View First Observation's Feature Data # View first observation test_features [ 0 ] array([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 89, 27, 2, 2, 17, 199, 132, 5, 2, 16, 2, 24, 8, 760, 4, 2, 7, 4, 22, 2, 2, 16, 2, 17, 2, 7, 2, 2, 9, 4, 2, 8, 14, 991, 13, 877, 38, 19, 27, 239, 13, 100, 235, 61, 483, 2, 4, 7, 4, 20, 131, 2, 72, 8, 14, 251, 27, 2, 7, 308, 16, 735, 2, 17, 29, 144, 28, 77, 2, 18, 12], dtype=int32) Create LSTM Neural Network Architecture # Start neural network network = models . Sequential () # Add an embedding layer network . add ( layers . Embedding ( input_dim = number_of_features , output_dim = 128 )) # Add a long short-term memory layer with 128 units network . add ( layers . LSTM ( units = 128 )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compule LSTM Neural Network Architecture # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'Adam' , # Adam optimization metrics = [ 'accuracy' ]) # Accuracy performance metric Train LSTM Neural Network Architecture # Train neural network history = network . fit ( train_features , # Features train_target , # Target epochs = 3 , # Number of epochs verbose = 0 , # Do not print description after each epoch batch_size = 1000 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/lstm_recurrent_neural_network.html","loc":"http://chrisalbon.com/deep-learning-keras/lstm_recurrent_neural_network.html"},{"title":"Neural Network Early Stopping","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras.callbacks import EarlyStopping , ModelCheckpoint # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Text Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Create Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Setup Early Stopping In Keras, we can implement early stopping as a callback function. Callbacks are functions that can be applied at certain stages of the training process, such as at the end of each epoch. Specifically, in our solution, we included EarlyStopping(monitor='val_loss', patience=2) to define that we wanted to monitor the test (validation) loss at each epoch and after the test loss has not improved after two epochs, training is interrupted. However, since we set patience=2 , we won't get the best model, but the model two epochs after the best model. Therefore, optionally, we can include a second operation, ModelCheckpoint which saves the model to a file after every checkpoint (which can be useful in case a multi-day training session is interrupted for some reason. Helpful for us, if we set save_best_only=True then ModelCheckpoint will only save the best model. # Set callback functions to early stop training and save the best model so far callbacks = [ EarlyStopping ( monitor = 'val_loss' , patience = 2 ), ModelCheckpoint ( filepath = 'best_model.h5' , monitor = 'val_loss' , save_best_only = True )] Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 20 , # Number of epochs callbacks = callbacks , # Early stopping verbose = 0 , # Print description after each epoch batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/neural_network_early_stopping.html","loc":"http://chrisalbon.com/deep-learning-keras/neural_network_early_stopping.html"},{"title":"Neural Network Weight Regularization","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras import regularizers # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Text Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Create Neural Network Architecture With Weight Regularization In Keras, we can add a weight regularization by including using including kernel_regularizer=regularizers.l2(0.01) a later. In this example, 0.01 determines how much we penalize higher parameter values. # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function and L2 regularization network . add ( layers . Dense ( units = 16 , activation = 'relu' , kernel_regularizer = regularizers . l2 ( 0.01 ), input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function and L2 regularization network . add ( layers . Dense ( units = 16 , kernel_regularizer = regularizers . l2 ( 0.01 ), activation = 'relu' )) Compile Neural Network # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 3 , # Number of epochs verbose = 0 , # No output batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/neural_network_weight_regularization.html","loc":"http://chrisalbon.com/deep-learning-keras/neural_network_weight_regularization.html"},{"title":"Preprocessing Data For Neural Networks","text":"Typically, a neural network's parameters are initialized (i.e. created) as small random numbers. Neural networks often behave poorly when the feature values much larger than parameter values. Furthermore, since an observation's feature values will are combined as they pass through individual units, it is important that all features have the same scale. For these reasons, it is best practice (although not always necessary, for example when we have all binary features) to standardize each feature such that the feature's values have the mean of 0 and the standard deviation of 1. This can be easily accomplished using scikit-learn's StandardScaler . Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature Data # Create feature features = np . array ([[ - 100.1 , 3240.1 ], [ - 200.2 , - 234.1 ], [ 5000.5 , 150.1 ], [ 6000.6 , - 125.1 ], [ 9000.9 , - 673.1 ]]) Standardize Feature Data # Create scaler scaler = preprocessing . StandardScaler () # Transform the feature features_standardized = scaler . fit_transform ( features ) Show Standardized Features # Show feature features_standardized array([[-1.12541308, 1.96429418], [-1.15329466, -0.50068741], [ 0.29529406, -0.22809346], [ 0.57385917, -0.42335076], [ 1.40955451, -0.81216255]]) Show Standardized Features Summary Statistics # Print mean and standard deviation print ( 'Mean:' , round ( features_standardized [:, 0 ] . mean ())) print ( 'Standard deviation:' , features_standardized [:, 0 ] . std ()) Mean : 0.0 Standard deviation : 1.0","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/preprocessing_data_for_neural_networks.html","loc":"http://chrisalbon.com/deep-learning-keras/preprocessing_data_for_neural_networks.html"},{"title":"Save Model Training Progress","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers from keras.callbacks import ModelCheckpoint # Set random seed np . random . seed ( 0 ) Load IMDB Movie Review Data # Set the number of features we want number_of_features = 1000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Create Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Save Training Progress After Each Epoch After every epoch ModelCheckpoint saves a model to the location specified by the filepath parameter. If we include only a filename (e.g. models.hdf5 ) that file will be overridden with the latest model every epoch. If we only wanted to save the best model according to the performance of some loss function, we can set save_best_only=True and monitor='val_loss' to not override a file if the model has a worse test loss than the previous model. Alternatively, we can save every epoch's model as its own file by including the epoch number and test loss score into the filename itself. For example if we set filepath to model_{epoch:02d}_{val_loss:.2f}.hdf5 , the name of the file containing the model saved after the 11th epoch with a test loss value of 0.33 would be model_10_0.35.hdf5 (notice that the epoch number if 0-indexed). # Set callback functions to early stop training and save the best model so far checkpoint = [ ModelCheckpoint ( filepath = 'models.hdf5' )] Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target vector epochs = 3 , # Number of epochs callbacks = checkpoint , # Checkpoint verbose = 0 , # No output batch_size = 100 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/save_model_training_progress.html","loc":"http://chrisalbon.com/deep-learning-keras/save_model_training_progress.html"},{"title":"Tuning Neural Network Hyperparameters","text":"Preliminaries # Load libraries import numpy as np from keras import models from keras import layers from keras.wrappers.scikit_learn import KerasClassifier from sklearn.model_selection import GridSearchCV from sklearn.datasets import make_classification # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Generate Target And Feature Data # Number of features number_of_features = 100 # Generate features matrix and target vector features , target = make_classification ( n_samples = 10000 , n_features = number_of_features , n_informative = 3 , n_redundant = 0 , n_classes = 2 , weights = [ . 5 , . 5 ], random_state = 0 ) Create Function That Constructs A Neural Network # Create function returning a compiled network def create_network ( optimizer = 'rmsprop' ): # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = optimizer , # Optimizer metrics = [ 'accuracy' ]) # Accuracy performance metric # Return compiled network return network Wrap Function In KerasClassifier # Wrap Keras model so it can be used by scikit-learn neural_network = KerasClassifier ( build_fn = create_network , verbose = 0 ) Create Hyperparameter Search Space # Create hyperparameter space epochs = [ 5 , 10 ] batches = [ 5 , 10 , 100 ] optimizers = [ 'rmsprop' , 'adam' ] # Create hyperparameter options hyperparameters = dict ( optimizer = optimizers , epochs = epochs , batch_size = batches ) Conduct Grid Search # Create grid search grid = GridSearchCV ( estimator = neural_network , param_grid = hyperparameters ) # Fit grid search grid_result = grid . fit ( features , target ) Find Best Model's Hyperparameters # View hyperparameters of best neural network grid_result . best_params_ {'batch_size': 10, 'epochs': 5, 'optimizer': 'adam'}","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/tuning_neural_network_hyperparameters.html","loc":"http://chrisalbon.com/deep-learning-keras/tuning_neural_network_hyperparameters.html"},{"title":"Visual Neural Network Architecutre","text":"Preliminaries # Load libraries from keras import models from keras import layers from IPython.display import SVG from keras.utils.vis_utils import model_to_dot from keras.utils import plot_model Using TensorFlow backend. Construct Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( 10 ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Visualize Network Architecture # Visualize network architecture SVG ( model_to_dot ( network , show_shapes = True ) . create ( prog = 'dot' , format = 'svg' )) Save To File # Save the visualization as a file plot_model ( network , show_shapes = True , to_file = 'network.png' )","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/visual_neural_network_architecture.html","loc":"http://chrisalbon.com/deep-learning-keras/visual_neural_network_architecture.html"},{"title":"Visualize Loss History","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers import matplotlib.pyplot as plt # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Data # Set the number of features we want number_of_features = 10000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Create Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target epochs = 15 , # Number of epochs verbose = 0 , # No output batch_size = 1000 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation Visualize Neural Network Loss History # Get training and test loss histories training_loss = history . history [ 'loss' ] test_loss = history . history [ 'val_loss' ] # Create count of the number of epochs epoch_count = range ( 1 , len ( training_loss ) + 1 ) # Visualize loss history plt . plot ( epoch_count , training_loss , 'r--' ) plt . plot ( epoch_count , test_loss , 'b-' ) plt . legend ([ 'Training Loss' , 'Test Loss' ]) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . show ();","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/visualize_loss_history.html","loc":"http://chrisalbon.com/deep-learning-keras/visualize_loss_history.html"},{"title":"Visualize Performance History","text":"Preliminaries # Load libraries import numpy as np from keras.datasets import imdb from keras.preprocessing.text import Tokenizer from keras import models from keras import layers import matplotlib.pyplot as plt # Set random seed np . random . seed ( 0 ) Using TensorFlow backend. Load Movie Review Data # Set the number of features we want number_of_features = 10000 # Load data and target vector from movie review data ( train_data , train_target ), ( test_data , test_target ) = imdb . load_data ( num_words = number_of_features ) # Convert movie review data to a one-hot encoded feature matrix tokenizer = Tokenizer ( num_words = number_of_features ) train_features = tokenizer . sequences_to_matrix ( train_data , mode = 'binary' ) test_features = tokenizer . sequences_to_matrix ( test_data , mode = 'binary' ) Create Neural Network Architecture # Start neural network network = models . Sequential () # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' , input_shape = ( number_of_features ,))) # Add fully connected layer with a ReLU activation function network . add ( layers . Dense ( units = 16 , activation = 'relu' )) # Add fully connected layer with a sigmoid activation function network . add ( layers . Dense ( units = 1 , activation = 'sigmoid' )) Compile Neural Network # Compile neural network network . compile ( loss = 'binary_crossentropy' , # Cross-entropy optimizer = 'rmsprop' , # Root Mean Square Propagation metrics = [ 'accuracy' ]) # Accuracy performance metric Train Neural Network # Train neural network history = network . fit ( train_features , # Features train_target , # Target epochs = 15 , # Number of epochs verbose = 0 , # No output batch_size = 1000 , # Number of observations per batch validation_data = ( test_features , test_target )) # Data for evaluation Visualize Neural Network Performance History Specifically, we visualize the neural network's accuracy score on training and test sets over each epoch. # Get training and test accuracy histories training_accuracy = history . history [ 'acc' ] test_accuracy = history . history [ 'val_acc' ] # Create count of the number of epochs epoch_count = range ( 1 , len ( training_accuracy ) + 1 ) # Visualize accuracy history plt . plot ( epoch_count , training_accuracy , 'r--' ) plt . plot ( epoch_count , test_accuracy , 'b-' ) plt . legend ([ 'Training Accuracy' , 'Test Accuracy' ]) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Accuracy Score' ) plt . show ();","tags":"Deep Learning - Keras","url":"http://chrisalbon.com/deep-learning-keras/visualize_performance_history.html","loc":"http://chrisalbon.com/deep-learning-keras/visualize_performance_history.html"},{"title":"Agglomerative Clustering","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import AgglomerativeClustering Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Conduct Agglomerative Clustering In scikit-learn, AgglomerativeClustering uses the linkage parameter to determine the merging strategy to minimize the 1) variance of merged clusters ( ward ), 2) average of distance between observations from pairs of clusters ( average ), or 3) maximum distance between observations from pairs of clusters ( complete ). Two other parameters are useful to know. First, the affinity parameter determines the distance metric used for linkage ( minkowski , euclidean , etc.). Second, n_clusters sets the number of clusters the clustering algorithm will attempt to find. That is, clusters are successively merged until there are only n_clusters remaining. # Create meanshift object clt = AgglomerativeClustering ( linkage = 'complete' , affinity = 'euclidean' , n_clusters = 3 ) # Train model model = clt . fit ( X_std ) Show Cluster Membership # Show cluster membership model . labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/agglomerative_clustering.html","loc":"http://chrisalbon.com/machine-learning/agglomerative_clustering.html"},{"title":"Bernoulli Naive Bayes Classifier","text":"The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded). Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import BernoulliNB Create Binary Feature And Target Data # Create three binary features X = np . random . randint ( 2 , size = ( 100 , 3 )) # Create a binary target vector y = np . random . randint ( 2 , size = ( 100 , 1 )) . ravel () View Feature Data # View first ten observations X [ 0 : 10 ] array([[1, 1, 1], [0, 1, 0], [1, 1, 1], [0, 0, 0], [1, 0, 1], [1, 1, 1], [0, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 0]]) Train Bernoulli Naive Bayes Classifier # Create Bernoulli Naive Bayes object with prior probabilities of each class clf = BernoulliNB ( class_prior = [ 0.25 , 0.5 ]) # Train model model = clf . fit ( X , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/bernoulli_naive_bayes_classifier.html","loc":"http://chrisalbon.com/machine-learning/bernoulli_naive_bayes_classifier.html"},{"title":"Calibrate Predicted Probabilities","text":"Class probabilities are a common and useful part of machine learning models. In scikit-learn, most learning algortihms allow us to see the predicted probabilities of class membership using predict_proba . This can be extremely useful if, for instance, we want to only predict a certain class if the model predicts the probability that they are that class is over 90%. However, some models, including naive Bayes classifiers output probabilities that are not based on the real world. That is, predict_proba might predict an observation has a 0.70 chance of being a certain class, when the reality is that it is 0.10 or 0.99. Specifically in naive Bayes, while the ranking of predicted probabilities for the different target classes is valid, the raw predicted probabilities tend to take on extreme values close to 0 and 1. To obtain meaningful predicted probabilities we need conduct what is called calibration. In scikit-learn we can use the CalibratedClassifierCV class to create well calibrated predicted probabilities using k-fold cross-validation. In CalibratedClassifierCV the training sets are used to train the model and the test sets is used to calibrate the predicted probabilities. The returned predicted probabilities are the average of the k-folds. Preliminaries # Load libraries from sklearn import datasets from sklearn.naive_bayes import GaussianNB from sklearn.calibration import CalibratedClassifierCV Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Naive Bayes Classifier # Create Gaussian Naive Bayes object clf = GaussianNB () Create Calibrator # Create calibrated cross-validation with sigmoid calibration clf_sigmoid = CalibratedClassifierCV ( clf , cv = 2 , method = 'sigmoid' ) Create Classifier With Calibrated Probabilities # Calibrate probabilities clf_sigmoid . fit ( X , y ) CalibratedClassifierCV(base_estimator=GaussianNB(priors=None), cv=2, method='sigmoid') Create Previously Unseen Observation # Create new observation new_observation = [[ 2.6 , 2.6 , 2.6 , 0.4 ]] View Calibrated Probabilities # View calibrated probabilities clf_sigmoid . predict_proba ( new_observation ) array([[ 0.31859969, 0.63663466, 0.04476565]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calibrate_predicted_probabilities.html","loc":"http://chrisalbon.com/machine-learning/calibrate_predicted_probabilities.html"},{"title":"Calibrate Predicted Probabilities In SVC","text":"SVC's use of a hyperplane to create decision regions do not naturally output a probability estimate that an observation is a member of a certain class. However, we can in fact output calibrated class probabilities with a few caveats. In an SVC, Platt scaling can be used, wherein first the SVC is trained, then a separate cross-validated logistic regression is trained to map the SVC outputs into probabilities: $$P(y=1 \\mid x)={\\frac {1}{1+e&#94;{(A*f(x)+B)}}}$$ where $A$ and $B$ are parameter vectors and $f$ is the $i$th observation's signed distance from the hyperplane. When we have more than two classes, an extension of Platt scaling is used. In scikit-learn, the predicted probabilities must be generated when the model is being trained. This can be done by setting SVC 's probability to True . After the model is trained, we can output the estimated probabilities for each class using predict_proba . Preliminaries # Load libraries from sklearn.svm import SVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Support Vector Classifier # Create support vector classifier object svc = SVC ( kernel = 'linear' , probability = True , random_state = 0 ) # Train classifier model = svc . fit ( X_std , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ . 4 , . 4 , . 4 , . 4 ]] View Predicted Probabilities # View predicted probabilities model . predict_proba ( new_observation ) array([[ 0.00588822, 0.96874828, 0.0253635 ]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calibrate_predicted_probabilities_in_svc.html","loc":"http://chrisalbon.com/machine-learning/calibrate_predicted_probabilities_in_svc.html"},{"title":"DBSCAN Clustering","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import DBSCAN Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Conduct Meanshift Clustering DBSCAN has three main parameters to set: eps : The maximum distance from an observation for another observation to be considered its neighbor. min_samples : The minimum number of observation less than eps distance from an observation for to be considered a core observation. metric : The distance metric used by eps . For example, minkowski , euclidean , etc. (note that if Minkowski distance is used, the parameter p can be used to set the power of the Minkowski metric) If we look at the clusters in our training data we can see two clusters have been identified, 0 and 1 , while outlier observations are labeled -1 . # Create meanshift object clt = DBSCAN ( n_jobs =- 1 ) # Train model model = clt . fit ( X_std )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/dbscan_clustering.html","loc":"http://chrisalbon.com/machine-learning/dbscan_clustering.html"},{"title":"Find Support Vectors","text":"Preliminaries # Load libraries from sklearn.svm import SVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Dataset #Load data with only two classes iris = datasets . load_iris () X = iris . data [: 100 ,:] y = iris . target [: 100 ] Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Support Vector Classifier # Create support vector classifier object svc = SVC ( kernel = 'linear' , random_state = 0 ) # Train classifier model = svc . fit ( X_std , y ) View Support Vectors # View support vectors model . support_vectors_ array([[-0.5810659 , 0.43490123, -0.80621461, -0.50581312], [-1.52079513, -1.67626978, -1.08374115, -0.8607697 ], [-0.89430898, -1.46515268, 0.30389157, 0.38157832], [-0.5810659 , -1.25403558, 0.09574666, 0.55905661]]) View Indices Of Support Vectors # View indices of support vectors model . support_ array([23, 41, 57, 98], dtype=int32) View Number Of Support Vectors With Each Class # View number of support vectors for each class model . n_support_ array([2, 2], dtype=int32)","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/find_support_vectors.html","loc":"http://chrisalbon.com/machine-learning/find_support_vectors.html"},{"title":"Gaussian Naive Bayes Classifier","text":"Because of the assumption of the normal distribution, Gaussian Naive Bayes is best used in cases when all our features are continuous. Preliminaries # Load libraries from sklearn import datasets from sklearn.naive_bayes import GaussianNB Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Train Gaussian Naive Bayes Classifier # Create Gaussian Naive Bayes object with prior probabilities of each class clf = GaussianNB ( priors = [ 0.25 , 0.25 , 0.5 ]) # Train model model = clf . fit ( X , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ 4 , 4 , 4 , 0.4 ]] Predict Class # Predict class model . predict ( new_observation ) array([1]) Note: the raw predicted probabilities from Gaussian naive Bayes (outputted using predict_proba ) are not calibrated. That is, they should not be believed. If we want to create useful predicted probabilities we will need to calibrate them using an isotonic regression or a related method.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/gaussian_naive_bayes_classifier.html","loc":"http://chrisalbon.com/machine-learning/gaussian_naive_bayes_classifier.html"},{"title":"Imbalanced Classes In SVM","text":"In support vector machines, $C$ is a hyperparameter determining the penalty for misclassifying an observation. One method for handling imbalanced classes in support vector machines is to weight $C$ by classes, so that $$C_k = C * w_j$$ where $C$ is the penalty for misclassification, $w_j$ is a weight inversely proportional to class $j$'s frequency, and $C_j$ is the $C$ value for class $j$. The general idea is to increase the penalty for misclassifying minority classes to prevent them from being \"overwhelmed\" by the majority class. In scikit-learn, when using SVC we can set the values for $C_j$ automatically by setting class_weight='balanced' The balanced argument automatically weighs classes such that: $$w_j = \\frac{n}{kn_{j}}$$ where $w_j$ is the weight to class $j$, $n$ is the number of observations, $n_j$ is the number of observations in class $j$, and $k$ is the total number of classes. Preliminaries # Load libraries from sklearn.svm import SVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Dataset #Load data with only two classes iris = datasets . load_iris () X = iris . data [: 100 ,:] y = iris . target [: 100 ] Imbalanced Iris Flower Classes # Make class highly imbalanced by removing first 40 observations X = X [ 40 :,:] y = y [ 40 :] # Create target vector indicating if class 0, otherwise 1 y = np . where (( y == 0 ), 0 , 1 ) Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Support Vector Classifier With Weighted Classes # Create support vector classifier svc = SVC ( kernel = 'linear' , class_weight = 'balanced' , C = 1.0 , random_state = 0 ) # Train classifier model = svc . fit ( X_std , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/imbalanced_classes_in_svm.html","loc":"http://chrisalbon.com/machine-learning/imbalanced_classes_in_svm.html"},{"title":"k-Means Clustering","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Conduct k-Means Clustering # Create k-mean object clt = KMeans ( n_clusters = 3 , random_state = 0 , n_jobs =- 1 ) # Train model model = clt . fit ( X_std ) Show Each Observation's Cluster Membership # View predict class model . labels_ array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32) Create New Observation # Create new observation new_observation = [[ 0.8 , 0.8 , 0.8 , 0.8 ]] Predict Observation's Cluster # Predict observation's cluster model . predict ( new_observation ) array([0], dtype=int32) View Centers Of Each Cluster # View cluster centers model . cluster_centers_ array([[ 1.13597027, 0.09659843, 0.996271 , 1.01717187], [-1.01457897, 0.84230679, -1.30487835, -1.25512862], [-0.05021989, -0.88029181, 0.34753171, 0.28206327]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/k-means_clustering.html","loc":"http://chrisalbon.com/machine-learning/k-means_clustering.html"},{"title":"Meanshift Clustering","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import MeanShift Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Conduct Meanshift Clustering MeanShift has two important parameters we should be aware of. First, bandwidth sets radius of the area (i.e. kernel) an observation uses to determine the direction to shift. In our analogy, bandwidth was how far a person could see through the fog. We can set this parameter manually, however by default a reasonable bandwidth is estimated automatically (with a significant increase in computational cost). Second, sometimes in meanshift there are no other observations within an observation's kernel. That is, a person on our football cannot see a single other person. By default, MeanShift assigns all these \"orphan\" observations to the kernel of the nearest observation. However, if we want to leave out these orphans, we can set cluster_all=False wherein orphan observations the label of -1 . # Create meanshift object clt = MeanShift ( n_jobs =- 1 ) # Train model model = clt . fit ( X_std )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/meanshift_clustering.html","loc":"http://chrisalbon.com/machine-learning/meanshift_clustering.html"},{"title":"Mini-Batch k-Means Clustering","text":"Mini-batch k-means works similarly to the k-means algorithm discussed in the last recipe. Without going into too much detail, the difference is that in mini-batch k-means the most computationally costly step is conducted on only a random sample of observations as opposed to all observations. This approach can significantly reduce the time required for the algorithm to find convergence (i.e. fit the data) with only a small cost in quality. Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.cluster import MiniBatchKMeans Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Conduct k-Means Clustering MiniBatchKMeans works similarly to KMeans , with one significance difference: the batch_size parameter. batch_size controls the number of randomly selected observations in each batch. The larger the the size of the batch, the more computationally costly the training process. # Create k-mean object clustering = MiniBatchKMeans ( n_clusters = 3 , random_state = 0 , batch_size = 100 ) # Train model model = clustering . fit ( X_std )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/minibatch_k-means_clustering.html","loc":"http://chrisalbon.com/machine-learning/minibatch_k-means_clustering.html"},{"title":"Multinomial Naive Bayes Classifier","text":"Multinomial naive Bayes works similar to Gaussian naive Bayes, however the features are assumed to be multinomially distributed. In practice, this means that this classifier is commonly used when we have discrete data (e.g. movie ratings ranging 1 and 5). Preliminaries # Load libraries import numpy as np from sklearn.naive_bayes import MultinomialNB from sklearn.feature_extraction.text import CountVectorizer Create Text Data # Create text text_data = np . array ([ 'I love Brazil. Brazil!' , 'Brazil is best' , 'Germany beats both' ]) Create Bag Of Words # Create bag of words count = CountVectorizer () bag_of_words = count . fit_transform ( text_data ) # Create feature matrix X = bag_of_words . toarray () Create Target Vector # Create target vector y = np . array ([ 0 , 0 , 1 ]) Train Multinomial Naive Bayes Classifier # Create multinomial naive Bayes object with prior probabilities of each class clf = MultinomialNB ( class_prior = [ 0.25 , 0.5 ]) # Train model model = clf . fit ( X , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ 0 , 0 , 0 , 1 , 0 , 1 , 0 ]] Predict Observation's Class # Predict new observation's class model . predict ( new_observation ) array([0])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/multinomial_naive_bayes_classifier.html","loc":"http://chrisalbon.com/machine-learning/multinomial_naive_bayes_classifier.html"},{"title":"Plot The Support Vector Classifier's Hyperplane","text":"Preliminaries # Load libraries from sklearn.svm import LinearSVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np from matplotlib import pyplot as plt Load Iris Flower Data # Load data with only two classes and two features iris = datasets . load_iris () X = iris . data [: 100 ,: 2 ] y = iris . target [: 100 ] Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Support Vector Classifier # Create support vector classifier svc = LinearSVC ( C = 1.0 ) # Train model model = svc . fit ( X_std , y ) Plot Decision Boundary Hyperplane In this visualization, all observations of class 0 are black and observations of class 1 are light gray. The hyperplane is the decision-boundary deciding how new observations are classified. Specifically, any observation above the line will by classified as class 0 while any observation below the line will be classified as class 1. # Plot data points and color using their class color = [ 'black' if c == 0 else 'lightgrey' for c in y ] plt . scatter ( X_std [:, 0 ], X_std [:, 1 ], c = color ) # Create the hyperplane w = svc . coef_ [ 0 ] a = - w [ 0 ] / w [ 1 ] xx = np . linspace ( - 2.5 , 2.5 ) yy = a * xx - ( svc . intercept_ [ 0 ]) / w [ 1 ] # Plot the hyperplane plt . plot ( xx , yy ) plt . axis ( \"off\" ), plt . show ();","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/plot_support_vector_classifier_hyperplane.html","loc":"http://chrisalbon.com/machine-learning/plot_support_vector_classifier_hyperplane.html"},{"title":"Support Vector Classifier","text":"There is a balance between SVC maximizing the margin of the hyperplane and minimizing the misclassification. In SVC, the later is controlled with the hyperparameter $C$, the penalty imposed on errors. C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias but low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias but high variance). In scikit-learn, $C$ is determined by the parameter C and defaults to C=1.0 . We should treat $C$ has a hyperparameter of our learning algorithm which we tune using model selection techniques. Preliminaries # Load libraries from sklearn.svm import LinearSVC from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Data # Load feature and target data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Support Vector Classifier # Create support vector classifier svc = LinearSVC ( C = 1.0 ) # Train model model = svc . fit ( X_std , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ - 0.7 , 1.1 , - 1.1 , - 1.7 ]] Predict Class Of Observation # Predict class of new observation svc . predict ( new_observation ) array([0])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/support_vector_classifier.html","loc":"http://chrisalbon.com/machine-learning/support_vector_classifier.html"},{"title":"Feature Importance","text":"Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets import numpy as np import matplotlib.pyplot as plt Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Train A Decision Tree Model # Create decision tree classifer object clf = RandomForestClassifier ( random_state = 0 , n_jobs =- 1 ) # Train model model = clf . fit ( X , y ) View Feature Importance # Calculate feature importances importances = model . feature_importances_ Visualize Feature Importance # Sort feature importances in descending order indices = np . argsort ( importances )[:: - 1 ] # Rearrange feature names so they match the sorted feature importances names = [ iris . feature_names [ i ] for i in indices ] # Create plot plt . figure () # Create plot title plt . title ( \"Feature Importance\" ) # Add bars plt . bar ( range ( X . shape [ 1 ]), importances [ indices ]) # Add feature names as x-axis labels plt . xticks ( range ( X . shape [ 1 ]), names , rotation = 90 ) # Show plot plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/feature_importance.html","loc":"http://chrisalbon.com/machine-learning/feature_importance.html"},{"title":"Handle Imbalanced Classes In Random Forest","text":"Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier import numpy as np from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Adjust Iris Dataset To Make Classes Imbalanced # Make class highly imbalanced by removing first 40 observations X = X [ 40 :,:] y = y [ 40 :] # Create target vector indicating if class 0, otherwise 1 y = np . where (( y == 0 ), 0 , 1 ) Train Random Forest While Balancing Classes When using RandomForestClassifier a useful setting is class_weight=balanced wherein classes are automatically weighted inversely proportional to how frequently they appear in the data. Specifically: $$w_j = \\frac{n}{kn_{j}}$$ where $w_j$ is the weight to class $j$, $n$ is the number of observations, $n_j$ is the number of observations in class $j$, and $k$ is the total number of classes. # Create decision tree classifer object clf = RandomForestClassifier ( random_state = 0 , n_jobs =- 1 , class_weight = \"balanced\" ) # Train model model = clf . fit ( X , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handle_imbalanced_classes_in_random_forests.html","loc":"http://chrisalbon.com/machine-learning/handle_imbalanced_classes_in_random_forests.html"},{"title":"Handling Imbalanced Classes In Logistic Regression","text":"Like many other learning algorithms in scikit-learn, LogisticRegression comes with a built-in method of handling imbalanced classes. If we have highly imbalanced classes and have no addressed it during preprocessing, we have the option of using the class_weight parameter to weight the classes to make certain we have a balanced mix of each class. Specifically, the balanced argument will automatically weigh classes inversely proportional to their frequency: $$w_j = \\frac{n}{kn_{j}}$$ where $w_j$ is the weight to class $j$, $n$ is the number of observations, $n_j$ is the number of observations in class $j$, and $k$ is the total number of classes. Preliminaries # Load libraries from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Make Classes Imbalanced # Make class highly imbalanced by removing first 40 observations X = X [ 40 :,:] y = y [ 40 :] # Create target vector indicating if class 0, otherwise 1 y = np . where (( y == 0 ), 0 , 1 ) Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train A Logistic Regression With Weighted Classes # Create decision tree classifer object clf = LogisticRegression ( random_state = 0 , class_weight = 'balanced' ) # Train model model = clf . fit ( X_std , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_in_logistic_regression.html","loc":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_in_logistic_regression.html"},{"title":"Logistic Regression","text":"Despite having \"regression\" in its name, a logistic regression is actually a widely used binary classifier (i.e. the target vector can only take two values). In a logistic regression, a linear model (e.g. $\\beta_{0}+\\beta_{1}x$) is included in a logistic (also called sigmoid) function, ${\\frac {1}{1+e&#94;{-z}}}$, such that: $$P(y_i=1 \\mid X)={\\frac {1}{1+e&#94;{-(\\beta_{0}+\\beta_{1}x)}}}$$ where $P(y_i=1 \\mid X)$ is the probability of the $i$th observation's target value, $y_i$, being class 1, $X$ is the training data, $\\beta_0$ and $\\beta_1$ are the parameters to be learned, and $e$ is Euler's number. Preliminaries # Load libraries from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler Load Iris Flower Dataset # Load data with only two classes iris = datasets . load_iris () X = iris . data [: 100 ,:] y = iris . target [: 100 ] Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Create Logistic Regression # Create logistic regression object clf = LogisticRegression ( random_state = 0 ) Train Logistic Regression # Train model model = clf . fit ( X_std , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ . 5 , . 5 , . 5 , . 5 ]] Predict Class Of Observation # Predict class model . predict ( new_observation ) array([1]) View Predicted Probabilities # View predicted probabilities model . predict_proba ( new_observation ) array([[ 0.18823041, 0.81176959]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/logistic_regression.html","loc":"http://chrisalbon.com/machine-learning/logistic_regression.html"},{"title":"Logistic Regression On Very Large Data","text":"scikit-learn's LogisticRegression offers a number of techniques for training a logistic regression, called solvers. Most of the time scikit-learn will select the best solver automatically for us or warn us that you cannot do some thing with that solver. However, there is one particular case we should be aware of. While an exact explanation is beyond the bounds of this book, stochastic average gradient descent allows us to train a model much faster than other solvers when our data is very large. However, it is also very sensitive to feature scaling to standardizing our features is particularly important. We can set our learning algorithm to use this solver by setting solver='sag' . Preliminaries # Load libraries from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Train Logistic Regression Using SAG solver # Create logistic regression object using sag solver clf = LogisticRegression ( random_state = 0 , solver = 'sag' ) # Train model model = clf . fit ( X_std , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/logistic_regression_on_very_large_data.html","loc":"http://chrisalbon.com/machine-learning/logistic_regression_on_very_large_data.html"},{"title":"Multinomial Logistic Regression","text":"In multinomial logistic regression (MLR) the logistic function we saw in Recipe 15.1 is replaced with a softmax function: $$P(y_i=k \\mid X)={\\frac {e&#94;{\\beta_{k}x_{i}}}{{\\sum_{j=1}&#94;{K}}e&#94;{\\beta_{j}x_{i}}}}$$ where $P(y_i=k \\mid X)$ is the probability the $i$th observation's target value, $y_i$, is class $k$, and $K$ is the total number of classes. One practical advantage of the MLR is that its predicted probabilities using the predict_proba method are more reliable (i.e. better calibrated). Preliminaries # Load libraries from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Create Multinomial Logistic Regression # Create one-vs-rest logistic regression object clf = LogisticRegression ( random_state = 0 , multi_class = 'multinomial' , solver = 'newton-cg' ) Train Multinomial Logistic Regression # Train model model = clf . fit ( X_std , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ . 5 , . 5 , . 5 , . 5 ]] Predict Observation's Class # Predict class model . predict ( new_observation ) array([1]) View Probability Observation Is Each Class # View predicted probabilities model . predict_proba ( new_observation ) array([[ 0.01944996, 0.74469584, 0.2358542 ]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/multinomial_logistic_regression.html","loc":"http://chrisalbon.com/machine-learning/multinomial_logistic_regression.html"},{"title":"One Vs. Rest Logistic Regression","text":"On their own, logistic regressions are only binary classifiers, meaning they cannot handle target vectors with more than two classes. However, there are clever extensions to logistic regression to do just that. In one-vs-rest logistic regression (OVR) a separate model is trained for each class predicted whether an observation is that class or not (thus making it a binary classification problem). It assumes that each classification problem (e.g. class 0 or not) is independent. Preliminaries # Load libraries from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.preprocessing import StandardScaler Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Create One-Vs-Rest Logistic Regression # Create one-vs-rest logistic regression object clf = LogisticRegression ( random_state = 0 , multi_class = 'ovr' ) Train One-Vs-Rest Logistic Regression # Train model model = clf . fit ( X_std , y ) Create Previously Unseen Observation # Create new observation new_observation = [[ . 5 , . 5 , . 5 , . 5 ]] Predict Observation's Class # Predict class model . predict ( new_observation ) array([2]) View Probability Observation Is Each Class # View predicted probabilities model . predict_proba ( new_observation ) array([[ 0.0829087 , 0.29697265, 0.62011865]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/one-vs-rest_logistic_regression.html","loc":"http://chrisalbon.com/machine-learning/one-vs-rest_logistic_regression.html"},{"title":"Radius-Based Nearest Neighbor Classifier","text":"Preliminaries # Load libraries from sklearn.neighbors import RadiusNeighborsClassifier from sklearn.preprocessing import StandardScaler from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Features # Create standardizer standardizer = StandardScaler () # Standardize features X_std = standardizer . fit_transform ( X ) Fit A Radius-Based Nearest Neighbor Classifier In scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers. # Train a radius neighbors classifier rnn = RadiusNeighborsClassifier ( radius =. 5 , n_jobs =- 1 ) . fit ( X_std , y ) Predict New Observation # Create two observations new_observations = [[ 1 , 1 , 1 , 1 ]] # Predict the class of two observations rnn . predict ( new_observations ) array([2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/radius_based_nearest_neighbor_classifier.html","loc":"http://chrisalbon.com/machine-learning/radius_based_nearest_neighbor_classifier.html"},{"title":"Random Forest Classifier","text":"Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets Load Iris Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Random Forest Classifier # Create random forest classifer object that uses entropy clf = RandomForestClassifier ( criterion = 'entropy' , random_state = 0 , n_jobs =- 1 ) Train Random Forest Classifier # Train model model = clf . fit ( X , y ) Predict Previously Unseen Observation # Make new observation observation = [[ 5 , 4 , 3 , 2 ]] # Predict observation's class model . predict ( observation ) array([1])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/random_forest_classifier.html","loc":"http://chrisalbon.com/machine-learning/random_forest_classifier.html"},{"title":"Random Forest Regression","text":"Preliminaries # Load libraries from sklearn.ensemble import RandomForestRegressor from sklearn import datasets Load Boston Housing Data # Load data with only two features boston = datasets . load_boston () X = boston . data [:, 0 : 2 ] y = boston . target Create Random Forest Regressor # Create decision tree classifer object regr = RandomForestRegressor ( random_state = 0 , n_jobs =- 1 ) Train Random Forest Regressor # Train model model = regr . fit ( X , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/random_forest_regression.html","loc":"http://chrisalbon.com/machine-learning/random_forest_regression.html"},{"title":"Select Important Features In Random Forest","text":"Preliminaries # Load libraries from sklearn.ensemble import RandomForestClassifier from sklearn import datasets from sklearn.feature_selection import SelectFromModel Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Random Forest Classifier # Create random forest classifier clf = RandomForestClassifier ( random_state = 0 , n_jobs =- 1 ) Select Features With Importance Greater Than Threshold The higher the number, the more important the feature (all importance scores sum to one). By plotting these values we can add interpretability to our random forest models. # Create object that selects features with importance greater than or equal to a threshold selector = SelectFromModel ( clf , threshold = 0.3 ) # Feature new feature matrix using selector X_important = selector . fit_transform ( X , y ) View Selected Important Features # View first five observations of the features X_important [ 0 : 5 ] array([[ 1.4, 0.2], [ 1.4, 0.2], [ 1.3, 0.2], [ 1.5, 0.2], [ 1.4, 0.2]]) Train Model With Selected Important Features # Train random forest using most important featres model = clf . fit ( X_important , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/select_important_features_in_random_forest.html","loc":"http://chrisalbon.com/machine-learning/select_important_features_in_random_forest.html"},{"title":"Decision Tree Classifier","text":"Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Decision Tree Using Gini Impurity # Create decision tree classifer object using gini clf = DecisionTreeClassifier ( criterion = 'gini' , random_state = 0 ) Train Model # Train model model = clf . fit ( X , y ) Create Observation To Predict # Make new observation observation = [[ 5 , 4 , 3 , 2 ]] Predict Observation # Predict observation's class model . predict ( observation ) array([1]) View Predicted Probabilities # View predicted class probabilities for the three classes model . predict_proba ( observation ) array([[ 0., 1., 0.]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/decision_tree_classifier.html","loc":"http://chrisalbon.com/machine-learning/decision_tree_classifier.html"},{"title":"Decision Tree regression","text":"Preliminaries # Load libraries from sklearn.tree import DecisionTreeRegressor from sklearn import datasets Load Boston Housing Dataset # Load data with only two features boston = datasets . load_boston () X = boston . data [:, 0 : 2 ] y = boston . target Create Decision Tree Decision tree regression works similar to decision tree classification, however instead of reducing Gini impurity or entropy, potential splits are measured on how much they reduce the mean squared error (MSE): $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}&#94;{n} (y_i - \\hat{y}_i)&#94;2$$ where $y_i$ is the true value of the target and $\\hat{y}_i$ is the predicted value. # Create decision tree classifer object regr = DecisionTreeRegressor ( random_state = 0 ) Train Model # Train model model = regr . fit ( X , y ) Create Observation To Predict # Make new observation observation = [[ 0.02 , 16 ]] # Predict observation's value model . predict ( observation ) array([ 33.])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/decision_tree_regression.html","loc":"http://chrisalbon.com/machine-learning/decision_tree_regression.html"},{"title":"Find Nearest Neighbors","text":"Preliminaries # Load libraries from sklearn.neighbors import NearestNeighbors from sklearn import datasets from sklearn.preprocessing import StandardScaler import numpy as np Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Iris Data It is important to standardize our data before we calculate any distances. # Create standardizer standardizer = StandardScaler () # Standardize features X_std = standardizer . fit_transform ( X ) Find Each Observation's Two Nearest Neighbors # Find three nearest neighbors based on euclidean distance (including itself) nn_euclidean = NearestNeighbors ( n_neighbors = 3 , metric = 'euclidean' ) . fit ( X ) # List of lists indicating each observation's 3 nearest neighors nearest_neighbors_with_self = nn_euclidean . kneighbors_graph ( X ) . toarray () # Remove 1's marking an observation is nearest to itself for i , x in enumerate ( nearest_neighbors_with_self ): x [ i ] = 0 Show nearest neighbors # View first observation's two nearest neighbors nearest_neighbors_with_self [ 0 ] array([ 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/find_nearest_neighbors.html","loc":"http://chrisalbon.com/machine-learning/find_nearest_neighbors.html"},{"title":"Identifying Best Value Of k","text":"Preliminaries # Load libraries from sklearn.neighbors import KNeighborsClassifier from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline , FeatureUnion from sklearn.model_selection import GridSearchCV Load Iris Flower Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Standardize Data # Create standardizer standardizer = StandardScaler () # Standardize features X_std = standardizer . fit_transform ( X ) Fit A k-Nearest Neighbor Classifier # Fit a KNN classifier with 5 neighbors knn = KNeighborsClassifier ( n_neighbors = 5 , metric = 'euclidean' , n_jobs =- 1 ) . fit ( X_std , y ) Create Search Space Of Possible Values Of k # Create a pipeline pipe = Pipeline ([( 'standardizer' , standardizer ), ( 'knn' , knn )]) # Create space of candidate values search_space = [{ 'knn__n_neighbors' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]}] Search Over Possible Values of k # Create grid search clf = GridSearchCV ( pipe , search_space , cv = 5 , verbose = 0 ) . fit ( X_std , y ) View k For Best Performing Model # Best neighborhood size (k) clf . best_estimator_ . get_params ()[ 'knn__n_neighbors' ] 6","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/identifying_best_value_of_k.html","loc":"http://chrisalbon.com/machine-learning/identifying_best_value_of_k.html"},{"title":"Visualize A Decision Tree","text":"Preliminaries # Load libraries from sklearn.tree import DecisionTreeClassifier from sklearn import datasets from IPython.display import Image from sklearn import tree import pydotplus Load Iris Data # Load data iris = datasets . load_iris () X = iris . data y = iris . target Train Decision Tree # Create decision tree classifer object clf = DecisionTreeClassifier ( random_state = 0 ) # Train model model = clf . fit ( X , y ) Visualize Decision Tree # Create DOT data dot_data = tree . export_graphviz ( clf , out_file = None , feature_names = iris . feature_names , class_names = iris . target_names ) # Draw graph graph = pydotplus . graph_from_dot_data ( dot_data ) # Show graph Image ( graph . create_png ()) Save Decision Tree Image To File # Create PDF graph . write_pdf ( \"iris.pdf\" ) # Create PNG graph . write_png ( \"iris.png\" ) True","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/visualize_a_decision_tree.html","loc":"http://chrisalbon.com/machine-learning/visualize_a_decision_tree.html"},{"title":"Adaboost Classifier","text":"Preliminaries # Load libraries from sklearn.ensemble import AdaBoostClassifier from sklearn import datasets Load Iris Flower Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Adaboost Classifier The most important parameters are base_estimator , n_estimators , and learning_rate . base_estimator is the learning algorithm to use to train the weak models. This will almost always not needed to be changed because by far the most common learner to use with AdaBoost is a decision tree -- this parameter's default argument. n_estimators is the number of models to iteratively train. learning_rate is the contribution of each model to the weights and defaults to 1 . Reducing the learning rate will mean the weights will be increased or decreased to a small degree, forcing the model train slower (but sometimes resulting in better performance scores). loss is exclusive to AdaBoostRegressor and sets the loss function to use when updating weights. This defaults to a linear loss function however can be changed to square or exponential . # Create adaboost-decision tree classifer object clf = AdaBoostClassifier ( n_estimators = 50 , learning_rate = 1 , random_state = 0 ) Train Adaboost Classifer # Train model model = clf . fit ( X , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/adaboost_classifier.html","loc":"http://chrisalbon.com/machine-learning/adaboost_classifier.html"},{"title":"Adding Interaction Terms","text":"Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston from sklearn.preprocessing import PolynomialFeatures import warnings # Suppress Warning warnings . filterwarnings ( action = \"ignore\" , module = \"scipy\" , message = \"&#94;internal gelsd\" ) Load Boston Housing Dataset # Load the data with only two features boston = load_boston () X = boston . data [:, 0 : 2 ] y = boston . target Add Interaction Term Interaction effects can be account for by including a new feature comprising the product of corresponding values from the interacting features: $$\\hat y = \\hat\\beta_{0} + \\hat\\beta_{1}x_{1}+ \\hat\\beta_{2}x_{2} + \\hat\\beta_{3}x_{1}x_{2} + \\epsilon$$ where $x_{1}$ and $ x_{2}$ are the values of the two features, respectively and $x_{1}x_{2}$ represents the interaction between the two. It can be useful to use scikit-learn's PolynomialFeatures to creative interaction terms for all combination of features. We can then use model selection strategies to identify the combination of features and interaction terms which produce the best model. # Create interaction term (not polynomial features) interaction = PolynomialFeatures ( degree = 3 , include_bias = False , interaction_only = True ) X_inter = interaction . fit_transform ( X ) Fit Linear Regression # Create linear regression regr = LinearRegression () # Fit the linear regression model = regr . fit ( X_inter , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/adding_interaction_terms.html","loc":"http://chrisalbon.com/machine-learning/adding_interaction_terms.html"},{"title":"Fast C Hyperparameter Tuning","text":"Sometimes the characteristics of a learning algorithm allows us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods. scikit-learn's LogisticRegressionCV method includes a parameter Cs . If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied a integer, Cs a list of that many candidate values will is drawn from a logarithmic scale between 0.0001 and and 10000 (a range of reasonable values for C). Preliminaries # Load libraries from sklearn import linear_model , datasets Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Use Cross-Validation To Find The Best Value Of C # Create cross-validated logistic regression clf = linear_model . LogisticRegressionCV ( Cs = 100 ) # Train model clf . fit ( X , y ) LogisticRegressionCV(Cs=100, class_weight=None, cv=None, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/fast_c_hyperparameter_tuning.html","loc":"http://chrisalbon.com/machine-learning/fast_c_hyperparameter_tuning.html"},{"title":"Find Best Preprocessing Steps During Model Selection","text":"We have to be careful to properly handle preprocessing when conducting model selection. First, GridSearchCV uses cross-validation to determine which model has the highest performance. However, in cross-validation we are in effect pretending that the fold held out as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g. scaling or standardization). For this reason, we cannot preprocess the data then run GridSearchCV . Second, some preprocessing methods have their own parameter which often have to be supplied by the user. By including candidate component values in the search space, they are treated like any other hyperparameter be to searched over. Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline , FeatureUnion from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # Set random seed np . random . seed ( 0 ) Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Proprocessing Object We are include two different preprocessing steps: principal component analysis and a k-best feature selection. # Create a combined preprocessing object preprocess = FeatureUnion ([( 'pca' , PCA ()), ( \"kbest\" , SelectKBest ( k = 1 ))]) Create Pipeline # Create a pipeline pipe = Pipeline ([( 'preprocess' , preprocess ), ( 'classifier' , LogisticRegression ())]) Create Search Space Of Hyperparameter Values # Create space of candidate values search_space = [{ 'preprocess__pca__n_components' : [ 1 , 2 , 3 ], 'classifier__penalty' : [ 'l1' , 'l2' ], 'classifier__C' : np . logspace ( 0 , 4 , 10 )}] Create Grid Search # Create grid search clf = GridSearchCV ( pipe , search_space , cv = 5 , verbose = 0 , n_jobs =- 1 ) Conduct Grid Search # Fit grid search best_model = clf . fit ( X , y ) View Best Model's Hyperparamters # View best hyperparameters print ( 'Best Number Of Princpal Components:' , best_model . best_estimator_ . get_params ()[ 'preprocess__pca__n_components' ]) print ( 'Best Penalty:' , best_model . best_estimator_ . get_params ()[ 'classifier__penalty' ]) print ( 'Best C:' , best_model . best_estimator_ . get_params ()[ 'classifier__C' ]) Best Number Of Princpal Components: 3 Best Penalty: l1 Best C: 59.9484250319","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/find_best_preprocessing_steps_during_model_selection.html","loc":"http://chrisalbon.com/machine-learning/find_best_preprocessing_steps_during_model_selection.html"},{"title":"Hyperparameter Tuning Using Grid Search","text":"Preliminaries # Load libraries import numpy as np from sklearn import linear_model , datasets from sklearn.model_selection import GridSearchCV Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Logistic Regression # Create logistic regression logistic = linear_model . LogisticRegression () Create Hyperparameter Search Space # Create regularization penalty space penalty = [ 'l1' , 'l2' ] # Create regularization hyperparameter space C = np . logspace ( 0 , 4 , 10 ) # Create hyperparameter options hyperparameters = dict ( C = C , penalty = penalty ) Create Grid Search # Create grid search using 5-fold cross validation clf = GridSearchCV ( logistic , hyperparameters , cv = 5 , verbose = 0 ) Conduct Grid Search # Fit grid search best_model = clf . fit ( X , y ) View Hyperparameter Values Of Best Model # View best hyperparameters print ( 'Best Penalty:' , best_model . best_estimator_ . get_params ()[ 'penalty' ]) print ( 'Best C:' , best_model . best_estimator_ . get_params ()[ 'C' ]) Best Penalty: l1 Best C: 7.74263682681 Predict Using Best Model # Predict target vector best_model . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/hyperparameter_tuning_using_grid_search.html","loc":"http://chrisalbon.com/machine-learning/hyperparameter_tuning_using_grid_search.html"},{"title":"Hyperparameter Tuning Using Random Search","text":"Preliminaries # Load libraries from scipy.stats import uniform from sklearn import linear_model , datasets from sklearn.model_selection import RandomizedSearchCV Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Logistic Regression # Create logistic regression logistic = linear_model . LogisticRegression () Create Hyperparameter Search Space # Create regularization penalty space penalty = [ 'l1' , 'l2' ] # Create regularization hyperparameter distribution using uniform distribution C = uniform ( loc = 0 , scale = 4 ) # Create hyperparameter options hyperparameters = dict ( C = C , penalty = penalty ) Create Random Search # Create randomized search 5-fold cross validation and 100 iterations clf = RandomizedSearchCV ( logistic , hyperparameters , random_state = 1 , n_iter = 100 , cv = 5 , verbose = 0 , n_jobs =- 1 ) Conduct Random Search # Fit randomized search best_model = clf . fit ( X , y ) View Hyperparameter Values Of Best Model # View best hyperparameters print ( 'Best Penalty:' , best_model . best_estimator_ . get_params ()[ 'penalty' ]) print ( 'Best C:' , best_model . best_estimator_ . get_params ()[ 'C' ]) Best Penalty: l1 Best C: 1.66808801881 Predict Using Best Model # Predict target vector best_model . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html","loc":"http://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html"},{"title":"Lasso Regression","text":"Preliminaries # Load library from sklearn.linear_model import Lasso from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston () X = boston . data y = boston . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Fit Ridge Regression The hyperparameter, $\\alpha$, lets us control how much we penalize the coefficients, with higher values of $\\alpha$ creating simpler modelers. The ideal value of $\\alpha$ should be tuned like any other hyperparameter. In scikit-learn, $\\alpha$ is set using the alpha parameter. # Create lasso regression with alpha value regr = Lasso ( alpha = 0.5 ) # Fit the linear regression model = regr . fit ( X_std , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/lasso_regression.html","loc":"http://chrisalbon.com/machine-learning/lasso_regression.html"},{"title":"Linear Regression Using Scikit-Learn","text":"Preliminaries # Load libraries from sklearn.linear_model import LinearRegression from sklearn.datasets import load_boston import warnings # Suppress Warning warnings . filterwarnings ( action = \"ignore\" , module = \"scipy\" , message = \"&#94;internal gelsd\" ) Load Boston Housing Dataset # Load data boston = load_boston () X = boston . data y = boston . target Fit A Linear Regression # Create linear regression regr = LinearRegression () # Fit the linear regression model = regr . fit ( X , y ) View Intercept Term # View the intercept model . intercept_ 36.491103280361038 View Coefficients # View the feature coefficients model . coef_ array([ -1.07170557e-01, 4.63952195e-02, 2.08602395e-02, 2.68856140e+00, -1.77957587e+01, 3.80475246e+00, 7.51061703e-04, -1.47575880e+00, 3.05655038e-01, -1.23293463e-02, -9.53463555e-01, 9.39251272e-03, -5.25466633e-01])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/linear_regression_using_scikit-learn.html","loc":"http://chrisalbon.com/machine-learning/linear_regression_using_scikit-learn.html"},{"title":"Model Selection Using Grid Search","text":"Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline # Set random seed np . random . seed ( 0 ) Load Iris Dataset # Load data iris = datasets . load_iris () X = iris . data y = iris . target Create Pipeline With Model Selection Search Space Notice that we include both multiple possible learning algorithms and multiple possible hyperparameter values to search over. # Create a pipeline pipe = Pipeline ([( 'classifier' , RandomForestClassifier ())]) # Create space of candidate learning algorithms and their hyperparameters search_space = [{ 'classifier' : [ LogisticRegression ()], 'classifier__penalty' : [ 'l1' , 'l2' ], 'classifier__C' : np . logspace ( 0 , 4 , 10 )}, { 'classifier' : [ RandomForestClassifier ()], 'classifier__n_estimators' : [ 10 , 100 , 1000 ], 'classifier__max_features' : [ 1 , 2 , 3 ]}] Create Model Selection Using Grid Search # Create grid search clf = GridSearchCV ( pipe , search_space , cv = 5 , verbose = 0 ) Conduct Model Selection Using Grid Search # Fit grid search best_model = clf . fit ( X , y ) View Best Model And Its Best Hyperparameters # View best model best_model . best_estimator_ . get_params ()[ 'classifier' ] LogisticRegression(C=7.7426368268112693, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l1', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) Predict Using Best Model # Predict target vector best_model . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/model_selection_tuning_using_grid_search.html","loc":"http://chrisalbon.com/machine-learning/model_selection_tuning_using_grid_search.html"},{"title":"Ridge Regression","text":"Preliminaries # Load libraries from sklearn.linear_model import Ridge from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston () X = boston . data y = boston . target Standardize Features # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Fit Ridge Regression The hyperparameter, $\\alpha$, lets us control how much we penalize the coefficients, with higher values of $\\alpha$ creating simpler modelers. The ideal value of $\\alpha$ should be tuned like any other hyperparameter. In scikit-learn, $\\alpha$ is set using the alpha parameter. # Create ridge regression with an alpha value regr = Ridge ( alpha = 0.5 ) # Fit the linear regression model = regr . fit ( X_std , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/ridge_regression.html","loc":"http://chrisalbon.com/machine-learning/ridge_regression.html"},{"title":"Selecting The Best Alpha Value In Ridge Regression","text":"Preliminaries # Load libraries from sklearn.linear_model import RidgeCV from sklearn.datasets import load_boston from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston () X = boston . data y = boston . target Standardize Features Note: Because in linear regression the value of the coefficients is partially determined by the scale of the feature, and in regularized models all coefficients are summed together, we must make sure to standardize the feature prior to training. # Standarize features scaler = StandardScaler () X_std = scaler . fit_transform ( X ) Create Ridge Regression With Candidate Alpha Values # Create ridge regression with three possible alpha values regr_cv = RidgeCV ( alphas = [ 0.1 , 1.0 , 10.0 ]) Fit Ridge Regression scikit-learn includes a RidgeCV method that allows us select the ideal value for $\\alpha$: # Fit the linear regression model_cv = regr_cv . fit ( X_std , y ) View Best Model's Alpha Value # View alpha model_cv . alpha_ 1.0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/selecting_best_alpha_value_in_ridge_regression.html","loc":"http://chrisalbon.com/machine-learning/selecting_best_alpha_value_in_ridge_regression.html"},{"title":"Accuracy","text":"Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X , y = make_classification ( n_samples = 10000 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , random_state = 1 ) Create Logistic Regression # Create logistic regression logit = LogisticRegression () Cross-Validate Model Using Accuracy # Cross-validate model using accuracy cross_val_score ( logit , X , y , scoring = \"accuracy\" ) array([ 0.95170966, 0.9580084 , 0.95558223])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/accuracy.html","loc":"http://chrisalbon.com/machine-learning/accuracy.html"},{"title":"Cross-Validation","text":"Preliminaries # Load libraries import numpy as np from sklearn import datasets from sklearn import metrics from sklearn.model_selection import KFold , cross_val_score from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler Load Digits Dataset # Load the digits dataset digits = datasets . load_digits () # Create the features matrix X = digits . data # Create the target vector y = digits . target Create Pipeline # Create standardizer standardizer = StandardScaler () # Create logistic regression logit = LogisticRegression () # Create a pipeline that standardizes, then runs logistic regression pipeline = make_pipeline ( standardizer , logit ) Create k-Fold Cross-Validation # Create k-Fold cross-validation kf = KFold ( n_splits = 10 , shuffle = True , random_state = 1 ) Conduct k-Fold Cross-Validation # Do k-fold cross-validation cv_results = cross_val_score ( pipeline , # Pipeline X , # Feature matrix y , # Target vector cv = kf , # Cross-validation technique scoring = \"accuracy\" , # Loss function n_jobs =- 1 ) # Use all CPU scores Calculate Mean Performance Score # Calculate mean cv_results . mean () 0.96493171942892597","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cross-validaton.html","loc":"http://chrisalbon.com/machine-learning/cross-validaton.html"},{"title":"F1 Score","text":"Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X , y = make_classification ( n_samples = 10000 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , random_state = 1 ) Create Logistic Regression # Create logistic regression logit = LogisticRegression () Cross-Validate Model Using F1 # Cross-validate model using precision cross_val_score ( logit , X , y , scoring = \"f1\" ) array([ 0.95166617, 0.95765275, 0.95558223])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/f1_score.html","loc":"http://chrisalbon.com/machine-learning/f1_score.html"},{"title":"Plot The Receiving Operating Characteristic Curve","text":"Preliminaries # Load libraries from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import roc_curve , roc_auc_score from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt Generate Features And Target # Create feature matrix and target vector X , y = make_classification ( n_samples = 10000 , n_features = 10 , n_classes = 2 , n_informative = 3 , random_state = 3 ) Split Data Intro Training And Test Sets # Split into training and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 1 ) Training Binary Classifier # Create classifier clf = LogisticRegression () # Train model clf . fit ( X_train , y_train ) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) Create Predicted Probabilities # Get predicted probabilities y_score = clf . predict_proba ( X_test )[:, 1 ] Plot Receiving Operating Characteristic Curve # Create true and false positive rates false_positive_rate , true_positive_rate , threshold = roc_curve ( y_test , y_score ) # Plot ROC curve plt . title ( 'Receiver Operating Characteristic' ) plt . plot ( false_positive_rate , true_positive_rate ) plt . plot ([ 0 , 1 ], ls = \"--\" ) plt . plot ([ 0 , 0 ], [ 1 , 0 ] , c = \".7\" ), plt . plot ([ 1 , 1 ] , c = \".7\" ) plt . ylabel ( 'True Positive Rate' ) plt . xlabel ( 'False Positive Rate' ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/plot_the_receiving_operating_characteristic_curve.html","loc":"http://chrisalbon.com/machine-learning/plot_the_receiving_operating_characteristic_curve.html"},{"title":"Precision","text":"Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X , y = make_classification ( n_samples = 10000 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , random_state = 1 ) Create Logistic Regression # Create logistic regression logit = LogisticRegression () Cross-Validate Model Using Precision # Cross-validate model using precision cross_val_score ( logit , X , y , scoring = \"precision\" ) array([ 0.95252404, 0.96583282, 0.95558223])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/precision.html","loc":"http://chrisalbon.com/machine-learning/precision.html"},{"title":"Recall","text":"Preliminaries # Load libraries from sklearn.model_selection import cross_val_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification Generate Features And Target Data # Generate features matrix and target vector X , y = make_classification ( n_samples = 10000 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , random_state = 1 ) Create Logistic Regression # Create logistic regression logit = LogisticRegression () Cross-Validate Model Using Recall # Cross-validate model using precision cross_val_score ( logit , X , y , scoring = \"recall\" ) array([ 0.95080984, 0.94961008, 0.95558223])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/recall.html","loc":"http://chrisalbon.com/machine-learning/recall.html"},{"title":"Split Data Into Training And Test Sets","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split Load Digits Dataset # Load the digits dataset digits = datasets . load_digits () # Create the features matrix X = digits . data # Create the target vector y = digits . target Split Into Training And Test Sets # Create training and test sets X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.1 , random_state = 1 ) Fit Standardizer To Training Set # Create standardizer standardizer = StandardScaler () # Fit standardizer to training set standardizer . fit ( X_train ) StandardScaler(copy=True, with_mean=True, with_std=True) Apply Standardizer To Training And Test Sets # Apply to both training and test sets X_train_std = standardizer . transform ( X_train ) X_test_std = standardizer . transform ( X_test )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/split_data_into_training_and_test_sets.html","loc":"http://chrisalbon.com/machine-learning/split_data_into_training_and_test_sets.html"},{"title":"ANOVA F-value For Feature Selection","text":"If the features are categorical, calculate a chi-square ($\\chi&#94;{2}$) statistic between each feature and the target vector. However, if the features are quantitative, compute the ANOVA F-value between each feature and the target vector. The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different. Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif Load Data # Load iris data iris = load_iris () # Create features and target X = iris . data y = iris . target Select Features With Best ANOVA F-Values # Create an SelectKBest object to select features with two best ANOVA F-Values fvalue_selector = SelectKBest ( f_classif , k = 2 ) # Apply the SelectKBest object to the features and target X_kbest = fvalue_selector . fit_transform ( X , y ) View Results # Show results print ( 'Original number of features:' , X . shape [ 1 ]) print ( 'Reduced number of features:' , X_kbest . shape [ 1 ]) Original number of features: 4 Reduced number of features: 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/anova_f-value_for_feature_selection.html","loc":"http://chrisalbon.com/machine-learning/anova_f-value_for_feature_selection.html"},{"title":"Chi-Squared For Feature Selection","text":"Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 Load Data # Load iris data iris = load_iris () # Create features and target X = iris . data y = iris . target # Convert to categorical data by converting data to integers X = X . astype ( int ) Compare Chi-Squared Statistics # Select two features with highest chi-squared statistics chi2_selector = SelectKBest ( chi2 , k = 2 ) X_kbest = chi2_selector . fit_transform ( X , y ) View Results # Show results print ( 'Original number of features:' , X . shape [ 1 ]) print ( 'Reduced number of features:' , X_kbest . shape [ 1 ]) Original number of features: 4 Reduced number of features: 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/chi-squared_for_feature_selection.html","loc":"http://chrisalbon.com/machine-learning/chi-squared_for_feature_selection.html"},{"title":"Create Baseline Classification Model","text":"Preliminaries # Load libraries from sklearn.datasets import load_iris from sklearn.dummy import DummyClassifier from sklearn.model_selection import train_test_split Load Iris Flower Dataset # Load data iris = load_iris () # Create target vector and feature matrix X , y = iris . data , iris . target Split Data Into Training And Test Set # Split into training and test set X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 0 ) Create Dummy Regression Always Predicts The Mean Value Of Target # Create dummy classifer dummy = DummyClassifier ( strategy = 'uniform' , random_state = 1 ) # \"Train\" model dummy . fit ( X_train , y_train ) DummyClassifier(constant=None, random_state=1, strategy='uniform') Evaluate Performance Metric # Get accuracy score dummy . score ( X_test , y_test ) 0.42105263157894735","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_baseline_classification_model.html","loc":"http://chrisalbon.com/machine-learning/create_baseline_classification_model.html"},{"title":"Create Baseline Regression Model","text":"Preliminaries # Load libraries from sklearn.datasets import load_boston from sklearn.dummy import DummyRegressor from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Load Boston Housing Dataset # Load data boston = load_boston () # Create features X , y = boston . data , boston . target Split Data Into Training And Test Set # Make test and training split X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 0 ) Create Dummy Regression Always Predicts The Mean Value Of Target # Create a dummy regressor dummy_mean = DummyRegressor ( strategy = 'mean' ) # \"Train\" dummy regressor dummy_mean . fit ( X_train , y_train ) DummyRegressor(constant=None, quantile=None, strategy='mean') Create Dummy Regression Always Predicts A Constant Value # Create a dummy regressor dummy_constant = DummyRegressor ( strategy = 'constant' , constant = 20 ) # \"Train\" dummy regressor dummy_constant . fit ( X_train , y_train ) DummyRegressor(constant=array(20), quantile=None, strategy='constant') Evaluate Performance Metric # Get R-squared score dummy_constant . score ( X_test , y_test ) -0.065105020293257265","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_baseline_regression_model.html","loc":"http://chrisalbon.com/machine-learning/create_baseline_regression_model.html"},{"title":"Custom Performance Metric","text":"Preliminaries # Load libraries from sklearn.metrics import make_scorer , r2_score from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.datasets import make_regression Create Feature # Generate features matrix and target vector X , y = make_regression ( n_samples = 100 , n_features = 3 , random_state = 1 ) # Create training set and test set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.10 , random_state = 1 ) Train model # Create ridge regression object classifier = Ridge () # Train ridge regression model model = classifier . fit ( X_train , y_train ) Create Custom Performance Metric For this example we are just calculating the r-squared score, but we can see that any calculation can be used. # Create custom metric def custom_metric ( y_test , y_pred ): # Calculate r-squared score r2 = r2_score ( y_test , y_pred ) # Return r-squared score return r2 Make Custom Metric A Scorer Object # Make scorer and define that higher scores are better score = make_scorer ( custom_metric , greater_is_better = True ) User Scorer To Evaluate Model Performance # Apply custom scorer to ridge regression score ( model , X_test , y_test ) 0.99979061028820582","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/custom_performance_metric.html","loc":"http://chrisalbon.com/machine-learning/custom_performance_metric.html"},{"title":"Drop Highly Correlated Features","text":"Preliminaries # Load libraries import pandas as pd import numpy as np Load Data # Create feature matrix with two highly correlated features X = np . array ([[ 1 , 1 , 1 ], [ 2 , 2 , 0 ], [ 3 , 3 , 1 ], [ 4 , 4 , 0 ], [ 5 , 5 , 1 ], [ 6 , 6 , 0 ], [ 7 , 7 , 1 ], [ 8 , 7 , 0 ], [ 9 , 7 , 1 ]]) # Convert feature matrix into DataFrame df = pd . DataFrame ( X ) # View the data frame df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 0 1 2 0 1 1 1 1 2 2 0 2 3 3 1 3 4 4 0 4 5 5 1 5 6 6 0 6 7 7 1 7 8 7 0 8 9 7 1 Identify Highly Correlated Features # Create correlation matrix corr_matrix = df . corr () . abs () # Select upper triangle of correlation matrix upper = corr_matrix . where ( np . triu ( np . ones ( corr_matrix . shape ), k = 1 ) . astype ( np . bool )) # Find index of feature columns with correlation greater than 0.95 to_drop = [ column for column in upper . columns if any ( upper [ column ] > 0.95 )] Drop Marked Features # Drop features df . drop ( df . columns [ to_drop ], axis = 1 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 0 2 0 1 1 1 2 0 2 3 1 3 4 0 4 5 1 5 6 0 6 7 1 7 8 0 8 9 1","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/drop_highly_correlated_features.html","loc":"http://chrisalbon.com/machine-learning/drop_highly_correlated_features.html"},{"title":"Evaluating Clustering","text":"Preliminaries import numpy as np from sklearn.metrics import silhouette_score from sklearn import datasets from sklearn.cluster import KMeans from sklearn.datasets import make_blobs Create Feature Data # Generate feature matrix X , _ = make_blobs ( n_samples = 1000 , n_features = 10 , centers = 2 , cluster_std = 0.5 , shuffle = True , random_state = 1 ) Cluster Observations # Cluster data using k-means to predict classes model = KMeans ( n_clusters = 2 , random_state = 1 ) . fit ( X ) # Get predicted classes y_hat = model . labels_ Calculate Silhouette Coefficient Formally, the $i$th observation's silhouette coefficient is: $$s_{i} = \\frac{b_{i} - a_{i}}{\\text{max}(a_{i}, b_{i})}$$ where $s_{i}$ is the silhouette coefficient for observation $i$, a_{i} is the mean distance between $i$ and all observations of the same class and b_{i} is the mean distance between $i$ and all observations from the closest cluster of a different class. The value returned by silhouette_score is the mean silhouette coefficient for all observations. Silhouette coefficients range between -1 and 1, with 1 indicating dense, well separated clusters. # Evaluate model silhouette_score ( X , y_hat ) 0.89162655640721422","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/evaluating_clustering.html","loc":"http://chrisalbon.com/machine-learning/evaluating_clustering.html"},{"title":"Generate Text Reports On Performance","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report Load Iris Flower Data # Load data iris = datasets . load_iris () # Create feature matrix X = iris . data # Create target vector y = iris . target # Create list of target class names class_names = iris . target_names Create Training And Test Sets # Create training and test set X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 1 ) Train A Logistic Regression Model # Create logistic regression classifier = LogisticRegression () # Train model and make predictions y_hat = classifier . fit ( X_train , y_train ) . predict ( X_test ) Generate Report # Create a classification report print ( classification_report ( y_test , y_hat , target_names = class_names )) precision recall f1-score support setosa 1.00 1.00 1.00 13 versicolor 1.00 0.62 0.77 16 virginica 0.60 1.00 0.75 9 avg / total 0.91 0.84 0.84 38 Note: Support refers to the number of observations in each class.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/generate_text_reports_on_performance.html","loc":"http://chrisalbon.com/machine-learning/generate_text_reports_on_performance.html"},{"title":"Plot The Learning Curve","text":"Preliminaries # Load libraries import numpy as np import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_digits from sklearn.model_selection import learning_curve Load Digits Dataset # Load data digits = load_digits () # Create feature matrix and target vector X , y = digits . data , digits . target Plot Learning Curve # Create CV training and test scores for various training set sizes train_sizes , train_scores , test_scores = learning_curve ( RandomForestClassifier (), X , y , # Number of folds in cross-validation cv = 10 , # Evaluation metric scoring = 'accuracy' , # Use all computer cores n_jobs =- 1 , # 50 different sizes of the training set train_sizes = np . linspace ( 0.01 , 1.0 , 50 )) # Create means and standard deviations of training set scores train_mean = np . mean ( train_scores , axis = 1 ) train_std = np . std ( train_scores , axis = 1 ) # Create means and standard deviations of test set scores test_mean = np . mean ( test_scores , axis = 1 ) test_std = np . std ( test_scores , axis = 1 ) # Draw lines plt . plot ( train_sizes , train_mean , '--' , color = \"#111111\" , label = \"Training score\" ) plt . plot ( train_sizes , test_mean , color = \"#111111\" , label = \"Cross-validation score\" ) # Draw bands plt . fill_between ( train_sizes , train_mean - train_std , train_mean + train_std , color = \"#DDDDDD\" ) plt . fill_between ( train_sizes , test_mean - test_std , test_mean + test_std , color = \"#DDDDDD\" ) # Create plot plt . title ( \"Learning Curve\" ) plt . xlabel ( \"Training Set Size\" ), plt . ylabel ( \"Accuracy Score\" ), plt . legend ( loc = \"best\" ) plt . tight_layout () plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/plot_the_learning_curve.html","loc":"http://chrisalbon.com/machine-learning/plot_the_learning_curve.html"},{"title":"Plot The Validation Curve","text":"Preliminaries # Load libraries import matplotlib.pyplot as plt import numpy as np from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import validation_curve Load Digits Dataset # Load data digits = load_digits () # Create feature matrix and target vector X , y = digits . data , digits . target Plot Validation Curve # Create range of values for parameter param_range = np . arange ( 1 , 250 , 2 ) # Calculate accuracy on training and test set using range of parameter values train_scores , test_scores = validation_curve ( RandomForestClassifier (), X , y , param_name = \"n_estimators\" , param_range = param_range , cv = 3 , scoring = \"accuracy\" , n_jobs =- 1 ) # Calculate mean and standard deviation for training set scores train_mean = np . mean ( train_scores , axis = 1 ) train_std = np . std ( train_scores , axis = 1 ) # Calculate mean and standard deviation for test set scores test_mean = np . mean ( test_scores , axis = 1 ) test_std = np . std ( test_scores , axis = 1 ) # Plot mean accuracy scores for training and test sets plt . plot ( param_range , train_mean , label = \"Training score\" , color = \"black\" ) plt . plot ( param_range , test_mean , label = \"Cross-validation score\" , color = \"dimgrey\" ) # Plot accurancy bands for training and test sets plt . fill_between ( param_range , train_mean - train_std , train_mean + train_std , color = \"gray\" ) plt . fill_between ( param_range , test_mean - test_std , test_mean + test_std , color = \"gainsboro\" ) # Create plot plt . title ( \"Validation Curve With Random Forest\" ) plt . xlabel ( \"Number Of Trees\" ) plt . ylabel ( \"Accuracy Score\" ) plt . tight_layout () plt . legend ( loc = \"best\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/plot_the_validation_curve.html","loc":"http://chrisalbon.com/machine-learning/plot_the_validation_curve.html"},{"title":"Recursive Feature Elimination","text":"Preliminaries # Load libraries from sklearn.datasets import make_regression from sklearn.feature_selection import RFECV from sklearn import datasets , linear_model import warnings # Suppress an annoying but harmless warning warnings . filterwarnings ( action = \"ignore\" , module = \"scipy\" , message = \"&#94;internal gelsd\" ) Create Data # Generate features matrix, target vector, and the true coefficients X , y = make_regression ( n_samples = 10000 , n_features = 100 , n_informative = 2 , random_state = 1 ) Create Linear Model # Create a linear regression ols = linear_model . LinearRegression () Conduct Recursive Feature Elimination # Create recursive feature eliminator that scores features by mean squared errors rfecv = RFECV ( estimator = ols , step = 1 , scoring = 'neg_mean_squared_error' ) # Fit recursive feature eliminator rfecv . fit ( X , y ) # Recursive feature elimination rfecv . transform ( X ) array([[ 0.00850799, 0.7031277 , -1.2416911 , -0.25651883, -0.10738769], [-1.07500204, 2.56148527, 0.5540926 , -0.72602474, -0.91773159], [ 1.37940721, -1.77039484, -0.59609275, 0.51485979, -1.17442094], ..., [-0.80331656, -1.60648007, 0.37195763, 0.78006511, -0.20756972], [ 0.39508844, -1.34564911, -0.9639982 , 1.7983361 , -0.61308782], [-0.55383035, 0.82880112, 0.24597833, -1.71411248, 0.3816852 ]]) Number Of Features Remaining # Number of best features rfecv . n_features_ 5","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/recursive_feature_elimination.html","loc":"http://chrisalbon.com/machine-learning/recursive_feature_elimination.html"},{"title":"Variance Thresholding Binary Features","text":"Preliminaries from sklearn.feature_selection import VarianceThreshold Load Data # Create feature matrix with: # Feature 0: 80% class 0 # Feature 1: 80% class 1 # Feature 2: 60% class 0, 40% class 1 X = [[ 0 , 1 , 0 ], [ 0 , 1 , 1 ], [ 0 , 1 , 0 ], [ 0 , 1 , 1 ], [ 1 , 0 , 0 ]] Conduct Variance Thresholding In binary features (i.e. Bernoulli random variables), variance is calculated as: $$\\operatorname {Var} (x)= p(1-p)$$ where $p$ is the proportion of observations of class 1 . Therefore, by setting $p$, we can remove features where the vast majority of observations are one class. # Run threshold by variance thresholder = VarianceThreshold ( threshold = ( . 75 * ( 1 - . 75 ))) thresholder . fit_transform ( X ) array([[0], [1], [0], [1], [0]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/variance_thresholding_binary_features.html","loc":"http://chrisalbon.com/machine-learning/variance_thresholding_binary_features.html"},{"title":"Variance Thresholding For Feature Selection","text":"Preliminaries from sklearn import datasets from sklearn.feature_selection import VarianceThreshold Load Data # Load iris data iris = datasets . load_iris () # Create features and target X = iris . data y = iris . target Conduct Variance Thresholding # Create VarianceThreshold object with a variance with a threshold of 0.5 thresholder = VarianceThreshold ( threshold =. 5 ) # Conduct variance thresholding X_high_variance = thresholder . fit_transform ( X ) View high variance features # View first five rows with features with variances above threshold X_high_variance [ 0 : 5 ] array([[ 5.1, 1.4, 0.2], [ 4.9, 1.4, 0.2], [ 4.7, 1.3, 0.2], [ 4.6, 1.5, 0.2], [ 5. , 1.4, 0.2]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/variance_thresholding_for_feature_selection.html","loc":"http://chrisalbon.com/machine-learning/variance_thresholding_for_feature_selection.html"},{"title":"Dimensionality Reduction On Sparse Feature Matrix","text":"Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets . load_digits () # Standardize the feature matrix X = StandardScaler () . fit_transform ( digits . data ) # Make sparse matrix X_sparse = csr_matrix ( X ) Create Truncated Singular Value Decomposition # Create a TSVD tsvd = TruncatedSVD ( n_components = 10 ) Run Truncated Singular Value Decomposition # Conduct TSVD on sparse matrix X_sparse_tsvd = tsvd . fit ( X_sparse ) . transform ( X_sparse ) View Results # Show results print ( 'Original number of features:' , X_sparse . shape [ 1 ]) print ( 'Reduced number of features:' , X_sparse_tsvd . shape [ 1 ]) Original number of features: 64 Reduced number of features: 10 View Percent Of Variance Explained By New Features # Sum of first three components' explained variance ratios tsvd . explained_variance_ratio_ [ 0 : 3 ] . sum () 0.30039385372588506","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/dimensionality_reduction_on_sparse_feature_matrix.html","loc":"http://chrisalbon.com/machine-learning/dimensionality_reduction_on_sparse_feature_matrix.html"},{"title":"Dimensionality Reduction With PCA","text":"Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn import datasets Load Data # Load the data digits = datasets . load_digits () Standardize Feature Values # Standardize the feature matrix X = StandardScaler () . fit_transform ( digits . data ) Conduct Principal Component Analysis # Create a PCA that will retain 99% of the variance pca = PCA ( n_components = 0.99 , whiten = True ) # Conduct PCA X_pca = pca . fit_transform ( X ) View Results # Show results print ( 'Original number of features:' , X . shape [ 1 ]) print ( 'Reduced number of features:' , X_pca . shape [ 1 ]) Original number of features: 64 Reduced number of features: 54","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/dimensionality_reduction_with_pca.html","loc":"http://chrisalbon.com/machine-learning/dimensionality_reduction_with_pca.html"},{"title":"Feature Extraction With PCA","text":"Principle Component Analysis (PCA) is a common feature extraction method in data science. Technically, PCA finds the eigenvectors of a covariance matrix with the highest eigenvalues and then uses those to project the data into a new subspace of equal or less dimensions. Practically, PCA converts a matrix of n features into a new dataset of (hopefully) less than n features. That is, it reduces the number of features by constructing a new, smaller number variables which capture a signficant portion of the information found in the original features. However, the goal of this tutorial is not to explain the concept of PCA, that is done very well elsewhere, but rather to demonstrate PCA in action. Preliminaries # Import packages import numpy as np from sklearn import decomposition , datasets from sklearn.preprocessing import StandardScaler Load Features # Load the breast cancer dataset dataset = datasets . load_breast_cancer () # Load the features X = dataset . data Notice that original data contains 569 observations and 30 features. # View the shape of the dataset X . shape (569, 30) Here is what the data looks like. # View the data X array([[ 1.79900000e+01, 1.03800000e+01, 1.22800000e+02, ..., 2.65400000e-01, 4.60100000e-01, 1.18900000e-01], [ 2.05700000e+01, 1.77700000e+01, 1.32900000e+02, ..., 1.86000000e-01, 2.75000000e-01, 8.90200000e-02], [ 1.96900000e+01, 2.12500000e+01, 1.30000000e+02, ..., 2.43000000e-01, 3.61300000e-01, 8.75800000e-02], ..., [ 1.66000000e+01, 2.80800000e+01, 1.08300000e+02, ..., 1.41800000e-01, 2.21800000e-01, 7.82000000e-02], [ 2.06000000e+01, 2.93300000e+01, 1.40100000e+02, ..., 2.65000000e-01, 4.08700000e-01, 1.24000000e-01], [ 7.76000000e+00, 2.45400000e+01, 4.79200000e+01, ..., 0.00000000e+00, 2.87100000e-01, 7.03900000e-02]]) Standardize Features # Create a scaler object sc = StandardScaler () # Fit the scaler to the features and transform X_std = sc . fit_transform ( X ) Conduct PCA Notice that PCA contains a parameter, the number of components. This is the number of output features and will need to be tuned. # Create a pca object with the 2 components as a parameter pca = decomposition . PCA ( n_components = 2 ) # Fit the PCA and transform the data X_std_pca = pca . fit_transform ( X_std ) View New Features After the PCA, the new data has been reduced to two features, with the same number of rows as the original feature. # View the new feature data's shape X_std_pca . shape (569, 2) # View the new feature data X_std_pca array([[ 9.19283683, 1.94858307], [ 2.3878018 , -3.76817174], [ 5.73389628, -1.0751738 ], ..., [ 1.25617928, -1.90229671], [ 10.37479406, 1.67201011], [ -5.4752433 , -0.67063679]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/feature_extraction_with_pca.html","loc":"http://chrisalbon.com/machine-learning/feature_extraction_with_pca.html"},{"title":"Using Linear Discriminant Analysis For Dimensionality Reduction","text":"Preliminaries # Load libraries from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis Load Iris Data # Load the Iris flower dataset: iris = datasets . load_iris () X = iris . data y = iris . target Create A Linear # Create an LDA that will reduce the data down to 1 feature lda = LinearDiscriminantAnalysis ( n_components = 1 ) # run an LDA and use it to transform the features X_lda = lda . fit ( X , y ) . transform ( X ) View Results # Print the number of features print ( 'Original number of features:' , X . shape [ 1 ]) print ( 'Reduced number of features:' , X_lda . shape [ 1 ]) Original number of features: 4 Reduced number of features: 1 View Percentage Of Variance Retained By New Features ## View the ratio of explained variance lda . explained_variance_ratio_ array([ 0.99147248])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/lda_for_dimensionality_reduction.html","loc":"http://chrisalbon.com/machine-learning/lda_for_dimensionality_reduction.html"},{"title":"Selecting The Best Number Of Components For LDA","text":"In scikit-learn, LDA is implemented using LinearDiscriminantAnalysis includes a parameter, n_components indicating the number of features we want returned. To figure out what argument value to use with n_components (e.g. how many parameters to keep), we can take advantage of the fact that explained_variance_ratio_ tells us the variance explained by each outputted feature and is a sorted array. Specifically, we can run LinearDiscriminantAnalysis with n_components set to None to return ratio of variance explained by every component feature, then calculate how many components are required to get above some threshold of variance explained (often 0.95 or 0.99). Preliminaries # Load libraries from sklearn import datasets from sklearn.discriminant_analysis import LinearDiscriminantAnalysis Load Iris Data # Load the Iris flower dataset: iris = datasets . load_iris () X = iris . data y = iris . target Run Linear Discriminant Analysis # Create and run an LDA lda = LinearDiscriminantAnalysis ( n_components = None ) X_lda = lda . fit ( X , y ) Create List Of Explained Variances # Create array of explained variance ratios lda_var_ratios = lda . explained_variance_ratio_ Create Function Calculating Number Of Components Required To Pass Threshold # Create a function def select_n_components ( var_ratio , goal_var : float ) -> int : # Set initial variance explained so far total_variance = 0.0 # Set initial number of features n_components = 0 # For the explained variance of each feature: for explained_variance in var_ratio : # Add the explained variance to the total total_variance += explained_variance # Add one to the number of components n_components += 1 # If we reach our goal level of explained variance if total_variance >= goal_var : # End the loop break # Return the number of components return n_components Run Function # Run function select_n_components ( lda_var_ratios , 0.95 ) 1","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/select_best_number_of_components_in_lda.html","loc":"http://chrisalbon.com/machine-learning/select_best_number_of_components_in_lda.html"},{"title":"Selecting The Best Number Of Components For TSVD","text":"Preliminaries # Load libraries from sklearn.preprocessing import StandardScaler from sklearn.decomposition import TruncatedSVD from scipy.sparse import csr_matrix from sklearn import datasets import numpy as np Load Digits Data And Make Sparse # Load the data digits = datasets . load_digits () # Standardize the feature matrix X = StandardScaler () . fit_transform ( digits . data ) # Make sparse matrix X_sparse = csr_matrix ( X ) Run Truncated Singular Value Decomposition # Create and run an TSVD with one less than number of features tsvd = TruncatedSVD ( n_components = X_sparse . shape [ 1 ] - 1 ) X_tsvd = tsvd . fit ( X ) Create List Of Explained Variances # List of explained variances tsvd_var_ratios = tsvd . explained_variance_ratio_ Create Function Calculating Number Of Components Required To Pass Threshold # Create a function def select_n_components ( var_ratio , goal_var : float ) -> int : # Set initial variance explained so far total_variance = 0.0 # Set initial number of features n_components = 0 # For the explained variance of each feature: for explained_variance in var_ratio : # Add the explained variance to the total total_variance += explained_variance # Add one to the number of components n_components += 1 # If we reach our goal level of explained variance if total_variance >= goal_var : # End the loop break # Return the number of components return n_components Run Function # Run function select_n_components ( tsvd_var_ratios , 0.95 ) 40","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/select_best_number_of_components_in_tsvd.html","loc":"http://chrisalbon.com/machine-learning/select_best_number_of_components_in_tsvd.html"},{"title":"Binarize Images","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as greyscale image_grey = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Apply Adaptive Thresholding # Apply adaptive thresholding max_output_value = 255 neighorhood_size = 99 subtract_from_mean = 10 image_binarized = cv2 . adaptiveThreshold ( image_grey , max_output_value , cv2 . ADAPTIVE_THRESH_GAUSSIAN_C , cv2 . THRESH_BINARY , neighorhood_size , subtract_from_mean ) View Image # Show image plt . imshow ( image_binarized , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/binarize_image.html","loc":"http://chrisalbon.com/machine-learning/binarize_image.html"},{"title":"Blurring Images","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Blur Image # Blur image image_blurry = cv2 . blur ( image , ( 5 , 5 )) View Image # Show image plt . imshow ( image_blurry , cmap = 'gray' ), plt . xticks ([]), plt . yticks ([]) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/blurring_images.html","loc":"http://chrisalbon.com/machine-learning/blurring_images.html"},{"title":"Break Up Dates And Times Into Multiple Features","text":"Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd . DataFrame () # Create five dates df [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 150 , freq = 'W' ) Break Up Dates And Times Into Individual Features # Create features for year, month, day, hour, and minute df [ 'year' ] = df [ 'date' ] . dt . year df [ 'month' ] = df [ 'date' ] . dt . month df [ 'day' ] = df [ 'date' ] . dt . day df [ 'hour' ] = df [ 'date' ] . dt . hour df [ 'minute' ] = df [ 'date' ] . dt . minute # Show three rows df . head ( 3 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } date year month day hour minute 0 2001-01-07 2001 1 7 0 0 1 2001-01-14 2001 1 14 0 0 2 2001-01-21 2001 1 21 0 0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/break_up_dates_and_times_into_multiple_features.html","loc":"http://chrisalbon.com/machine-learning/break_up_dates_and_times_into_multiple_features.html"},{"title":"Calculate Difference Between Dates And Times","text":"Preliminaries # Load library import pandas as pd Create Date And Time Data # Create data frame df = pd . DataFrame () # Create two datetime features df [ 'Arrived' ] = [ pd . Timestamp ( '01-01-2017' ), pd . Timestamp ( '01-04-2017' )] df [ 'Left' ] = [ pd . Timestamp ( '01-01-2017' ), pd . Timestamp ( '01-06-2017' )] Calculate Difference (Method 1) # Calculate duration between features df [ 'Left' ] - df [ 'Arrived' ] 0 0 days 1 2 days dtype: timedelta64[ns] Calculate Difference (Method 2) # Calculate duration between features pd . Series ( delta . days for delta in ( df [ 'Left' ] - df [ 'Arrived' ])) 0 0 1 2 dtype: int64","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calculate_difference_between_dates_and_times.html","loc":"http://chrisalbon.com/machine-learning/calculate_difference_between_dates_and_times.html"},{"title":"Convert pandas Column's Time Zone","text":"Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones [ 0 : 10 ] ['Africa/Abidjan', 'Africa/Accra', 'Africa/Addis_Ababa', 'Africa/Algiers', 'Africa/Asmara', 'Africa/Asmera', 'Africa/Bamako', 'Africa/Bangui', 'Africa/Banjul', 'Africa/Bissau'] Create pandas Series Of Dates # Create ten dates dates = pd . Series ( pd . date_range ( '2/2/2002' , periods = 10 , freq = 'M' )) Add Time Zone Of pandas Series # Set time zone dates_with_abidjan_time_zone = dates . dt . tz_localize ( 'Africa/Abidjan' ) # View pandas series dates_with_abidjan_time_zone 0 2002-02-28 00:00:00+00:00 1 2002-03-31 00:00:00+00:00 2 2002-04-30 00:00:00+00:00 3 2002-05-31 00:00:00+00:00 4 2002-06-30 00:00:00+00:00 5 2002-07-31 00:00:00+00:00 6 2002-08-31 00:00:00+00:00 7 2002-09-30 00:00:00+00:00 8 2002-10-31 00:00:00+00:00 9 2002-11-30 00:00:00+00:00 dtype: datetime64[ns, Africa/Abidjan] Convert Time Zone Of pandas Series # Convert time zone dates_with_london_time_zone = dates_with_abidjan_time_zone . dt . tz_convert ( 'Europe/London' ) # View pandas series dates_with_london_time_zone 0 2002-02-28 00:00:00+00:00 1 2002-03-31 00:00:00+00:00 2 2002-04-30 01:00:00+01:00 3 2002-05-31 01:00:00+01:00 4 2002-06-30 01:00:00+01:00 5 2002-07-31 01:00:00+01:00 6 2002-08-31 01:00:00+01:00 7 2002-09-30 01:00:00+01:00 8 2002-10-31 00:00:00+00:00 9 2002-11-30 00:00:00+00:00 dtype: datetime64[ns, Europe/London]","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/convert_pandas_column_timezone.html","loc":"http://chrisalbon.com/machine-learning/convert_pandas_column_timezone.html"},{"title":"Convert Strings To Dates","text":"Preliminaries # Load libraries import numpy as np import pandas as pd Create Strings # Create strings date_strings = np . array ([ '03-04-2005 11:35 PM' , '23-05-2010 12:01 AM' , '04-09-2009 09:09 PM' ]) Convert Strings To Timestamps If errors=\"coerce\" then any problem will not raise an error (the default behavior) but instead will set the value causing the error to NaT (i.e. a missing value). Code Description Example `%Y` Full year `2001` `%m` Month w/ zero padding `04` `%d` Day of the month w/ zero padding `09` `%I` Hour (12hr clock) w/ zero padding `02` `%p` AM or PM `AM` `%M` Minute w/ zero padding `05` `%S` Second w/ zero padding `09` # Convert to datetimes [ pd . to_datetime ( date , format = \" %d -%m-%Y %I:%M %p\" , errors = \"coerce\" ) for date in date_strings ] [Timestamp('2005-04-03 23:35:00'), Timestamp('2010-05-23 00:01:00'), Timestamp('2009-09-04 21:09:00')]","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/convert_strings_to_dates.html","loc":"http://chrisalbon.com/machine-learning/convert_strings_to_dates.html"},{"title":"Cropping Images","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Crop Image # Select first half of the columns and all rows image_cropped = image [:,: 126 ] View Image # View image plt . imshow ( image_cropped , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cropping_images.html","loc":"http://chrisalbon.com/machine-learning/cropping_images.html"},{"title":"Detect Edges","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as greyscale image_gray = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Detect Edges # Calculate median intensity median_intensity = np . median ( image_gray ) # Set thresholds to be one standard deviation above and below median intensity lower_threshold = int ( max ( 0 , ( 1.0 - 0.33 ) * median_intensity )) upper_threshold = int ( min ( 255 , ( 1.0 + 0.33 ) * median_intensity )) # Apply canny edge detector image_canny = cv2 . Canny ( image_gray , lower_threshold , upper_threshold ) View Edges # Show image plt . imshow ( image_canny , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/detect_edges.html","loc":"http://chrisalbon.com/machine-learning/detect_edges.html"},{"title":"Encode Days Of The Week","text":"Preliminaries # Load library import pandas as pd Create Date And Time Data # Create dates dates = pd . Series ( pd . date_range ( '2/2/2002' , periods = 3 , freq = 'M' )) # View data dates 0 2002-02-28 1 2002-03-31 2 2002-04-30 dtype: datetime64[ns] Show Days Of The Week # Show days of the week dates . dt . weekday_name 0 Thursday 1 Sunday 2 Tuesday dtype: object","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/encode_days_of_the_week.html","loc":"http://chrisalbon.com/machine-learning/encode_days_of_the_week.html"},{"title":"Enhance Contrast Of Color Image","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2 . imread ( 'images/plane.jpg' ) Convert Image To YUV Color Format # Convert to YUV image_yuv = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2YUV ) Enhance Image # Apply histogram equalization image_yuv [:, :, 0 ] = cv2 . equalizeHist ( image_yuv [:, :, 0 ]) Convert To RGB # Convert to RGB image_rgb = cv2 . cvtColor ( image_yuv , cv2 . COLOR_YUV2RGB ) View Image # Show image plt . imshow ( image_rgb ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/enhance_contrast_of_color_image.html","loc":"http://chrisalbon.com/machine-learning/enhance_contrast_of_color_image.html"},{"title":"Enhance Contrast Of Greyscale Image","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Enhance Image # Enhance image image_enhanced = cv2 . equalizeHist ( image ) View Image # Show image plt . imshow ( image_enhanced , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/enhance_contrast_of_greyscale_image.html","loc":"http://chrisalbon.com/machine-learning/enhance_contrast_of_greyscale_image.html"},{"title":"Handling Missing Values In Time Series","text":"Preliminaries # Load libraries import pandas as pd import numpy as np Create Date Data With Gap In Values # Create date time_index = pd . date_range ( '01/01/2010' , periods = 5 , freq = 'M' ) # Create data frame, set index df = pd . DataFrame ( index = time_index ) # Create feature with a gap of missing values df [ 'Sales' ] = [ 1.0 , 2.0 , np . nan , np . nan , 5.0 ] Interpolate Missing Values # Interpolate missing values df . interpolate () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Sales 2010-01-31 1.0 2010-02-28 2.0 2010-03-31 3.0 2010-04-30 4.0 2010-05-31 5.0 Forward-fill Missing Values # Forward-fill df . ffill () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Sales 2010-01-31 1.0 2010-02-28 2.0 2010-03-31 2.0 2010-04-30 2.0 2010-05-31 5.0 Backfill Missing Values # Back-fill df . bfill () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Sales 2010-01-31 1.0 2010-02-28 2.0 2010-03-31 5.0 2010-04-30 5.0 2010-05-31 5.0 Interpolate Missing Values But Only Up One Value # Interpolate missing values df . interpolate ( limit = 1 , limit_direction = 'forward' ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Sales 2010-01-31 1.0 2010-02-28 2.0 2010-03-31 3.0 2010-04-30 NaN 2010-05-31 5.0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_missing_values_in_time_series.html","loc":"http://chrisalbon.com/machine-learning/handling_missing_values_in_time_series.html"},{"title":"Handling Time Zones","text":"Preliminaries # Load libraries import pandas as pd from pytz import all_timezones View Timezones # Show ten time zones all_timezones [ 0 : 10 ] ['Africa/Abidjan', 'Africa/Accra', 'Africa/Addis_Ababa', 'Africa/Algiers', 'Africa/Asmara', 'Africa/Asmera', 'Africa/Bamako', 'Africa/Bangui', 'Africa/Banjul', 'Africa/Bissau'] Create Timestamp With Time Zone # Create datetime pd . Timestamp ( '2017-05-01 06:00:00' , tz = 'Europe/London' ) Timestamp('2017-05-01 06:00:00+0100', tz='Europe/London') Create Timestamp Without Time Zone # Create datetime date = pd . Timestamp ( '2017-05-01 06:00:00' ) Add Time Zone # Set time zone date_in_london = date . tz_localize ( 'Europe/London' ) Convert Time Zone # Change time zone date_in_london . tz_convert ( 'Africa/Abidjan' ) Timestamp('2017-05-01 05:00:00+0000', tz='Africa/Abidjan')","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_time_zones.html","loc":"http://chrisalbon.com/machine-learning/handling_time_zones.html"},{"title":"Harris Corner Detector","text":"The Harris Corner Detector is a commonly used method of detecting the intersection of two edges. It looks for windows (also called neighborhoods or patches) where small movements of the window (imagine shaking the window) creates big changes in the contents of the pixels inside the window. Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as grayscale image_bgr = cv2 . imread ( 'images/plane_256x256.jpg' ) image_gray = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2GRAY ) image_gray = np . float32 ( image_gray ) Define Corner Parameters # Set corner detector parameters block_size = 2 aperture = 29 free_parameter = 0.04 Detect Corners # Detect corners detector_responses = cv2 . cornerHarris ( image_gray , block_size , aperture , free_parameter ) Mark Corners # Large corner markers detector_responses = cv2 . dilate ( detector_responses , None ) # Only keep detector responses greater than threshold, mark as white threshold = 0.02 image_bgr [ detector_responses > threshold * detector_responses . max ()] = [ 255 , 255 , 255 ] View Image # Convert to grayscale image_gray = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2GRAY ) # Show image plt . imshow ( image_gray , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/harris_corner_detector.html","loc":"http://chrisalbon.com/machine-learning/harris_corner_detector.html"},{"title":"Installing OpenCV","text":"While there are a number of good libraries out there, OpenCV is the most popular and documented library for handling images. One of the biggest hurdles to using OpenCV is installing it. However, fortunately we can use Anaconda's package manager tool conda to install OpenCV in a single line of code in our terminal: conda install --channel https://conda.anaconda.org/menpo opencv3 Afterwards, we can check the installation by opening a notebook, importing OpenCV, and checking the version number (3.1.0): # Load library import cv2 # View version number cv2 . __version__ '3.2.0'","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/installing_opencv.html","loc":"http://chrisalbon.com/machine-learning/installing_opencv.html"},{"title":"Isolate Colors","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2 . imread ( 'images/plane_256x256.jpg' ) Convert To HSV Color Format # Convert BGR to HSV image_hsv = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2HSV ) Create Mask # Define range of blue values in HSV lower_blue = np . array ([ 50 , 100 , 50 ]) upper_blue = np . array ([ 130 , 255 , 255 ]) # Create mask mask = cv2 . inRange ( image_hsv , lower_blue , upper_blue ) Apply Mask # Mask image image_bgr_masked = cv2 . bitwise_and ( image_bgr , image_bgr , mask = mask ) Convert To RGB Format # Convert BGR to RGB image_rgb = cv2 . cvtColor ( image_bgr_masked , cv2 . COLOR_BGR2RGB ) View Image # Show image plt . imshow ( image_rgb ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/isolate_colors.html","loc":"http://chrisalbon.com/machine-learning/isolate_colors.html"},{"title":"Lag A Time Feature","text":"Preliminaries # Load library import pandas as pd Create Date Data # Create data frame df = pd . DataFrame () # Create data df [ 'dates' ] = pd . date_range ( '1/1/2001' , periods = 5 , freq = 'D' ) df [ 'stock_price' ] = [ 1.1 , 2.2 , 3.3 , 4.4 , 5.5 ] Lag Time Data By One Row # Lagged values by one row df [ 'previous_days_stock_price' ] = df [ 'stock_price' ] . shift ( 1 ) # Show data frame df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } dates stock_price previous_days_stock_price 0 2001-01-01 1.1 NaN 1 2001-01-02 2.2 1.1 2 2001-01-03 3.3 2.2 3 2001-01-04 4.4 3.3 4 2001-01-05 5.5 4.4","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/lag_a_time_feature.html","loc":"http://chrisalbon.com/machine-learning/lag_a_time_feature.html"},{"title":"Load Images","text":"Preliminaries # Load library import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane.jpg' , cv2 . IMREAD_GRAYSCALE ) # Show image plt . imshow ( image , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show () Load Image As RGB # Load image in color image_bgr = cv2 . imread ( 'images/plane.jpg' , cv2 . IMREAD_COLOR ) # Convert to RGB image_rgb = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2RGB ) # Show image plt . imshow ( image_rgb ), plt . axis ( \"off\" ) plt . show () View Image Data # Show image data image array([[140, 136, 146, ..., 132, 139, 134], [144, 136, 149, ..., 142, 124, 126], [152, 139, 144, ..., 121, 127, 134], ..., [156, 146, 144, ..., 157, 154, 151], [146, 150, 147, ..., 156, 158, 157], [143, 138, 147, ..., 156, 157, 157]], dtype=uint8) # Show dimensions image . shape (2270, 3600)","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/load_images.html","loc":"http://chrisalbon.com/machine-learning/load_images.html"},{"title":"Remove Backgrounds","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image # Load image image_bgr = cv2 . imread ( 'images/plane_256x256.jpg' ) Convert To RGB # Convert to RGB image_rgb = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2RGB ) Draw Rectangle Around Foreground # Rectange values: start x, start y, width, height rectangle = ( 0 , 56 , 256 , 150 ) Apply GrabCut # Create initial mask mask = np . zeros ( image_rgb . shape [: 2 ], np . uint8 ) # Create temporary arrays used by grabCut bgdModel = np . zeros (( 1 , 65 ), np . float64 ) fgdModel = np . zeros (( 1 , 65 ), np . float64 ) # Run grabCut cv2 . grabCut ( image_rgb , # Our image mask , # The Mask rectangle , # Our rectangle bgdModel , # Temporary array for background fgdModel , # Temporary array for background 5 , # Number of iterations cv2 . GC_INIT_WITH_RECT ) # Initiative using our rectangle # Create mask where sure and likely backgrounds set to 0, otherwise 1 mask_2 = np . where (( mask == 2 ) | ( mask == 0 ), 0 , 1 ) . astype ( 'uint8' ) # Multiply image with new mask to subtract background image_rgb_nobg = image_rgb * mask_2 [:, :, np . newaxis ] Show image # Show image plt . imshow ( image_rgb_nobg ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/remove_backgrounds.html","loc":"http://chrisalbon.com/machine-learning/remove_backgrounds.html"},{"title":"Rolling Time Window","text":"Preliminaries # Load library import pandas as pd Create Date Data # Create datetimes time_index = pd . date_range ( '01/01/2010' , periods = 5 , freq = 'M' ) # Create data frame, set index df = pd . DataFrame ( index = time_index ) # Create feature df [ 'Stock_Price' ] = [ 1 , 2 , 3 , 4 , 5 ] Create A Rolling Time Window Of Two Rows # Calculate rolling mean df . rolling ( window = 2 ) . mean () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Stock_Price 2010-01-31 NaN 2010-02-28 1.5 2010-03-31 2.5 2010-04-30 3.5 2010-05-31 4.5 # Identify max value in rolling time window df . rolling ( window = 2 ) . max () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Stock_Price 2010-01-31 NaN 2010-02-28 2.0 2010-03-31 3.0 2010-04-30 4.0 2010-05-31 5.0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/rolling_time_windows.html","loc":"http://chrisalbon.com/machine-learning/rolling_time_windows.html"},{"title":"Save Images","text":"Preliminaries # Load library import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane.jpg' , cv2 . IMREAD_GRAYSCALE ) # Show image plt . imshow ( image , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show () Save Image # Save image cv2 . imwrite ( 'images/plane_new.jpg' , image ) True","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/save_images.html","loc":"http://chrisalbon.com/machine-learning/save_images.html"},{"title":"Select Date And Time Ranges","text":"Preliminaries # Load library import pandas as pd Create pandas Series Time Data # Create data frame df = pd . DataFrame () # Create datetimes df [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 100000 , freq = 'H' ) Select Time Range (Method 1) Use this method if your data frame is not indexed by time. # Select observations between two datetimes df [( df [ 'date' ] > '2002-1-1 01:00:00' ) & ( df [ 'date' ] <= '2002-1-1 04:00:00' )] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } date 8762 2002-01-01 02:00:00 8763 2002-01-01 03:00:00 8764 2002-01-01 04:00:00 Select Time Range (Method 2) Use this method if your data frame is indexed by time. # Set index df = df . set_index ( df [ 'date' ]) # Select observations between two datetimes df . loc [ '2002-1-1 01:00:00' : '2002-1-1 04:00:00' ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } date date 2002-01-01 01:00:00 2002-01-01 01:00:00 2002-01-01 02:00:00 2002-01-01 02:00:00 2002-01-01 03:00:00 2002-01-01 03:00:00 2002-01-01 04:00:00 2002-01-01 04:00:00","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/select_date_and_time_ranges.html","loc":"http://chrisalbon.com/machine-learning/select_date_and_time_ranges.html"},{"title":"Sharpen Images","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load Image As Greyscale # Load image as grayscale image = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_GRAYSCALE ) Sharpen Image # Create kernel kernel = np . array ([[ 0 , - 1 , 0 ], [ - 1 , 5 , - 1 ], [ 0 , - 1 , 0 ]]) # Sharpen image image_sharp = cv2 . filter2D ( image , - 1 , kernel ) View Image # Show image plt . imshow ( image_sharp , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/sharpen_images.html","loc":"http://chrisalbon.com/machine-learning/sharpen_images.html"},{"title":"Shi-Tomasi Corner Detector","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load images image_bgr = cv2 . imread ( 'images/plane_256x256.jpg' ) image_gray = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2GRAY ) Define Corner Parameters # Number of corners to detect corners_to_detect = 10 minimum_quality_score = 0.05 minimum_distance = 25 Detect Corners # Detect corners corners = cv2 . goodFeaturesToTrack ( image_gray , corners_to_detect , minimum_quality_score , minimum_distance ) corners = np . float32 ( corners ) Mark Corners # Draw white circle at each corner for corner in corners : x , y = corner [ 0 ] cv2 . circle ( image_bgr , ( x , y ), 10 , ( 255 , 255 , 255 ), - 1 ) View Image # Convert to grayscale image_gray = cv2 . cvtColor ( image_bgr , cv2 . COLOR_BGR2GRAY ) # Show image plt . imshow ( image_gray , cmap = 'gray' ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/ski-tomasi_corner_detector.html","loc":"http://chrisalbon.com/machine-learning/ski-tomasi_corner_detector.html"},{"title":"Using Mean Color As A Feature","text":"Preliminaries # Load image import cv2 import numpy as np from matplotlib import pyplot as plt Load image # Load image as BGR image_bgr = cv2 . imread ( 'images/plane_256x256.jpg' , cv2 . IMREAD_COLOR ) Calculate Mean Color Of Each Color Channel # Calculate the mean of each channel channels = cv2 . mean ( image_bgr ) # Swap blue and red values (making it RGB, not BGR) observation = np . array ([( channels [ 2 ], channels [ 1 ], channels [ 0 ])]) Show Values # Show mean channel values observation array([[ 90.53204346, 133.11735535, 169.03074646]]) View Mean Image Colors # Show image plt . imshow ( observation ), plt . axis ( \"off\" ) plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/using_mean_color_as_a_feature.html","loc":"http://chrisalbon.com/machine-learning/using_mean_color_as_a_feature.html"},{"title":"Bag Of Words","text":"Preliminaries # Load library import numpy as np from sklearn.feature_extraction.text import CountVectorizer import pandas as pd Create Text Data # Create text text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) Create Bag Of Words # Create the bag of words feature matrix count = CountVectorizer () bag_of_words = count . fit_transform ( text_data ) # Show feature matrix bag_of_words . toarray () array([[0, 0, 0, 2, 0, 0, 1, 0], [0, 1, 0, 0, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64) View Bag Of Words Matrix Column Headers # Get feature names feature_names = count . get_feature_names () # View feature names feature_names ['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden'] View As A Data Frame # Create data frame pd . DataFrame ( bag_of_words . toarray (), columns = feature_names ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } beats best both brazil germany is love sweden 0 0 0 0 2 0 0 1 0 1 0 1 0 0 0 1 0 1 2 1 0 1 0 1 0 0 0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/bag_of_words.html","loc":"http://chrisalbon.com/machine-learning/bag_of_words.html"},{"title":"Dimensionality Reduction With Kernel PCA","text":"Preliminaries # Load libraries from sklearn.decomposition import PCA , KernelPCA from sklearn.datasets import make_circles Create Linearly Inseparable Data # Create linearly inseparable data X , _ = make_circles ( n_samples = 1000 , random_state = 1 , noise = 0.1 , factor = 0.1 ) Conduct Kernel PCA # Apply kernal PCA with radius basis function (RBF) kernel kpca = KernelPCA ( kernel = \"rbf\" , gamma = 15 , n_components = 1 ) X_kpca = kpca . fit_transform ( X ) View Results print ( 'Original number of features:' , X . shape [ 1 ]) print ( 'Reduced number of features:' , X_kpca . shape [ 1 ]) Original number of features: 2 Reduced number of features: 1","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/dimensionality_reduction_with_kernel_pca.html","loc":"http://chrisalbon.com/machine-learning/dimensionality_reduction_with_kernel_pca.html"},{"title":"Term Frequency Inverse Document Frequency","text":"Preliminaries # Load libraries import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer import pandas as pd Create Text Data # Create text text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) Create Feature Matrix # Create the tf-idf feature matrix tfidf = TfidfVectorizer () feature_matrix = tfidf . fit_transform ( text_data ) # Show tf-idf feature matrix feature_matrix . toarray () array([[ 0. , 0. , 0. , 0.89442719, 0. , 0. , 0.4472136 , 0. ], [ 0. , 0.57735027, 0. , 0. , 0. , 0.57735027, 0. , 0.57735027], [ 0.57735027, 0. , 0.57735027, 0. , 0.57735027, 0. , 0. , 0. ]]) # Show tf-idf feature matrix tfidf . get_feature_names () ['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden'] View Feature Matrix As Data Frame # Create data frame pd . DataFrame ( feature_matrix . toarray (), columns = tfidf . get_feature_names ()) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } beats best both brazil germany is love sweden 0 0.00000 0.00000 0.00000 0.894427 0.00000 0.00000 0.447214 0.00000 1 0.00000 0.57735 0.00000 0.000000 0.00000 0.57735 0.000000 0.57735 2 0.57735 0.00000 0.57735 0.000000 0.57735 0.00000 0.000000 0.00000","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/tf-idf.html","loc":"http://chrisalbon.com/machine-learning/tf-idf.html"},{"title":"Delete Observations With Missing Values","text":"Preliminaries # Load libraries import numpy as np import pandas as pd Create Feature Matrix # Create feature matrix X = np . array ([[ 1.1 , 11.1 ], [ 2.2 , 22.2 ], [ 3.3 , 33.3 ], [ 4.4 , 44.4 ], [ np . nan , 55 ]]) Delete Observations With Missing Values # Remove observations with missing values X [ ~ np . isnan ( X ) . any ( axis = 1 )] array([[ 1.1, 11.1], [ 2.2, 22.2], [ 3.3, 33.3], [ 4.4, 44.4]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/delete_observations_with_missing_values.html","loc":"http://chrisalbon.com/machine-learning/delete_observations_with_missing_values.html"},{"title":"Calculate The Average, Variance, And Standard Deviation","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Calculate Mean # Return mean np . mean ( matrix ) 5.0 Calculate Variance # Return variance np . var ( matrix ) 6.666666666666667 Calculate Standard Deviation # Return standard deviation np . std ( matrix ) 2.5819888974716112","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calculate_average_variance_and_standard_deviation.html","loc":"http://chrisalbon.com/machine-learning/calculate_average_variance_and_standard_deviation.html"},{"title":"Reshape An Array","text":"Preliminaries # Load library import numpy as np Create Array # Create a 4x3 matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) Reshape Array # Reshape matrix into 2x6 matrix matrix . reshape ( 2 , 6 ) array([[ 1, 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11, 12]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/reshape_an_array.html","loc":"http://chrisalbon.com/machine-learning/reshape_an_array.html"},{"title":"Adding And Subtracting Matrices","text":"Preliminaries # Load library import numpy as np Create Matrices # Create matrix matrix_a = np . array ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ], [ 1 , 1 , 2 ]]) # Create matrix matrix_b = np . array ([[ 1 , 3 , 1 ], [ 1 , 3 , 1 ], [ 1 , 3 , 8 ]]) Add Matrices # Add two matrices np . add ( matrix_a , matrix_b ) array([[ 2, 4, 2], [ 2, 4, 2], [ 2, 4, 10]]) Subtract Matrices # Subtract two matrices np . subtract ( matrix_a , matrix_b ) array([[ 0, -2, 0], [ 0, -2, 0], [ 0, -2, -6]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/adding_and_subtracting_matrices.html","loc":"http://chrisalbon.com/machine-learning/adding_and_subtracting_matrices.html"},{"title":"Apply Operations To Elements","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Create Vectorized Function # Create a function that adds 100 to something add_100 = lambda i : i + 100 # Create a vectorized function vectorized_add_100 = np . vectorize ( add_100 ) Apply Function To Elements # Apply function to all elements in matrix vectorized_add_100 ( matrix ) array([[101, 102, 103], [104, 105, 106], [107, 108, 109]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/apply_operations_to_elements.html","loc":"http://chrisalbon.com/machine-learning/apply_operations_to_elements.html"},{"title":"Create A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 4 ], [ 2 , 5 ]]) Note NumPy's mat data structure is less flexible for our purposes and should be avoided.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/create_a_matrix.html"},{"title":"Create A Sparse Matrix","text":"Preliminaries # Load libraries import numpy as np from scipy import sparse Create Dense Matrix # Create a matrix matrix = np . array ([[ 0 , 0 ], [ 0 , 1 ], [ 3 , 0 ]]) Convert To Sparse Matrix # Create compressed sparse row (CSR) matrix matrix_sparse = sparse . csr_matrix ( matrix ) Note: There are many types of sparse matrices. In the example above we use CSR but the type we use should reflect our use case.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_a_sparse_matrix.html","loc":"http://chrisalbon.com/machine-learning/create_a_sparse_matrix.html"},{"title":"Describe An Array","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ]]) View Shape # View number of rows and columns matrix . shape (3, 4) View Total Elements # View number of elements (rows * columns) matrix . size 12 View Number Of Dimensions # View number of dimensions matrix . ndim 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/describe_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/describe_a_matrix.html"},{"title":"Find The Maximum And Minimum","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Find Maximum Element # Return maximum element np . max ( matrix ) 9 Find Minimum Element # Return minimum element np . min ( matrix ) 1 Find Maximum Element By Column # Find the maximum element in each column np . max ( matrix , axis = 0 ) array([7, 8, 9]) Find Maximum Element By Row # Find the maximum element in each row np . max ( matrix , axis = 1 ) array([3, 6, 9])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/find_maximum_and_minimum.html","loc":"http://chrisalbon.com/machine-learning/find_maximum_and_minimum.html"},{"title":"Invert A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 4 ], [ 2 , 5 ]]) Invert Matrix # Calculate inverse of matrix np . linalg . inv ( matrix ) array([[-1.66666667, 1.33333333], [ 0.66666667, -0.33333333]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/invert_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/invert_a_matrix.html"},{"title":"Selecting Elements In An Array","text":"Preliminaries # Load library import numpy as np Create Vector # Create row vector vector = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) Select Element # Select second element vector [ 1 ] 2 Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Select Element # Select second row, second column matrix [ 1 , 1 ] 5 Create Tensor # Create matrix tensor = np . array ([ [[[ 1 , 1 ], [ 1 , 1 ]], [[ 2 , 2 ], [ 2 , 2 ]]], [[[ 3 , 3 ], [ 3 , 3 ]], [[ 4 , 4 ], [ 4 , 4 ]]] ]) Select Element # Select second element of each of the three dimensions tensor [ 1 , 1 , 1 ] array([4, 4])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/selecting_elements_in_an_array.html","loc":"http://chrisalbon.com/machine-learning/selecting_elements_in_an_array.html"},{"title":"Calculate Dot Product Of Two Vectors","text":"Preliminaries # Load library import numpy as np Create Two Vectors # Create two vectors vector_a = np . array ([ 1 , 2 , 3 ]) vector_b = np . array ([ 4 , 5 , 6 ]) Calculate Dot Product (Method 1) # Calculate dot product np . dot ( vector_a , vector_b ) 32 Calculate Dot Product (Method 2) # Calculate dot product vector_a @ vector_b 32","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calculate_dot_product_of_two_vectors.html","loc":"http://chrisalbon.com/machine-learning/calculate_dot_product_of_two_vectors.html"},{"title":"Calculate The Determinant Of A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Calculate Determinant # Return determinant of matrix np . linalg . det ( matrix ) -9.5161973539299405e-16","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calculate_the_determinant_of_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/calculate_the_determinant_of_a_matrix.html"},{"title":"Calculate The Trace Of A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Calculate The Trace # Calculate the tracre of the matrix matrix . diagonal () . sum () 15","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/calculate_the_trace_of_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/calculate_the_trace_of_a_matrix.html"},{"title":"Create A Vector","text":"Preliminaries # Load library import numpy as np Create Row Vector # Create a vector as a row vector_row = np . array ([ 1 , 2 , 3 ]) Create Column Vector # Create a vector as a column vector_column = np . array ([[ 1 ], [ 2 ], [ 3 ]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_a_vector.html","loc":"http://chrisalbon.com/machine-learning/create_a_vector.html"},{"title":"Deleting Missing Values","text":"Preliminaries # Load library import numpy as np import pandas as pd Create Data Frame # Create feature matrix X = np . array ([[ 1 , 2 ], [ 6 , 3 ], [ 8 , 4 ], [ 9 , 5 ], [ np . nan , 4 ]]) Drop Missing Values Using NumPy # Remove observations with missing values X [ ~ np . isnan ( X ) . any ( axis = 1 )] array([[ 1., 2.], [ 6., 3.], [ 8., 4.], [ 9., 5.]]) Drop Missing Values Using pandas # Load data as a data frame df = pd . DataFrame ( X , columns = [ 'feature_1' , 'feature_2' ]) # Remove observations with missing values df . dropna () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } feature_1 feature_2 0 1.0 2.0 1 6.0 3.0 2 8.0 4.0 3 9.0 5.0","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/deleting_missing_values.html","loc":"http://chrisalbon.com/machine-learning/deleting_missing_values.html"},{"title":"Find The Rank Of A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Find Rank Of Matrix # Return matrix rank np . linalg . matrix_rank ( matrix ) 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/find_the_rank_of_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/find_the_rank_of_a_matrix.html"},{"title":"Flatten A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Flatten Matrix # Flatten matrix matrix . flatten () array([1, 2, 3, 4, 5, 6, 7, 8, 9])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/flatten_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/flatten_a_matrix.html"},{"title":"Getting The Diagonal Of A Matrix","text":"Preliminaries # Load library import numpy as np Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Get The Diagonal # Return diagonal elements matrix . diagonal () array([1, 5, 9]) Calculate The Trace # Calculate the tracre of the matrix matrix . diagonal () . sum () 15","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/getting_the_diagonal_of_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/getting_the_diagonal_of_a_matrix.html"},{"title":"Transpose A Vector Or Matrix","text":"Preliminaries # Load library import numpy as np Create Vector # Create vector vector = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) Create Matrix # Create matrix matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) Transpose Vector # Tranpose vector vector . T array([1, 2, 3, 4, 5, 6]) Transpose Matrix # Transpose matrix matrix . T array([[1, 4, 7], [2, 5, 8], [3, 6, 9]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/transpose_a_vector_or_matrix.html","loc":"http://chrisalbon.com/machine-learning/transpose_a_vector_or_matrix.html"},{"title":"Display Scientific Notation As Floats","text":"Create Values # Create a numbers in scientific notation value_scientific_notation = 6.32000000e-03 # Create a vector of numbers in scientific notation vector_scientific_notation = [ 6.32000000e-03 , 1.80000000e+01 , 2.31000000e+00 , 0.00000000e+00 , 5.38000000e-01 , 6.57500000e+00 , 6.52000000e+01 , 4.09000000e+00 , 1.00000000e+00 , 2.96000000e+02 , 1.53000000e+01 , 3.96900000e+02 , 4.98000000e+00 ] Display Values As Floats # Display value as a float '{:f}' . format ( value_scientific_notation ) '0.006320' # Display vector values as floats [ '{:f}' . format ( x ) for x in vector_scientific_notation ] ['0.006320', '18.000000', '2.310000', '0.000000', '0.538000', '6.575000', '65.200000', '4.090000', '1.000000', '296.000000', '15.300000', '396.900000', '4.980000']","tags":"Python","url":"http://chrisalbon.com/python/display_scientific_notation_as_floats.html","loc":"http://chrisalbon.com/python/display_scientific_notation_as_floats.html"},{"title":"Summer Of Machine Learning","text":"I love summers, but not for the reason you'd think. I don't particularly like hot weather and prefer a good coffee shop to a beach resort. For me, a good summer will always mean what they meant during my Ph.D.: four months to grind your heart out -- to read 500 journal articles, submit five research papers, or finish that one big project. It is a time when the burden of daily responsibilities and thousand little emergencies lifts and you are free to work on big projects with big goals -- to enter the chill of fall better, smarter, and farther than before. I left academia and if there is one thing I miss, it is the summers. The demand for new sales and product releases never lets up and nature of work makes August feel much like February. Over the years I have noticed that being in that environment has made me reach less in personal goals. I took on smaller personal projects, read fewer books, and slowly gained weight. I don't regret this drift. Effort is zero-sum and I was focused on other things including a starting an AI company . However, this summer I am going back to the summers of my Ph.D.: a summer of machine learning. From June 1st to September 30th I will make a four month sprint to become a better data scientist and machine learning engineer, filling the dog days of summer with reading, writing, coding, and running. And the finish line? Eight concrete, quantifiable goals for the next 122 days: Work through 200 machine learning tutorials online. Watch or listen to 100 hours of video lectures or podcast episodes on machine learning. Read 20 books on relevant machine learning topics. Create 400 #machinelearningflashcards to study and memorize. Create 504 tutorials or posts on my personal site. Create 100 \"recipes\" for a forthcoming machine learning book. Run 500 miles. Lose 40lbs. Want to cheer me on? I'm on Twitter .","tags":"Blog","url":"http://chrisalbon.com/blog/summer_of_machine_learning.html","loc":"http://chrisalbon.com/blog/summer_of_machine_learning.html"},{"title":"Find Largest Key Or Value In A Map","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Map // Create an immutable map with three key value pairs val numbers = Map ( 1 -> 100 , 2 -> 200 , 3 -> 300 ) Find Largest Key // Find largest key numbers . max (3,300) Find Largest Value // Find the largest value numbers . valuesIterator . max 300","tags":"Scala","url":"http://chrisalbon.com/scala/find_largest_key_or_value_in_a_map.html","loc":"http://chrisalbon.com/scala/find_largest_key_or_value_in_a_map.html"},{"title":"Iterate Over A Map","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Map // Create a map with three key value pairs val prices = Map ( \"Video Card\" -> 200.00 , \"Motherboard\" -> 400.00 , \"CPU\" -> 100.00 ) Loop Over A Map // for each key and value in prices for (( k , v ) <- prices ) yield { // Return the value plus 100 v + 100 } List(300.0, 500.0, 200.0) Apply Function To Each Map Value // Increase each value in the map by 1000 prices . mapValues ( _ + 1000 ) Map(Video Card -> 1200.0, Motherboard -> 1400.0, CPU -> 1100.0)","tags":"Scala","url":"http://chrisalbon.com/scala/iterate_over_a_map.html","loc":"http://chrisalbon.com/scala/iterate_over_a_map.html"},{"title":"Mutable Maps","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Mutable Map val army = collection . mutable . Map ( \"Tank\" -> \"A-1 Abrams\" , \"Aircraft\" -> \"F35\" , \"Ship\" -> \"Nimitz Class\" ) Add An Element // Add an element army += ( \"APC\" -> \"Bradley IFC\" ) // Add an element (alternative) army . put ( \"Weapon\" , \"M60\" ) None Add Multiple Elements // Add two elements army += ( \"Helicopter\" -> \"Apache\" , \"Missile\" -> \"Sidewinder\" ) Map(Weapon -> M60, APC -> Bradley IFC, Missile -> Sidewinder, Tank -> A-1 Abrams, Aircraft -> F35, Helicopter -> Apache, Ship -> Nimitz Class) Remove An Element // Remove an element army -= \"Ship\" // Remove an element (alternative) army . remove ( \"Tank\" ) Some(A-1 Abrams) Change A Value // Change the value of an element army ( \"Tank\" ) = \"Tiger Tank\" Filter A Map // Keep only the key, value pairs that meet the criteria army . retain (( k , v ) => k == \"Tank\" ) Map(Tank -> Tiger Tank)","tags":"Scala","url":"http://chrisalbon.com/scala/mutable_maps.html","loc":"http://chrisalbon.com/scala/mutable_maps.html"},{"title":"N Dimension Arrays","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create 2 x 2 Array // Set the number of rows and columns val rows = 2 val columns = 2 // Create an array of integers that is 2x2 val matrix = Array . ofDim [ Int ]( rows , columns ) // View array matrix Array(Array(0, 0), Array(0, 0)) Add Values To Array // First row, first column matrix ( 0 )( 0 ) = 1 // First row, second column matrix ( 0 )( 1 ) = 0 // Second row, first column matrix ( 1 )( 0 ) = 0 // Second row, second column matrix ( 1 )( 1 ) = 1 // View array matrix Array(Array(1, 0), Array(0, 1))","tags":"Scala","url":"http://chrisalbon.com/scala/n_dimension_arrays.html","loc":"http://chrisalbon.com/scala/n_dimension_arrays.html"},{"title":"Search A Map","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Map // Create an immutable map with three key value pairs val staff = Map ( \"CEO\" -> \"Judith Jackson\" , \"CFO\" -> \"Sally Shields\" , \"CTO\" -> \"Steven Miller\" ) Test If Key Exists // Test if key exists staff . contains ( \"CTO\" ) true Test If Value Exists // Test is any value exists which contains part of a string staff . valuesIterator . exists ( _ . contains ( \"Miller\" )) true","tags":"Scala","url":"http://chrisalbon.com/scala/search_a_map.html","loc":"http://chrisalbon.com/scala/search_a_map.html"},{"title":"Area Plot","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple . filterData ( data , \"Owner\" , [ \"Aperture\" , \"Black Mesa\" ]) // Create a chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis of time, grouped by Month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x-axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the actual data var s = myChart . addSeries ( null , dimple . plot . area ); // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple.filterData(data, \"Owner\", [\"Aperture\", \"Black Mesa\"]) // Create a chart var myChart = new dimple.chart(svg, data); // Add an x-axis of time, grouped by Month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x-axis by date x.addOrderRule(\"Date\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the actual data var s = myChart.addSeries(null, dimple.plot.area); // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/area-plot.html","loc":"http://chrisalbon.com/javascript/area-plot.html"},{"title":"Bar Chart","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis that groups the data by month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis for the variable \"Unit Sales\" myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the data to the chart var avg = myChart . addSeries ( null , dimple . plot . bar ); // Calculate the average Unit Sales avg . aggregate = dimple . aggregateMethod . avg ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add an x-axis that groups the data by month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x axis by date x.addOrderRule(\"Date\"); // Add a y-axis for the variable \"Unit Sales\" myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the data to the chart var avg = myChart.addSeries(null, dimple.plot.bar); // Calculate the average Unit Sales avg.aggregate = dimple.aggregateMethod.avg; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/bar-chart.html","loc":"http://chrisalbon.com/javascript/bar-chart.html"},{"title":"Basic Mathematics","text":"< script > // Create an array of numbers var ages = [ 22 , 43 , 24 , 48 , 35 , 34 , 20 ]; // Print to log console . log ( 'ages: ' , ages ); // Get the length of the ages array var numberOfEmployes = ages . length ; // Print to log console . log ( 'numberOfEmployes: ' , numberOfEmployes ); // Square root var squareRoot = Math . sqrt ( numberOfEmployes ); // Print to log console . log ( 'squareRoot: ' , squareRoot ); // Absolute Value var absoluteValue = Math . abs ( - 4.7 ); // Print to log console . log ( 'absoluteValue: ' , absoluteValue ); // Minimum value of an array var min = Math . min . apply ( null , ages ); // Print to log console . log ( 'min: ' , min ); // Maximum value of an array var max = Math . max . apply ( null , ages ); // Print to log console . log ( 'max: ' , max ); // Log var log = Math . log ( numberOfEmployes ); // Print to log console . log ( 'log: ' , log ); // Exponent var exponent = Math . exp ( numberOfEmployes ); // Print to log console . log ( 'exponent: ' , exponent ); </ script > // Create an array of numbers var ages = [22,43,24,48,35,34,20]; // Print to log console.log('ages: ', ages); // Get the length of the ages array var numberOfEmployes = ages.length; // Print to log console.log('numberOfEmployes: ', numberOfEmployes); // Square root var squareRoot = Math.sqrt(numberOfEmployes); // Print to log console.log('squareRoot: ', squareRoot); // Absolute Value var absoluteValue = Math.abs(-4.7); // Print to log console.log('absoluteValue: ', absoluteValue); // Minimum value of an array var min = Math.min.apply(null, ages); // Print to log console.log('min: ', min); // Maximum value of an array var max = Math.max.apply(null, ages); // Print to log console.log('max: ', max); // Log var log = Math.log(numberOfEmployes); // Print to log console.log('log: ', log); // Exponent var exponent = Math.exp(numberOfEmployes); // Print to log console.log('exponent: ', exponent);","tags":"Javascript","url":"http://chrisalbon.com/javascript/basic-mathematics.html","loc":"http://chrisalbon.com/javascript/basic-mathematics.html"},{"title":"Bubble Plot","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add a x-axis myChart . addMeasureAxis ( \"x\" , \"Price Monthly Change\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Operating Profit\" ); // Add a z-axis (which will determine marker size) myChart . addMeasureAxis ( \"z\" , \"Unit Sales\" ); // Add the data to the chart, grouped by SKU and Channel var avg = myChart . addSeries ([ \"SKU\" , \"Channel\" ], dimple . plot . bubble ); // Add a legend myChart . addLegend ( 5 , 5 , 360 , 20 , \"right\" ); // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add a x-axis myChart.addMeasureAxis(\"x\", \"Price Monthly Change\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Operating Profit\"); // Add a z-axis (which will determine marker size) myChart.addMeasureAxis(\"z\", \"Unit Sales\"); // Add the data to the chart, grouped by SKU and Channel var avg = myChart.addSeries([\"SKU\", \"Channel\"], dimple.plot.bubble); // Add a legend myChart.addLegend(5, 5, 360, 20, \"right\"); // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/bubble-plot.html","loc":"http://chrisalbon.com/javascript/bubble-plot.html"},{"title":"Convert Strings To Numbers","text":"<!-- Load D3 Library --> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > <!-- Use D3 Library --> < script > // Create a variable with the url to the data var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\" ; // Load a CSV file d3 . csv ( url , function ( error , data ) { // Create a variable, map to each row/element in the data a function var mappedAndConverted = data . map ( function ( d ) { // Return return { // Convert episode to numeric and add Episode : + d . Episode , // Convert USViewers to numeric and add USViewers : + d . USViewers , // Add title Title : d . Title }; }); // Output to console console . log ( mappedAndConverted ); }); </ script > // Create a variable with the url to the data var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\"; // Load a CSV file d3.csv(url, function(error, data) { // Create a variable, map to each row/element in the data a function var mappedAndConverted = data.map(function(d) { // Return return { // Convert episode to numeric and add Episode: +d.Episode, // Convert USViewers to numeric and add USViewers: +d.USViewers, // Add title Title: d.Title }; }); // Output to console console.log(mappedAndConverted); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/convert-strings-to-numbers.html","loc":"http://chrisalbon.com/javascript/convert-strings-to-numbers.html"},{"title":"Dates And Times","text":"< script > // Create current date and time var current = new Date (); // Output to log console . log ( 'current:' , current ); // Specific date: Year, month (zero based!), day, hour, minute, second, and millisecond var birthday = new Date ( 1983 , 08 , 15 , 11 , 33 , 30 , 0 ); // Output to log console . log ( 'birthday:' , birthday ); // Date in string format var stringDate = new Date ( \"2015-11-21\" ); // Output to log, printing only the year, month, day, and day of the week console . log ( 'stringDate:' , stringDate . toDateString ()); </ script > // Create current date and time var current = new Date(); // Output to log console.log('current:', current); // Specific date: Year, month (zero based!), day, hour, minute, second, and millisecond var birthday = new Date(1983, 08, 15, 11, 33, 30, 0); // Output to log console.log('birthday:', birthday); // Date in string format var stringDate = new Date(\"2015-11-21\"); // Output to log, printing only the year, month, day, and day of the week console.log('stringDate:', stringDate.toDateString());","tags":"Javascript","url":"http://chrisalbon.com/javascript/dates-and-times.html","loc":"http://chrisalbon.com/javascript/dates-and-times.html"},{"title":"Displaying Outputs","text":"< p id = \"output\" ></ p > < script > // In the html document, select the element with the ID of 'output' and add 5+6 inside of the element document . getElementById ( \"output\" ). innerHTML = 5 + 6 ; // Output 5 + 6 to the console console . log ( 2 + 3 ); </ script > // In the html document, select the element with the ID of 'output' and add 5+6 inside of the element document.getElementById(\"output\").innerHTML = 5 + 6; // Output 5 + 6 to the console console.log(2 + 3);","tags":"Javascript","url":"http://chrisalbon.com/javascript/displaying-outputs.html","loc":"http://chrisalbon.com/javascript/displaying-outputs.html"},{"title":"For Loops And While Loops","text":"< script > // Create an array of employee names var employees = [ \"Steve Miller\" , \"Mark Jacob\" , \"Sam Stein\" , \"Judith Lack\" , \"Ado Stone\" , \"Bob Bill\" ]; // Create an empty array var upperCaseEmployees = []; // Create a variable for index var i ; // Start by setting i = 0 // If i is less than length of employees, run loop // After each loop add 1 to i for ( i = 0 ; i < employees . length ; i ++ ) { // Add uppercase versions of employee names upperCaseEmployees [ i ] = employees [ i ]. toUpperCase (); } // Output to log console . log ( \"upperCaseEmployees:\" , upperCaseEmployees ) // Create an empty array var lowerCaseEmployees = []; // Create a variable for index var j ; // For each element in the array for ( j in employees ) { // Change case and add to lowerCaseEmployees array lowerCaseEmployees [ j ] = employees [ j ]. toLowerCase (); } // Output to log console . log ( \"lowerCaseEmployees:\" , lowerCaseEmployees ) // Create an empty array var twoLowerCaseEmployees = []; // Create a variable for index var k = 0 ; // While k is less than 2 while ( k < 2 ) { // Change case and add to array twoLowerCaseEmployees [ k ] = employees [ k ]. toLowerCase (); // And add 1 to k k ++ ; } // Output to log console . log ( \"twoLowerCaseEmployees:\" , twoLowerCaseEmployees ) </ script > // Create an array of employee names var employees = [\"Steve Miller\", \"Mark Jacob\", \"Sam Stein\", \"Judith Lack\", \"Ado Stone\", \"Bob Bill\"]; // Create an empty array var upperCaseEmployees = []; // Create a variable for index var i; // Start by setting i = 0 // If i is less than length of employees, run loop // After each loop add 1 to i for (i = 0; i < employees.length; i++) { // Add uppercase versions of employee names upperCaseEmployees[i] = employees[i].toUpperCase(); } // Output to log console.log(\"upperCaseEmployees:\", upperCaseEmployees) // Create an empty array var lowerCaseEmployees = []; // Create a variable for index var j; // For each element in the array for (j in employees) { // Change case and add to lowerCaseEmployees array lowerCaseEmployees[j] = employees[j].toLowerCase(); } // Output to log console.log(\"lowerCaseEmployees:\", lowerCaseEmployees) // Create an empty array var twoLowerCaseEmployees = []; // Create a variable for index var k = 0; // While k is less than 2 while (k < 2) { // Change case and add to array twoLowerCaseEmployees[k] = employees[k].toLowerCase(); // And add 1 to k k++; } // Output to log console.log(\"twoLowerCaseEmployees:\", twoLowerCaseEmployees)","tags":"Javascript","url":"http://chrisalbon.com/javascript/for-loops-and-while-loops.html","loc":"http://chrisalbon.com/javascript/for-loops-and-while-loops.html"},{"title":"Functions","text":"< p id = \"demo\" ></ p > < script > // Create a function with two parameters, x and y function multiply ( x , y ) { // Return x multipled to y return x * y ; } // Call the function and insert the output inside the HTML element with ID=\"demo\" document . getElementById ( \"demo\" ). innerHTML = multiply ( 4 , 3 ); </ script > // Create a function with two parameters, x and y function multiply(x, y) { // Return x multipled to y return x * y; } // Call the function and insert the output inside the HTML element with ID=\"demo\" document.getElementById(\"demo\").innerHTML = multiply(4, 3);","tags":"Javascript","url":"http://chrisalbon.com/javascript/functions.html","loc":"http://chrisalbon.com/javascript/functions.html"},{"title":"Generate Random Numbers","text":"< script > // Create a function that takes a max and min function randomInteger ( min , max ) { // Returns the integer of a random number scaled by max and min, inclusively return Math . floor ( Math . random () * ( max - min + 1 )) + min ; }; // Print to console console . log ( 'randomInteger:' , randomInteger ( 0 , 100 )); // Create a function that takes a max and min function randomfloat ( min , max ) { // Returns the float of a random number scaled by max and min, exclusively return Math . random () * ( max - min ) + min ; }; // Print to console console . log ( 'randomfloat:' , randomfloat ( - 1 , 1 )); </ script > // Create a function that takes a max and min function randomInteger(min, max) { // Returns the integer of a random number scaled by max and min, inclusively return Math.floor(Math.random() * (max - min + 1)) + min; }; // Print to console console.log('randomInteger:', randomInteger(0,100)); // Create a function that takes a max and min function randomfloat(min, max) { // Returns the float of a random number scaled by max and min, exclusively return Math.random() * (max - min) + min; }; // Print to console console.log('randomfloat:', randomfloat(-1,1));","tags":"Javascript","url":"http://chrisalbon.com/javascript/generate-random-numbers.html","loc":"http://chrisalbon.com/javascript/generate-random-numbers.html"},{"title":"Grouped Bar Chart","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis that groups the data by two categorical variables myChart . addCategoryAxis ( \"x\" , [ \"Price Tier\" , \"Channel\" ]); // Add a y-axis that is the percent of unit sales. myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the data to the chart, grouped by \"Channel\" var avg = myChart . addSeries ( \"Channel\" , dimple . plot . bar ); // Calculate the average Unit Sales avg . aggregate = dimple . aggregateMethod . avg ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Add a legend 50% from the left, 5% from the top, 510 width and 20 height, with inside text aligned right myChart . addLegend ( \"50%\" , \"5%\" , 510 , 20 , \"right\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add an x-axis that groups the data by two categorical variables myChart.addCategoryAxis(\"x\", [\"Price Tier\", \"Channel\"]); // Add a y-axis that is the percent of unit sales. myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the data to the chart, grouped by \"Channel\" var avg = myChart.addSeries(\"Channel\", dimple.plot.bar); // Calculate the average Unit Sales avg.aggregate = dimple.aggregateMethod.avg; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Add a legend 50% from the left, 5% from the top, 510 width and 20 height, with inside text aligned right myChart.addLegend(\"50%\", \"5%\", 510, 20, \"right\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/grouped-bar-chart.html","loc":"http://chrisalbon.com/javascript/grouped-bar-chart.html"},{"title":"If... Else","text":"< script > // Create a function that takes a max and min function randomInteger ( min , max ) { // Returns the integer of a random number scaled by max and min, inclusively return Math . floor ( Math . random () * ( max - min + 1 )) + min ; }; // Create a random integer between 1 and 100 var price = randomInteger ( 1 , 100 ) // If price is less than two if ( price < 2 ) { // Set output variable output = \"Less than 2\" ; // Else, if price is between 2 and 10 } else if ( price > 2 && price < 10 ) { // Set output variable output = \"Between 2 and 10\" ; // Else, if price is exactly 16 } else if ( price == 16 ) { // Set output variable output = \"Exactly 16\" ; // Else, if price is either 30 or 31 } else if ( price == 30 || price == 31 ) { // Set output variable output = \"30 or 31\" ; // Else, if price is less than or equal to 20 } else if ( price <= 20 ) { // Set output variable output = \"Less than or equal to 20\" ; // Else if price is not equal to 21 } else if ( price != 21 ) { // Set output variable output = \"Not equal to 21\" ; // Otherwise } else { // Set output variable output = \"Some other number\" ; } // Output to console console . log ( output + ':' , price ) </ script > // Create a function that takes a max and min function randomInteger(min, max) { // Returns the integer of a random number scaled by max and min, inclusively return Math.floor(Math.random() * (max - min + 1)) + min; }; // Create a random integer between 1 and 100 var price = randomInteger(1, 100) // If price is less than two if (price < 2) { // Set output variable output = \"Less than 2\"; // Else, if price is between 2 and 10 } else if (price > 2 && price < 10) { // Set output variable output = \"Between 2 and 10\"; // Else, if price is exactly 16 } else if (price == 16) { // Set output variable output = \"Exactly 16\"; // Else, if price is either 30 or 31 } else if (price == 30 || price == 31) { // Set output variable output = \"30 or 31\"; // Else, if price is less than or equal to 20 } else if (price <= 20) { // Set output variable output = \"Less than or equal to 20\"; // Else if price is not equal to 21 } else if (price != 21) { // Set output variable output = \"Not equal to 21\"; // Otherwise } else { // Set output variable output = \"Some other number\"; } // Output to console console.log(output + ':', price)","tags":"Javascript","url":"http://chrisalbon.com/javascript/if-else.html","loc":"http://chrisalbon.com/javascript/if-else.html"},{"title":"Load A CSV File","text":"<!-- Load D3 Library --> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > <!-- Use D3 Library --> < script > // Create a variable with the url to the csv file var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\" ; // Load the csv file d3 . csv ( url , function ( error , data ) { // Output the first observation to the log console . log ( data [ 0 ]); }); </ script > // Create a variable with the url to the csv file var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\"; // Load the csv file d3.csv(url, function (error, data) { // Output the first observation to the log console.log(data[0]); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/load-a-csv-file.html","loc":"http://chrisalbon.com/javascript/load-a-csv-file.html"},{"title":"Load A JSON File","text":"<!-- Load D3 Library --> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > <!-- Use D3 Library --> < script > // Create a variable with the url to the JSON file var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.json\" ; // Load the json file d3 . json ( url , function ( error , data ) { // Output the first observation to the log console . log ( data [ 0 ]); }); </ script > // Create a variable with the url to the JSON file var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.json\"; // Load the json file d3.json(url, function(error, data) { // Output the first observation to the log console.log(data[0]); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/load-a-json-file.html","loc":"http://chrisalbon.com/javascript/load-a-json-file.html"},{"title":"Manipulating Arrays","text":"< script > // Create some arrays var names = [ 'Steve' , 'Bob' , 'Jack' , 'Abe' , 'Sven' ] var ages = [ 32 , 12 , 52 , 69 , 93 , 23 , 94 ]; var moreAges = [ 32 , 50 , 29 , 3 ] // Pop out last element in array console . log ( 'ages.pop():' , ages . pop ()); // Remove the first element of the array console . log ( 'ages.shift():' , ages . shift ()); // Push in a new element at the end of the array console . log ( 'ages.push(32):' , ages . push ( 32 )); // Push in a new element at the start of the array console . log ( 'ages.unshift(23)' , ages . unshift ( 23 )); // Deleting element of the array delete ages [ 2 ]; // Concatenating two arrays console . log ( 'ages.concat(moreAges):' , ages . concat ( moreAges )); // Sort the array alphabetically console . log ( 'names.sort():' , names . sort ()); // Sort the array reverse alphabetically console . log ( 'names.reverse():' , names . reverse ()); // Sort ascending console . log ( 'ages.sort(function(a, b){return a - b}):' , ages . sort ( function ( a , b ){ return a - b })); // Sort decending console . log ( 'ages.sort(function(a, b){return b - a}):' , ages . sort ( function ( a , b ){ return b - a })); </ script > // Create some arrays var names = ['Steve', 'Bob', 'Jack', 'Abe', 'Sven'] var ages = [32,12,52,69,93,23,94]; var moreAges = [32,50,29,3] // Pop out last element in array console.log('ages.pop():', ages.pop()); // Remove the first element of the array console.log('ages.shift():', ages.shift()); // Push in a new element at the end of the array console.log('ages.push(32):', ages.push(32)); // Push in a new element at the start of the array console.log('ages.unshift(23)', ages.unshift(23)); // Deleting element of the array delete ages[2]; // Concatenating two arrays console.log('ages.concat(moreAges):', ages.concat(moreAges)); // Sort the array alphabetically console.log('names.sort():', names.sort()); // Sort the array reverse alphabetically console.log('names.reverse():', names.reverse()); // Sort ascending console.log('ages.sort(function(a, b){return a - b}):', ages.sort(function(a, b){return a - b})); // Sort decending console.log('ages.sort(function(a, b){return b - a}):', ages.sort(function(a, b){return b - a}));","tags":"Javascript","url":"http://chrisalbon.com/javascript/manipulating-arrays.html","loc":"http://chrisalbon.com/javascript/manipulating-arrays.html"},{"title":"Multiline Plot","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis of time, grouped by Month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x-axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the actual data, grouped by owner var s = myChart . addSeries ( 'Owner' , dimple . plot . line ); // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a chart var myChart = new dimple.chart(svg, data); // Add an x-axis of time, grouped by Month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x-axis by date x.addOrderRule(\"Date\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the actual data, grouped by owner var s = myChart.addSeries('Owner', dimple.plot.line); // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/multiline-plot.html","loc":"http://chrisalbon.com/javascript/multiline-plot.html"},{"title":"Objects","text":"< script > // Create an object called army var army = { // With key value pair with a string name : '5th Army' , // And a nested object vehicles : { trucks : 4 , tanks : 6 , aircraft : 10 , helicpters : 12 , }, }; // View the number of trucks in the army console . log ( 'Number of trucks:' , army . vehicles . trucks ) </ script > // Create an object called army var army = { // With key value pair with a string name: '5th Army', // And a nested object vehicles: { trucks: 4, tanks: 6, aircraft: 10, helicpters: 12, }, }; // View the number of trucks in the army console.log('Number of trucks:', army.vehicles.trucks)","tags":"Javascript","url":"http://chrisalbon.com/javascript/objects.html","loc":"http://chrisalbon.com/javascript/objects.html"},{"title":"Proportional Stacked Bar Chart","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis that groups the data by month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis that is the percent of unit sales. myChart . addPctAxis ( \"y\" , \"Unit Sales\" ); // Add the data to the chart, grouped by \"Channel\" var avg = myChart . addSeries ( \"Channel\" , dimple . plot . bar ); // Calculate the average Unit Sales avg . aggregate = dimple . aggregateMethod . avg ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add an x-axis that groups the data by month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x axis by date x.addOrderRule(\"Date\"); // Add a y-axis that is the percent of unit sales. myChart.addPctAxis(\"y\", \"Unit Sales\"); // Add the data to the chart, grouped by \"Channel\" var avg = myChart.addSeries(\"Channel\", dimple.plot.bar); // Calculate the average Unit Sales avg.aggregate = dimple.aggregateMethod.avg; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/proportional-stacked-bar-chart.html","loc":"http://chrisalbon.com/javascript/proportional-stacked-bar-chart.html"},{"title":"Replacing Strings","text":"< p id = \"output\" ></ p > < script > // Create a string var string = \"The army will attack at dawn! Go army!\" ; // Create new string that replaces ALL (i.e. globally) instances of 'army' with 'navy' var newString = string . replace ( /army/g , \"Navy\" ); // Insert the position inside the HTML element with ID=\"output\" document . getElementById ( \"output\" ). innerHTML = newString ; </ script > // Create a string var string = \"The army will attack at dawn! Go army!\"; // Create new string that replaces ALL (i.e. globally) instances of 'army' with 'navy' var newString = string.replace(/army/g, \"Navy\"); // Insert the position inside the HTML element with ID=\"output\" document.getElementById(\"output\").innerHTML = newString;","tags":"Javascript","url":"http://chrisalbon.com/javascript/replacing-strings.html","loc":"http://chrisalbon.com/javascript/replacing-strings.html"},{"title":"Scale Numeric Data To Pixels","text":"<!-- Load D3 Library --> < script src = \"http://d3js.org/d3.v3.min.js\" charset = \"utf-8\" ></ script > <!-- Use D3 Library --> < script > // Create a variable with the url to the data var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\" ; // Load csv data d3 . csv ( url , function ( error , data ) { // Create a function that converts USViewers to numeric var convertToNumeric = data . map ( function ( d ) { return { USViewers : + d . USViewers , }; }); // Convert viewership to numeric var viewership = convertToNumeric . map ( function ( d ) { return d . USViewers ; }); // Find the smallest viewership var minViewership = d3 . min ( viewership ); // Find the largest viewership var maxViewership = d3 . max ( viewership ); // Set the minimum bar height var minBarHeight = 100 // Set the maxiumum bar height var maxBarHeight = 400 ; // Create a variable that scales data to pixels var yScale = d3 . scale . linear () // Input min and max . domain ([ minViewership , maxViewership ]) // Output min and max . range ([ minBarHeight , maxBarHeight ]); // As an example, scale a certain viewership value into pixels console . log ( \"Input in viewership: \" + 15000000 , \"Output in pixels: \" + yScale ( 15000000 )); }); </ script > // Create a variable with the url to the data var url = \"https://gist.githubusercontent.com/d3byex/e5ce6526ba2208014379/raw/8fefb14cc18f0440dc00248f23cbf6aec80dcc13/walking_dead_s5.csv\"; // Load csv data d3.csv(url, function(error, data) { // Create a function that converts USViewers to numeric var convertToNumeric = data.map(function(d) { return { USViewers: +d.USViewers, }; }); // Convert viewership to numeric var viewership = convertToNumeric.map(function(d) { return d.USViewers; }); // Find the smallest viewership var minViewership = d3.min(viewership); // Find the largest viewership var maxViewership = d3.max(viewership); // Set the minimum bar height var minBarHeight = 100 // Set the maxiumum bar height var maxBarHeight = 400; // Create a variable that scales data to pixels var yScale = d3.scale .linear() // Input min and max .domain([minViewership, maxViewership]) // Output min and max .range([minBarHeight, maxBarHeight]); // As an example, scale a certain viewership value into pixels console.log(\"Input in viewership: \" + 15000000, \"Output in pixels: \" + yScale(15000000)); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/scale-numeric-data-to-pixels.html","loc":"http://chrisalbon.com/javascript/scale-numeric-data-to-pixels.html"},{"title":"Scatter Plot","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add a x-axis myChart . addMeasureAxis ( \"x\" , \"Unit Sales\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Operating Profit\" ); // Add the data to the chart, grouped by SKU and Channel var avg = myChart . addSeries ([ \"SKU\" , \"Channel\" ], dimple . plot . bubble ); // Add a legend myChart . addLegend ( 5 , 5 , 360 , 20 , \"right\" ); // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add a x-axis myChart.addMeasureAxis(\"x\", \"Unit Sales\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Operating Profit\"); // Add the data to the chart, grouped by SKU and Channel var avg = myChart.addSeries([\"SKU\", \"Channel\"], dimple.plot.bubble); // Add a legend myChart.addLegend(5, 5, 360, 20, \"right\"); // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/scatter-plot.html","loc":"http://chrisalbon.com/javascript/scatter-plot.html"},{"title":"Searching Strings","text":"< p id = \"output\" ></ p > < script > // Create a string var string = \"The army will attack at dawn!\" ; // Return the index of `string` where locate occurs var position = string . search ( \"dawn\" ); // Insert the position inside the HTML element with ID=\"output\" document . getElementById ( \"output\" ). innerHTML = position ; </ script > // Create a string var string = \"The army will attack at dawn!\"; // Return the index of `string` where locate occurs var position = string.search(\"dawn\"); // Insert the position inside the HTML element with ID=\"output\" document.getElementById(\"output\").innerHTML = position;","tags":"Javascript","url":"http://chrisalbon.com/javascript/searching-strings.html","loc":"http://chrisalbon.com/javascript/searching-strings.html"},{"title":"Slicing Strings","text":"< p id = \"output\" ></ p > < script > // Create a string var string = \"The army will attack at dawn!\" ; // Create substring that is one from the end to five from the end var substring = string . slice ( - 5 , - 1 ); // Insert the position inside the HTML element with ID=\"output\" document . getElementById ( \"output\" ). innerHTML = substring ; </ script > // Create a string var string = \"The army will attack at dawn!\"; // Create substring that is one from the end to five from the end var substring = string.slice(-5, -1); // Insert the position inside the HTML element with ID=\"output\" document.getElementById(\"output\").innerHTML = substring;","tags":"Javascript","url":"http://chrisalbon.com/javascript/slicing-strings.html","loc":"http://chrisalbon.com/javascript/slicing-strings.html"},{"title":"Stacked Area Plots","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple . filterData ( data , \"Owner\" , [ \"Aperture\" , \"Black Mesa\" ]) // Create a chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis of time, grouped by Month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x-axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the actual data, grouped by channel var s = myChart . addSeries ( \"Channel\" , dimple . plot . area ); // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple.filterData(data, \"Owner\", [\"Aperture\", \"Black Mesa\"]) // Create a chart var myChart = new dimple.chart(svg, data); // Add an x-axis of time, grouped by Month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x-axis by date x.addOrderRule(\"Date\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the actual data, grouped by channel var s = myChart.addSeries(\"Channel\", dimple.plot.area); // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/stacked-area-plots.html","loc":"http://chrisalbon.com/javascript/stacked-area-plots.html"},{"title":"Stacked Bar Chart","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a new chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis that groups the data by month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis for the variable \"Unit Sales\" myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the data to the chart, grouped by \"Channel\" var avg = myChart . addSeries ( \"Channel\" , dimple . plot . bar ); // Calculate the average Unit Sales avg . aggregate = dimple . aggregateMethod . avg ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a new chart var myChart = new dimple.chart(svg, data); // Add an x-axis that groups the data by month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x axis by date x.addOrderRule(\"Date\"); // Add a y-axis for the variable \"Unit Sales\" myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the data to the chart, grouped by \"Channel\" var avg = myChart.addSeries(\"Channel\", dimple.plot.bar); // Calculate the average Unit Sales avg.aggregate = dimple.aggregateMethod.avg; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/stacked-bar-chart.html","loc":"http://chrisalbon.com/javascript/stacked-bar-chart.html"},{"title":"Stacked Step Area Plot","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Create a chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis of time, grouped by Month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x-axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the actual data, grouped by owner var s = myChart . addSeries ( null , dimple . plot . area ); // Display as a step chart s . interpolation = \"step\" ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Create a chart var myChart = new dimple.chart(svg, data); // Add an x-axis of time, grouped by Month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x-axis by date x.addOrderRule(\"Date\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the actual data, grouped by owner var s = myChart.addSeries(null, dimple.plot.area); // Display as a step chart s.interpolation = \"step\"; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/stacked-step-area-plot.html","loc":"http://chrisalbon.com/javascript/stacked-step-area-plot.html"},{"title":"Stacked Step Area Plots","text":"This code uses D3 and Dimple.js . <!-- Create an element for our chart --> < div id = \"chart\" style = \"height: 400px; width: 100%;\" ></ div > <!-- Load D3 --> < script src = \"https://d3js.org/d3.v4.min.js\" ></ script > <!-- Load Dimple --> < script src = \"https://cdnjs.cloudflare.com/ajax/libs/dimple/2.3.0/dimple.latest.min.js\" ></ script > <!-- Create a dimble chart --> < script type = \"text/javascript\" > // Create a new svg variable var svg = dimple . newSvg ( \"#chart\" , \"100%\" , \"100%\" ); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3 . tsv ( url , function ( data ) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple . filterData ( data , \"Owner\" , [ \"Aperture\" , \"Black Mesa\" ]) // Create a chart var myChart = new dimple . chart ( svg , data ); // Add an x-axis of time, grouped by Month var x = myChart . addCategoryAxis ( \"x\" , \"Month\" ); // Order the x-axis by date x . addOrderRule ( \"Date\" ); // Add a y-axis myChart . addMeasureAxis ( \"y\" , \"Unit Sales\" ); // Add the actual data, grouped by owner var s = myChart . addSeries ( 'Owner' , dimple . plot . area ); // Display as a step chart s . interpolation = \"step\" ; // Add some margins myChart . setMargins ( \"10%\" , \"10%\" , \"10%\" , \"20%\" ); // Draw the chart myChart . draw (); }); </ script > // Create a new svg variable var svg = dimple.newSvg(\"#chart\", \"100%\", \"100%\"); // Create a url to the data var url = \"https://raw.githubusercontent.com/chrisalbon/learn_js/master/data/example_data.tsv\" // Load the data d3.tsv(url, function(data) { // Filter the data so we only see data where Owner = Aperture or Black Mesa data = dimple.filterData(data, \"Owner\", [\"Aperture\", \"Black Mesa\"]) // Create a chart var myChart = new dimple.chart(svg, data); // Add an x-axis of time, grouped by Month var x = myChart.addCategoryAxis(\"x\", \"Month\"); // Order the x-axis by date x.addOrderRule(\"Date\"); // Add a y-axis myChart.addMeasureAxis(\"y\", \"Unit Sales\"); // Add the actual data, grouped by owner var s = myChart.addSeries('Owner', dimple.plot.area); // Display as a step chart s.interpolation = \"step\"; // Add some margins myChart.setMargins(\"10%\", \"10%\", \"10%\", \"20%\"); // Draw the chart myChart.draw(); });","tags":"Javascript","url":"http://chrisalbon.com/javascript/stacked-step-area-plots.html","loc":"http://chrisalbon.com/javascript/stacked-step-area-plots.html"},{"title":"While Loops","text":"< script > // You start with 50 bitcoins var bitcoins = 50 ; // While bitcoins is more than 1 and less than 100 while ( bitcoins > 0 && bitcoins < 100 ) { // Loop over and over, each time subtracting 1 bitcoin bitcoins = bitcoins - 1 } // View the final number of bitcoins console . log ( 'Number of bitcoins:' , bitcoins ) </ script > // You start with 50 bitcoins var bitcoins = 50; // While bitcoins is more than 1 and less than 100 while (bitcoins > 0 && bitcoins < 100) { // Loop over and over, each time subtracting 1 bitcoin bitcoins = bitcoins - 1 } // View the final number of bitcoins console.log('Number of bitcoins:', bitcoins)","tags":"Javascript","url":"http://chrisalbon.com/javascript/while-loops.html","loc":"http://chrisalbon.com/javascript/while-loops.html"},{"title":"Add Padding Around String","text":"Create Some Text text = 'Chapter 1' Add Padding Around Text # Add Spaces Of Padding To The Left format ( text , '>20' ) ' Chapter 1' # Add Spaces Of Padding To The Right format ( text , '<20' ) 'Chapter 1 ' # Add Spaces Of Padding On Each Side format ( text , '&#94;20' ) ' Chapter 1 ' # Add * Of Padding On Each Side format ( text , '*&#94;20' ) '*****Chapter 1******'","tags":"Python","url":"http://chrisalbon.com/python/add_padding_around_string.html","loc":"http://chrisalbon.com/python/add_padding_around_string.html"},{"title":"Chain Together Lists","text":"Preliminaries from itertools import chain Create Two Lists # Create a list of allies allies = [ 'Spain' , 'Germany' , 'Namibia' , 'Austria' ] # Create a list of enemies enemies = [ 'Mexico' , 'United Kingdom' , 'France' ] Iterate Over Both Lists As A Single Sequence # For each country in allies and enemies for country in chain ( allies , enemies ): # print the country print ( country ) Spain Germany Namibia Austria Mexico United Kingdom France","tags":"Python","url":"http://chrisalbon.com/python/chain_together_lists.html","loc":"http://chrisalbon.com/python/chain_together_lists.html"},{"title":"Compare Two Dictionaries","text":"One of the great features of Python dictionaries is that they are hashtables, meaning we can do some operations at O(1) time-complexity. Make Two Dictionaries importers = { 'El Salvador' : 1234 , 'Nicaragua' : 152 , 'Spain' : 252 } exporters = { 'Spain' : 252 , 'Germany' : 251 , 'Italy' : 1563 } Find Duplicate Keys # Find the intersection of importers and exporters importers . keys () & exporters . keys () {'Spain'} Find Difference In Keys # Find the difference between importers and exporters importers . keys () - exporters . keys () {'El Salvador', 'Nicaragua'} Find Key, Values Pairs In Common # Find countries where the amount of exports matches the amount of imports importers . items () & exporters . items () {('Spain', 252)}","tags":"Python","url":"http://chrisalbon.com/python/compare_two_dictionaries.html","loc":"http://chrisalbon.com/python/compare_two_dictionaries.html"},{"title":"Convert HTML Characters To Strings","text":"## Preliminaries import html ## Create Text text = 'This item costs &#165;400 or &#163;4.' ## Convert To String html . unescape ( text ) 'This item costs ¥400 or £4.' ## Convert To HTML Entities html . escape ( text ) 'This item costs &amp;#165;400 or &amp;#163;4.'","tags":"Python","url":"http://chrisalbon.com/python/convert_html_symbols_to_strings.html","loc":"http://chrisalbon.com/python/convert_html_symbols_to_strings.html"},{"title":"Create A New File Then Write To It","text":"Create A New File And Write To It # Create a file if it doesn't already exist with open ( 'file.txt' , 'xt' ) as f : # Write to the file f . write ( 'This file now exsits!' ) # Close the connection to the file f . close () Open The File And Read It # Open the file with open ( 'file.txt' , 'rt' ) as f : # Read the data in the file data = f . read () # Close the connection to the file f . close () View The Contents Of The File # View the data in the file data 'This file now exsits!' Delete The File # Import the os package import os # Delete the file os . remove ( 'file.txt' )","tags":"Python","url":"http://chrisalbon.com/python/create_a_new_file_and_the_write_to_it.html","loc":"http://chrisalbon.com/python/create_a_new_file_and_the_write_to_it.html"},{"title":"Create A Temporary File","text":"Preliminaries from tempfile import NamedTemporaryFile Create A Temporary File f = NamedTemporaryFile ( 'w+t' ) Write To The Temp File # Write to the file, the output is the number of characters f . write ( 'Nobody lived on Deadweather but us and the pirates. It wasn't hard to understand why.' ) 85 View The Tmp File's Name f . name '/var/folders/0b/pj3wsd750fjf8xzfb0n127w80000gn/T/tmphv1dkovx' Read The File # Go to the top of the file f . seek ( 0 ) # Read the file f . read () 'Nobody lived on Deadweather but us and the pirates. It wasn't hard to understand why.' Close (And Thus Delete) The File f . close ()","tags":"Python","url":"http://chrisalbon.com/python/create_a_temporary_file.html","loc":"http://chrisalbon.com/python/create_a_temporary_file.html"},{"title":"Find The Max Value In A Dictionary","text":"Create A Dictionary ages = { 'John' : 21 , 'Mike' : 52 , 'Sarah' : 12 , 'Bob' : 43 } Find The Maximum Value Of The Values max ( zip ( ages . values (), ages . keys ())) (52, 'Mike')","tags":"Python","url":"http://chrisalbon.com/python/find_the_max_value_in_a_dictionary.html","loc":"http://chrisalbon.com/python/find_the_max_value_in_a_dictionary.html"},{"title":"Formatting Numbers","text":"Create A Long Number annual_revenue = 9282904.9282872782 Format Number # Format rounded to two decimal places format ( annual_revenue , '0.2f' ) '9282904.93' # Format with commas and rounded to one decimal place format ( annual_revenue , '0,.1f' ) '9,282,904.9' # Format as scientific notation format ( annual_revenue , 'e' ) '9.282905e+06' # Format as scientific notation rounded to two deciminals format ( annual_revenue , '0.2E' ) '9.28E+06'","tags":"Python","url":"http://chrisalbon.com/python/formatting_numbers.html","loc":"http://chrisalbon.com/python/formatting_numbers.html"},{"title":"Hard Wrapping Text","text":"Preliminaries import textwrap Create Text # Create some text excerpt = 'Then there was the bad weather. It would come in one day when the fall was over. We would have to shut the windows in the night against the rain and the cold wind would strip the leaves from the trees in the Place Contrescarpe. The leaves lay sodden in the rain and the wind drove the rain against the big green autobus at the terminal and the Café des Amateurs was crowded and the windows misted over from the heat and the smoke inside.' Hard Wrap Text # Hard wrap the excerpt at 50 characters print ( textwrap . fill ( excerpt , 50 )) Then there was the bad weather. It would come in one day when the fall was over. We would have to shut the windows in the night against the rain and the cold wind would strip the leaves from the trees in the Place Contrescarpe. The leaves lay sodden in the rain and the wind drove the rain against the big green autobus at the terminal and the Café des Amateurs was crowded and the windows misted over from the heat and the smoke inside.","tags":"Python","url":"http://chrisalbon.com/python/hard_wrapping_text.html","loc":"http://chrisalbon.com/python/hard_wrapping_text.html"},{"title":"Iterate Over Multiple Lists Simultaneously","text":"Create Two Lists names = [ 'James' , 'Bob' , 'Sarah' , 'Marco' , 'Nancy' , 'Sally' ] ages = [ 42 , 13 , 14 , 25 , 63 , 23 ] Iterate Over Both Lists At Once for name , age in zip ( names , ages ): print ( name , age ) James 42 Bob 13 Sarah 14 Marco 25 Nancy 63 Sally 23","tags":"Python","url":"http://chrisalbon.com/python/iterate_over_multiple_lists_simultaneously.html","loc":"http://chrisalbon.com/python/iterate_over_multiple_lists_simultaneously.html"},{"title":"Priority Queues","text":"Preliminaries import heapq Create A Priority Queue Object # Create a priority queue abstract base class class priority_queue : # Initialize the instance def __init__ ( self ): # Create a list to use as the queue self . _queue = [] # Create an index to use as ordering self . _index = 0 # Create a function to add a task to the queue def add_task ( self , item , priority ): # Push the arguments to the _queue using a heap heapq . heappush ( self . _queue , ( - priority , self . _index , item )) # Add one to the index self . _index += 1 # Create a function to get the next item from the queue def next_task ( self ): # Return the next item in the queue return heapq . heappop ( self . _queue )[ - 1 ] # Create a priority queue called task_list task_list = priority_queue () Add Items To Queue # Add an item to the queue task_list . add_task ( 'Clean Dishes' , 1 ) # Add an item to the queue task_list . add_task ( 'Wash Car' , 2 ) # Add an item to the queue task_list . add_task ( 'Walk Dog' , 3 ) Retrieve Items From Queue By Priority # Retrieve items from the queue task_list . next_task () 'Walk Dog' # Retrieve items from the queue task_list . next_task () 'Wash Car' # Retrieve items from the queue task_list . next_task () 'Clean Dishes'","tags":"Python","url":"http://chrisalbon.com/python/priority_queues.html","loc":"http://chrisalbon.com/python/priority_queues.html"},{"title":"Streaming Data Pipeline","text":"Create Some Raw Data raw_data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] Create Data Processing Functions # Define a generator that yields input+6 def add_6 ( numbers ): for x in numbers : output = x + 6 yield output # Define a generator that yields input-2 def subtract_2 ( numbers ): for x in numbers : output = x - 2 yield output # Define a generator that yields input*100 def multiply_by_100 ( numbers ): for x in numbers : output = x * 100 yield output Create Data Pipeline # Step 1 of the pipeline step1 = add_6 ( raw_data ) # Step 2 of the pipeline step2 = subtract_2 ( step1 ) # Step 3 of the pipeline pipeline = multiply_by_100 ( step2 ) Send First Two Pieces Of Raw Data Through Pipeline # First element of the raw data next ( pipeline ) 500 # Second element of the raw data next ( pipeline ) 600 Send All Raw Data Through Pipeline # Process all data for raw_data in pipeline : print ( raw_data ) 700 800 900 1000 1100 1200 1300 1400","tags":"Python","url":"http://chrisalbon.com/python/streaming_data_pipeline.html","loc":"http://chrisalbon.com/python/streaming_data_pipeline.html"},{"title":"Test For A Specific Exception","text":"Preliminaries import unittest Create A Function To Test def add ( x , y ): return x + y Create Test Case # Create a test case class TestAdd ( unittest . TestCase ): # Create the unit test def test_input_string ( self ): # Test To make sure a TypeError exception is raised self . assertRaises ( TypeError , add ( 'Banana' , 'Boat' )) Run Test # Run the unit test (and don't shut down the Jupyter Notebook) unittest . main ( argv = [ 'ignored' , '-v' ], exit = False ) test_input_string (__main__.TestAdd) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK <unittest.main.TestProgram at 0x104855320>","tags":"Python","url":"http://chrisalbon.com/python/test_for_a_specific_exception.html","loc":"http://chrisalbon.com/python/test_for_a_specific_exception.html"},{"title":"Create A Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create A Table %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Select Everything In That Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/create_a_table.html","loc":"http://chrisalbon.com/sql/create_a_table.html"},{"title":"Add New Row To Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 Add New Row %% sql -- Add into the table criminals INSERT INTO criminals -- A new row with these values VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View Table Again %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0","tags":"SQL","url":"http://chrisalbon.com/sql/add_new_row_into_table.html","loc":"http://chrisalbon.com/sql/add_new_row_into_table.html"},{"title":"Using Aliases","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Alias Criminals Table A C , Then Select All Names From C %% sql -- Select all names from the table 'c' SELECT c . name -- From the criminals table , now called c FROM criminals AS c name James Smith Bill James Stacy Miller Betty Bob Jaden Ado Gordon Ado Bill Byson Bob Iton","tags":"SQL","url":"http://chrisalbon.com/sql/aliases.html","loc":"http://chrisalbon.com/sql/aliases.html"},{"title":"Commenting SQL Code","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , NULL , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , NULL , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] Write Some SQL Code With Single And Multiline Comments %% sql -- This is a single line of commenting SELECT name , age FROM criminals -- It can also be placed at the end of the line /* This is multiple lines of comments so we can include more details if we need to . */ WHERE name IS NOT NULL name age James Smith 15 Gordon Ado 32 Bill Byson 21","tags":"SQL","url":"http://chrisalbon.com/sql/commenting_sql_code.html","loc":"http://chrisalbon.com/sql/commenting_sql_code.html"},{"title":"Drop Rows","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Drop Row Based On A Conditional %% sql -- Delete all rows DELETE FROM criminals -- if the age is less than 18 WHERE age < 18 [] View Table Again %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/drop_rows.html","loc":"http://chrisalbon.com/sql/drop_rows.html"},{"title":"Edit Tables","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 Update One Row %% sql -- Update the criminals table UPDATE criminals -- To say city : 'Palo Alto' SET City = 'Palo Alto' -- If the prisoner ID number is 412 WHERE pid = 412 ; [] Update Multiple Rows Using A Conditional %% sql -- Update the criminals table UPDATE criminals -- To say minor : 'No' SET minor = 'No' -- If age is greater than 12 WHERE age > 12 ; [] View Table Again %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Palo Alto No 901 Gordon Ado 32 F San Francisco No 512 Bill Byson 21 M Petaluma No","tags":"SQL","url":"http://chrisalbon.com/sql/edit_tables.html","loc":"http://chrisalbon.com/sql/edit_tables.html"},{"title":"Ignoring Null or Missing Values","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , NULL , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , NULL , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] Select Name And Ages Only When The Name Is Known %% sql -- Select name and average age , SELECT name , age -- from the table 'criminals' , FROM criminals -- if age is not a null value WHERE name IS NOT NULL name age James Smith 15 Gordon Ado 32 Bill Byson 21","tags":"SQL","url":"http://chrisalbon.com/sql/ignoring_null_values.html","loc":"http://chrisalbon.com/sql/ignoring_null_values.html"},{"title":"Make Simulated Data For Classification","text":"Preliminaries from sklearn.datasets import make_classification import pandas as pd Create Simulated Data # Create a simulated feature matrix and output vector with 100 samples, features , output = make_classification ( n_samples = 100 , # ten features n_features = 10 , # five features that actually predict the output's classes n_informative = 5 , # five features that are random and unrelated to the output's classes n_redundant = 5 , # three output classes n_classes = 3 , # with 20% of observations in the first class, 30% in the second class, # and 50% in the third class. ('None' makes balanced classes) weights = [ . 2 , . 3 , . 8 ]) View Data # View the first five observations and their 10 features pd . DataFrame ( features ) . head () 0 1 2 3 4 5 6 7 8 9 0 -1.338796 2.218025 3.333541 2.586772 -2.050240 -5.289060 4.364050 3.010074 3.073564 0.827317 1 1.535519 1.964163 -0.053789 0.610150 -4.256450 -6.044707 7.617702 4.654903 0.632368 3.234648 2 0.249576 -4.051890 -4.578764 -1.629710 2.188123 1.488968 -1.977744 -2.888737 -4.957220 3.599833 3 3.778789 -4.797895 -1.187821 0.724315 1.083952 0.165924 -0.352818 0.615942 -4.392519 1.683278 4 0.856266 0.568888 -0.520666 -1.970701 0.597743 2.224923 0.065515 0.250906 -1.512495 -0.859869 # View the first five observation's classes pd . DataFrame ( output ) . head () 0 0 2 1 2 2 1 3 2 4 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/make_simulated_data_for_classification.html","loc":"http://chrisalbon.com/machine-learning/make_simulated_data_for_classification.html"},{"title":"Make Simulated Data For Clustering","text":"Inspired by Python Machine Learning Preliminaries from sklearn.datasets import make_blobs import matplotlib.pyplot as plt Make Data # Make the features (X) and output (y) with 200 samples, X , y = make_blobs ( n_samples = 200 , # two feature variables, n_features = 2 , # three clusters, centers = 3 , # with .5 cluster standard deviation, cluster_std = 0.5 , # shuffled, shuffle = True ) View Data # Create a scatterplot of the first and second features plt . scatter ( X [:, 0 ], X [:, 1 ]) # Show the scatterplot plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/make_simulated_data_for_clustering.html","loc":"http://chrisalbon.com/machine-learning/make_simulated_data_for_clustering.html"},{"title":"Make Simulated Data For Regression","text":"Preliminaries import pandas as pd from sklearn.datasets import make_regression Create Simulated Data # Generate fetures, outputs, and true coefficient of 100 samples, features , output , coef = make_regression ( n_samples = 100 , # three features n_features = 3 , # where only two features are useful, n_informative = 2 , # a single target value per observation n_targets = 1 , # 0.0 standard deviation of the guassian noise noise = 0.0 , # show the true coefficient used to generated the data coef = True ) View Simulated Data # View the features of the first five rows pd . DataFrame ( features , columns = [ 'Store 1' , 'Store 2' , 'Store 3' ]) . head () Store 1 Store 2 Store 3 0 -0.166697 -0.177142 -2.329568 1 -0.093566 -0.544292 0.685165 2 0.625958 -0.193049 1.168012 3 -0.843925 -0.567444 -0.193631 4 -1.079227 -0.819236 1.609171 # View the output of the first five rows pd . DataFrame ( output , columns = [ 'Sales' ]) . head () Sales 0 -149.387162 1 -4.164344 2 52.166904 3 -56.996180 4 27.246575 # View the actual, true coefficients used to generate the data pd . DataFrame ( coef , columns = [ 'True Coefficient Values' ]) True Coefficient Values 0 0.000000 1 80.654346 2 57.993548","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/make_simulated_data_for_regression.html","loc":"http://chrisalbon.com/machine-learning/make_simulated_data_for_regression.html"},{"title":"Multiple Conditional Statements","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View All Rows %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 412 James Smith 15 M Santa Rosa 1 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 View Rows Where Age Is Greater Than 20 And City Is San Francisco %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 20 and city is San Francisco WHERE age > 20 AND city == 'San Francisco' pid name age sex city minor 901 Gordon Ado 32 F San Francisco 0 View Rows Where Age Is Greater Than 20 or City Is San Francisco %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 20 and city is San Francisco WHERE age > 20 OR city == 'San Francisco' pid name age sex city minor 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0","tags":"SQL","url":"http://chrisalbon.com/sql/multiple_conditional_statements.html","loc":"http://chrisalbon.com/sql/multiple_conditional_statements.html"},{"title":"Nested Select","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Select Based On The Result Of A Select %% sql -- Select name and age , SELECT name , age -- from the table 'criminals' , FROM criminals -- where age is greater than , WHERE age > -- select age , ( SELECT age -- from criminals FROM criminals -- where the name is 'James Smith' WHERE name == 'James Smith' ) name age Bill James 22 Stacy Miller 23 Jaden Ado 49 Gordon Ado 32 Bill Byson 21","tags":"SQL","url":"http://chrisalbon.com/sql/nested_select.html","loc":"http://chrisalbon.com/sql/nested_select.html"},{"title":"Assign A New Column To A Pandas DataFrame","text":"Preliminaries import pandas as pd Create Dataframe # Create empty dataframe df = pd . DataFrame () # Create a column df [ 'name' ] = [ 'John' , 'Steve' , 'Sarah' ] # View dataframe df name 0 John 1 Steve 2 Sarah Assign New Column To Dataframe # Assign a new column to df called 'age' with a list of ages df . assign ( age = [ 31 , 32 , 19 ]) name age 0 John 31 1 Steve 32 2 Sarah 19","tags":"Python","url":"http://chrisalbon.com/python/pandas_assign_new_column_dataframe.html","loc":"http://chrisalbon.com/python/pandas_assign_new_column_dataframe.html"},{"title":"Create A Pipeline In Pandas","text":"Pandas' pipeline feature allows you to string together Python functions in order to build a pipeline of data processing. Preliminaries import pandas as pd Create Dataframe # Create empty dataframe df = pd . DataFrame () # Create a column df [ 'name' ] = [ 'John' , 'Steve' , 'Sarah' ] df [ 'gender' ] = [ 'Male' , 'Male' , 'Female' ] df [ 'age' ] = [ 31 , 32 , 19 ] # View dataframe df name gender age 0 John Male 31 1 Steve Male 32 2 Sarah Female 19 Create Functions To Process Data # Create a function that def mean_age_by_group ( dataframe , col ): # groups the data by a column and returns the mean age per group return dataframe . groupby ( col ) . mean () # Create a function that def uppercase_column_name ( dataframe ): # Capitalizes all the column headers dataframe . columns = dataframe . columns . str . upper () # And returns them return dataframe Create A Pipeline Of Those Functions # Create a pipeline that applies the mean_age_by_group function ( df . pipe ( mean_age_by_group , col = 'gender' ) # then applies the uppercase column name function . pipe ( uppercase_column_name ) ) AGE gender Female 19.0 Male 31.5","tags":"Python","url":"http://chrisalbon.com/python/pandas_create_pipeline.html","loc":"http://chrisalbon.com/python/pandas_create_pipeline.html"},{"title":"Select First X Rows","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Drop Row Based On A Conditional %% sql -- Select all SELECT * -- From the criminals table FROM criminals -- Only return the first two rows LIMIT 2 ; pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0","tags":"SQL","url":"http://chrisalbon.com/sql/select_first_x_rows.html","loc":"http://chrisalbon.com/sql/select_first_x_rows.html"},{"title":"Selecting Rows Based On Conditionals","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill Bayes' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Jack Killer' , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View All Rows %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill Bayes 22 M Santa Rosa 0 632 Jack Killer 23 F San Francisco 0 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 View Rows Where Age Is Greater Than 30 %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 30 WHERE age > 30 pid name age sex city minor 901 Gordon Ado 32 F San Francisco 0 View Rows Where Age Is Greater Than Or Equal To 23 %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 23 WHERE age >= 23 pid name age sex city minor 632 Jack Killer 23 F San Francisco 0 901 Gordon Ado 32 F San Francisco 0 View Rows Where Age Is 23 %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 23 WHERE age = 23 pid name age sex city minor 632 Jack Killer 23 F San Francisco 0 View Rows Where Age Is Not 23 %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where age is greater than 23 WHERE age <> 23 pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill Bayes 22 M Santa Rosa 0 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 View Rows Where Name Begins With 'J' %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where name starts with 'J' WHERE name LIKE 'J%' pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 632 Jack Killer 23 F San Francisco 0 View Rows Where Name Contains The String 'ames' %% sql -- Select all SELECT distinct * -- From the criminals table FROM criminals -- Where name contains the string 'ames' WHERE name LIKE '%ames%' pid name age sex city minor 412 James Smith 15 M Santa Rosa 1","tags":"SQL","url":"http://chrisalbon.com/sql/select_rows_based_on_conditionals.html","loc":"http://chrisalbon.com/sql/select_rows_based_on_conditionals.html"},{"title":"Select Rows Based On Text String","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Select Rows With Names Starting With G %% sql -- Select all SELECT * -- From the criminals table FROM criminals -- If name starts with G WHERE name LIKE 'G%' pid name age sex city minor 901 Gordon Ado 32 F Santa Rosa 0 Select Rows With Names Ending With o %% sql -- Select all SELECT * -- From the criminals table FROM criminals -- If name starts ends with o WHERE name LIKE ' %o ' pid name age sex city minor 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 Select Rows With Names Starting With Any Character, Then ordon %% sql -- Select all SELECT * -- From the criminals table FROM criminals -- If name starts with any character then continues with 'ordon' WHERE name LIKE '_ordon%' pid name age sex city minor 901 Gordon Ado 32 F Santa Rosa 0","tags":"SQL","url":"http://chrisalbon.com/sql/select_rows_based_on_text_string.html","loc":"http://chrisalbon.com/sql/select_rows_based_on_text_string.html"},{"title":"Selecting Unique Rows","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View All Rows Notice that 'James Smith' appears three times %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 412 James Smith 15 M Santa Rosa 1 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 View Unique Rows %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0","tags":"SQL","url":"http://chrisalbon.com/sql/selecting_unique_rows.html","loc":"http://chrisalbon.com/sql/selecting_unique_rows.html"},{"title":"Sort By Multiple Columns","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] Sort By Ascending Age And Then Alphabetically By Name %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Sort by ascending age ORDER BY age ASC , name pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 512 Bill Byson 21 M Petaluma 0 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F San Francisco 0 901 Gordon Ado 32 F San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/sort_by_multiple_columns.html","loc":"http://chrisalbon.com/sql/sort_by_multiple_columns.html"},{"title":"Sorting","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View All Rows %% sql -- Select all SELECT * -- From the criminals table FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F San Francisco 0 901 Gordon Ado 32 F San Francisco 0 512 Bill Byson 21 M Petaluma 0 Sort By Ascending Age %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Sort by ascending age ORDER BY age ASC pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 512 Bill Byson 21 M Petaluma 0 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F San Francisco 0 901 Gordon Ado 32 F San Francisco 0 Sort By Descending Age %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Sort by descending age ORDER BY age DESC pid name age sex city minor 901 Gordon Ado 32 F San Francisco 0 632 Stacy Miller 23 F San Francisco 0 234 Bill James 22 M Santa Rosa 0 512 Bill Byson 21 M Petaluma 0 412 James Smith 15 M Santa Rosa 1 Sort Alphabetically %% sql -- Select all unique SELECT distinct * -- From the criminals table FROM criminals -- Sort by name ORDER BY name pid name age sex city minor 512 Bill Byson 21 M Petaluma 0 234 Bill James 22 M Santa Rosa 0 901 Gordon Ado 32 F San Francisco 0 412 James Smith 15 M Santa Rosa 1 632 Stacy Miller 23 F San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/sorting.html","loc":"http://chrisalbon.com/sql/sorting.html"},{"title":"Calculate Counts, Sums, Max, and Averages","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'San Francisco' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Petaluma' , 0 ); [] View Average Ages By City %% sql -- Select name and average age , SELECT city , avg ( age ) -- from the table 'criminals' , FROM criminals -- after grouping by city GROUP BY city city avg(age) Petaluma 21.0 San Francisco 27.5 Santa Rosa 18.5 View Max Age By City %% sql -- Select name and average age , SELECT city , max ( age ) -- from the table 'criminals' , FROM criminals -- after grouping by city GROUP BY city city max(age) Petaluma 21 San Francisco 32 Santa Rosa 22 View Count Of Criminals By City %% sql -- Select name and average age , SELECT city , count ( name ) -- from the table 'criminals' , FROM criminals -- after grouping by city GROUP BY city city count(name) Petaluma 1 San Francisco 2 Santa Rosa 2 View Total Age By City %% sql -- Select name and average age , SELECT city , total ( age ) -- from the table 'criminals' , FROM criminals -- after grouping by city GROUP BY city city total(age) Petaluma 21.0 San Francisco 55.0 Santa Rosa 37.0","tags":"SQL","url":"http://chrisalbon.com/sql/sums_counts_max_averages.html","loc":"http://chrisalbon.com/sql/sums_counts_max_averages.html"},{"title":"Break A Sequence Into Groups","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create An Array Sequence // Create an array that contains arrays with first and last names val ages = List ( 42 , 25 , 28 , 38 , 58 , 63 , 23 , 458 , 2569 , 584 , 25 , 25 , 878 ) Group Array By Anonymous Function // If an element is even, return True, if not, return False val isEven = ages . groupBy ( _ % 2 == 0 ) View Groups // View group that is evens evensOdds ( true ) List(42, 28, 38, 58, 458, 584, 878) // View group that is odds evensOdds ( false ) List(25, 63, 23, 2569, 25, 25)","tags":"Scala","url":"http://chrisalbon.com/scala/break_a_sequence_into_groups.html","loc":"http://chrisalbon.com/scala/break_a_sequence_into_groups.html"},{"title":"Change Data Type","text":"This tutorial was inspired by the awesome Scala Cookbook . Change To Integer // Convert a float to an integer 32.34 . toInt 32 // Convert a string to an integer \"23394\" . toInt 23394 Change To Double // Convert a string to a double \"23394\" . toDouble 23394.0 Change To Float // Convert a string to float \"23394\" . toFloat 23394.0 // Convert an integer to float 3923. toFloat 3923.0 Change To Long // Convert a string to long \"23394\" . toLong 23394 Change To Short // Convert a string to short \"23394\" . toShort 23394 Change To Byte // Convert a string to byte \"10\" . toByte 10 Change To String // Convert an integer to string 2393. toString 2393","tags":"Scala","url":"http://chrisalbon.com/scala/change_data_type.html","loc":"http://chrisalbon.com/scala/change_data_type.html"},{"title":"Chunk Sequence In Equal Sized Groups","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create An Array Sequence // Create an array that contains arrays with first and last names val ages = List ( 42 , 25 , 28 , 38 , 58 , 63 , 23 , 458 , 2569 , 584 , 25 , 25 , 878 ) Chunk Array Into Groups Of Two Elements // Slide over sequence, create a list of two elements, then take two steps ages . sliding ( 2 , 2 ). toArray Array(List(42, 25), List(28, 38), List(58, 63), List(23, 458), List(2569, 584), List(25, 25), List(878)) Chunk Array Into Groups Of Two Elements, With Overlap // Slide over sequence, create a list of two elements, then take one step ages . sliding ( 2 , 1 ). toArray Array(List(42, 25), List(25, 28), List(28, 38), List(38, 58), List(58, 63), List(63, 23), List(23, 458), List(458, 2569), List(2569, 584), List(584, 25), List(25, 25), List(25, 878))","tags":"Scala","url":"http://chrisalbon.com/scala/chuck_sequence_into_equal_sized_groups.html","loc":"http://chrisalbon.com/scala/chuck_sequence_into_equal_sized_groups.html"},{"title":"Compare Two Floats","text":"This tutorial was inspired by the awesome Scala Cookbook . Create Two Float Values // Create a value val price_old = 2.343232 // Create a value that is very slight different val price_new = 2.343231 Create A Function That Compares Two Floats // Define a function called ~= that contains three arguments: two numbers and a precision level, def ~=( x : Double , y : Double , precision : Double ) = { // If the absolute difference is less than the precision level, return true, otherwise return false if (( x - y ). abs < precision ) true else false } Apply Function With High Precision // Compare price_old and price_new with 0.000001 precision ~=( price_old , price_new , 0.000001 ) false Apply Function With Low Precision // Compare price_old and price_new with 0.1 precision ~=( price_old , price_new , 0.1 ) true","tags":"Scala","url":"http://chrisalbon.com/scala/compare_two_floats.html","loc":"http://chrisalbon.com/scala/compare_two_floats.html"},{"title":"Create A Range","text":"This tutorial was inspired by the awesome Scala Cookbook . Create A Range 1 to 10 // Create a range between 1 and 10 1 to 10 Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) Create A Range In An Array // Create an array between 1 and 10 and put in an array ( 1 to 10 ). toArray Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) Use A Range In A For Loop // For each 1 in 1,2,3,4,5 for ( i <- 1 to 10 ) // Print i println ( \"index: \" + i ) index : 1 index : 2 index : 3 index : 4 index : 5 index : 6 index : 7 index : 8 index : 9 index : 10","tags":"Scala","url":"http://chrisalbon.com/scala/create_a_range.html","loc":"http://chrisalbon.com/scala/create_a_range.html"},{"title":"Extract Substrings Using Regex","text":"This tutorial was inspired by the awesome Scala Cookbook . Create String // Create a string value val text : String = \"27 aircraft\" Create Regex Pattern // Create a regex with two pattern matches (one number and one word) val pattern = \"([0-9]+) ([A-Za-z]+)\" . r Extract Substrings That Match Regex // Apply the regex pattern such that each of the two pattern matches is assigned to a seperate value val pattern ( vehicle_number , vehicle_type ) = text View Output // View the value vehicle_number 27 // View the value vehicle_type aircraft","tags":"Scala","url":"http://chrisalbon.com/scala/extract_substring_using_regex.html","loc":"http://chrisalbon.com/scala/extract_substring_using_regex.html"},{"title":"Filter A Sequence","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create An Array Sequence // Create an array that contains arrays with first and last names val ages = Array ( 42 , 25 , 28 , 38 , 58 , 63 , 23 , 458 , 2569 , 584 , 25 , 25 , 878 ) Elements Less Than 100 ages . filter ( _ < 100 ) Array(42, 25, 28, 38, 58, 63, 23, 25, 25) Elements Greater Than 100 ages . filter ( _ >= 100 ) Array(458, 2569, 584, 878) Elements That Are Even ages . filter ( _ % 2 == 0 ) Array(42, 28, 38, 58, 458, 584, 878)","tags":"Scala","url":"http://chrisalbon.com/scala/filter_a_sequence.html","loc":"http://chrisalbon.com/scala/filter_a_sequence.html"},{"title":"Flatten Sequence Of Sequences","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create An Array Sequence // Create an array that contains arrays with first and last names val fullNames = Array ( Array ( \"Jason\" , \"Miller\" ), Array ( \"Jason\" , \"Miller\" ), // Duplicate Array ( \"Sally\" , \"Fields\" ), Array ( \"Betty\" , \"Johnson\" ) ) Flatten The Sequence // Flatten the sequence fullNames . flatten Array(Jason, Miller, Jason, Miller, Sally, Fields, Betty, Johnson) Flatten The Sequence And Only Keep Unique Values // Flatten the sequence and remove any duplicates fullNames . flatten . distinct Array(Jason, Miller, Sally, Fields, Betty, Johnson)","tags":"Scala","url":"http://chrisalbon.com/scala/flatten_sequence_of_sequences.html","loc":"http://chrisalbon.com/scala/flatten_sequence_of_sequences.html"},{"title":"For Loop A Map","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Map val vehicles = Map ( \"vehicle_type\" -> \"Tank\" , \"number\" -> 21 ) Loop With Value And Index // Create a value for the returned values, for each key and value in the map, val numberOfVehicles = for (( key , value ) <- vehicles ) yield { // Return the values value } // View the returned values numberOfVehicles List(Tank, 21)","tags":"Scala","url":"http://chrisalbon.com/scala/for_loop_a_map.html","loc":"http://chrisalbon.com/scala/for_loop_a_map.html"},{"title":"For Looping","text":"This tutorial was inspired by the awesome Scala Cookbook . Create An Array val staffMembers = Array ( \"Jason Miller\" , \"Steve Miller\" , \"Sally Fields\" ) Loop Over Every Item In The Array // Create a value that is the output, then for each person in staff val staffFirstNames = for ( person <- staffMembers ) yield { // Upper case the name val upperCaseFullNames = person . toUpperCase // Get the first name by splitting the full name by space and taking the first element val firstName = upperCaseFullNames . split ( \" \" )( 0 ) // Return the first name firstName } // View the returned values staffFirstNames Array(JASON, STEVE, SALLY) Loop Over Every Item In The Array That Meets A Criteria An if statement in a for loop is called a \"guard.\" // Create a value that is the output, then for each person in staff with the last name of Miller val staffWithLastNameMiller = for ( person <- staffMembers if person . split ( \" \" )( 1 ) == \"Miller\" ) yield { // Get the first name by splitting the full name by space and taking the first element val firstName = person . split ( \" \" )( 0 ) // Return the first name firstName } // View the returned values staffWithLastNameMiller Array(Jason, Steve) Loop With Value And Index This is like Python's enumerate() . // Create a value that is the output, then for each person and index, yield val staffNamesAndIndex = for (( person , i ) <- staffMembers . zipWithIndex ) yield { // Upper case the name val firstName = person . split ( \" \" )( 0 ) // Return a tuple with the first name and index ( firstName , i ) } // View the returned values staffNamesAndIndex Array((Jason,0), (Steve,1), (Sally,2))","tags":"Scala","url":"http://chrisalbon.com/scala/for_looping.html","loc":"http://chrisalbon.com/scala/for_looping.html"},{"title":"Format Numbers As Currency","text":"This tutorial was inspired by the awesome Scala Cookbook . Load The NumberFormat Currency Package // Create a value with the numberformat currency package val format_as_dollars = java . text . NumberFormat . getCurrencyInstance Format A Number As Dollars format_as_dollars . format ( 123.456789 ) $123.46 Change To A Local Currency Java's locale uses ISO 3166-1 country codes . // Load the java libraries import java.util. { Currency , Locale } // Create a value with the numberformat currency package val format_as_afghan = java . text . NumberFormat . getCurrencyInstance // Set the locale of Currency to Afganistan val af = Currency . getInstance ( new Locale ( \"af\" , \"AF\" )) // Set the locale of the numberformat currency package to the Afghan format_as_afghan . setCurrency ( af ) Format A Number As Afghans // Format the currency as Afghans format_as_afghan . format ( 123456.789 ) AFN123,456.79","tags":"Scala","url":"http://chrisalbon.com/scala/format_numbers_as_currency.html","loc":"http://chrisalbon.com/scala/format_numbers_as_currency.html"},{"title":"If Else","text":"If you want a deeper guide to Scala, when I was learning I used Programming Scala and Scala for Data Science . Create A Value // Create a value called x that is a short integer of 3 val x : Short = 3 Create A Conditional Expression // Create a value that is 1 if x is greater than 0, otherwise -1 val binary = if ( x > 0 ) 1 else - 1 // View that value binary 1","tags":"Scala","url":"http://chrisalbon.com/scala/if_else.html","loc":"http://chrisalbon.com/scala/if_else.html"},{"title":"Increment And Decrement Numbers","text":"This tutorial was inspired by the awesome Scala Cookbook . Create Integer Variable // Create an integer variable of 10 var i : Int = 10 Increment And Decrement // Increment up 1 i += 1 // View variable i 11 // Decrement up 1 i -= 1 // View variable i 10 // Increment up x2 i *= 2 // View variable i 20 // Decrement down by half i /= 2 // View variable i 10","tags":"Scala","url":"http://chrisalbon.com/scala/increment_and_decrement_numbers.html","loc":"http://chrisalbon.com/scala/increment_and_decrement_numbers.html"},{"title":"Insert Variables Into Strings","text":"If you want a deeper guide to Scala, when I was learning I used Programming Scala and Scala for Data Science . The proper term from this is string interpolation . Create A Value // Create some values val number_of_soldiers : Short = 542 val casualties : Short = 32 Add The Value To A String print ( f\"Before the battle we had $number_of_soldiers soldiers. However, now we have ${ number_of_soldiers - casualties } .\" ) Before the battle we had 542 soldiers. However, now we have 510.","tags":"Scala","url":"http://chrisalbon.com/scala/insert_variables_into_strings.html","loc":"http://chrisalbon.com/scala/insert_variables_into_strings.html"},{"title":"Loop A Collection","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A Vector Collection val vector = Vector ( \"Male\" , 2 , true ) Loop Over The Collection // For each item in the collection, print the class type of the element vector . foreach (( i : Any ) => println ( i , i . getClass . getSimpleName )) (Male,String) (2,Integer) (true,Boolean) // For each item in the collection vector . foreach { // If one of these, print \"Man\" case \"Male\" | \"M\" | \"Man\" | \"Gentleman\" | \"Boy\" => println ( \"Man\" ) // For everything else, print \"Something Else\" case _ => println ( \"Something Else\" ) } Man Something Else Something Else","tags":"Scala","url":"http://chrisalbon.com/scala/loop_a_collection.html","loc":"http://chrisalbon.com/scala/loop_a_collection.html"},{"title":"Make Numbers Pretty","text":"This tutorial was inspired by the awesome Scala Cookbook . Load The NumberFormat Library // Make value that is assigned to an instance of numberformat val make_pretty = java . text . NumberFormat . getInstance Make An Integer Pretty // Format 10000 to 10,000 make_pretty . format ( 10000 ) Make A Float Pretty // Format to 10000.1928 to 10,000.193 make_pretty . format ( 10000.1928 ) 10,000.193 Load The NumberFortmat Library Set For European Numbers // Set the locale to germany val germany = new java . util . Locale ( \"de\" , \"DE\" ) // Make value that is assigned to an instance of numberformat set to germany val make_pretty_de = java . text . NumberFormat . getIntegerInstance ( germany ) Make An Integer Pretty // Format 1000000 to 1.000.000 make_pretty_de . format ( 1000000 ) 1.000.000","tags":"Scala","url":"http://chrisalbon.com/scala/make_numbers_pretty.html","loc":"http://chrisalbon.com/scala/make_numbers_pretty.html"},{"title":"Mapping A Function To A Collection","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Preliminaries import scala.collection.mutable.ArrayBuffer Create Collection // Create an array of strings var birds = ArrayBuffer ( \"Hawk\" , \"Condor\" , \"Eagle\" , \"Pigeon\" ) Create Function // Create a function that returns the length of a string val getLength = ( i : String ) => i . length Map The Function To The Collection // Map the function to the array birds . map ( getLength ) ArrayBuffer(4, 6, 5, 6) Map An Anonymous Function To The Collection // Map the anonymous function to the collection birds . map ( _ . toUpperCase ) ArrayBuffer(HAWK, CONDOR, EAGLE, PIGEON)","tags":"Scala","url":"http://chrisalbon.com/scala/mapping_a_function_to_a_collection.html","loc":"http://chrisalbon.com/scala/mapping_a_function_to_a_collection.html"},{"title":"Matching Conditions","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create A String // Create some strings val text1 = \"Man\" val text2 = \"F\" val text3 = \"Dog\" Create A Function That Uses A Match Expression // Define a function that takes in a string, and matches it def findGender ( word : String ) = word match { // If any of these words, return \"Woman\" case \"Female\" | \"F\" | \"Woman\" | \"Lady\" | \"Girl\" => \"Woman\" // If any of these words, return \"Man\" case \"Male\" | \"M\" | \"Man\" | \"Gentleman\" | \"Boy\" => \"Man\" // If anything else, return \"Unknown\" case _ => \"Unknown\" } Apply The Function To The Strings findGender ( text1 ) Man findGender ( text2 ) Woman findGender ( text3 ) Unknown","tags":"Scala","url":"http://chrisalbon.com/scala/matching_conditions.html","loc":"http://chrisalbon.com/scala/matching_conditions.html"},{"title":"Partial Functions","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . isDefinedAt determines which inputs are accepted. apply is the actual operation. Create A Partial Function // Create a new partial function that inputs a integer and outputs a string val dayOfTheWeek = new PartialFunction [ Int , String ] { // Create an array with the days of the week val days = Array ( \"Monday\" , \"Tuesday\" , \"Wednesday\" , \"Thursday\" , \"Friday\" , \"Saturday\" , \"Sunday\" ) // Only accept input integers that are between 0 and 6 def isDefinedAt ( i : Int ) = i > 0 && i < 6 // If accepted, return the correct day of the week string def apply ( i : Int ) = days ( i - 1 ) } Run The Partial Function dayOfTheWeek ( 2 ) Tuesday","tags":"Scala","url":"http://chrisalbon.com/scala/partial_functions.html","loc":"http://chrisalbon.com/scala/partial_functions.html"},{"title":"Random Integer Between Two Values","text":"This tutorial was inspired by the awesome Scala Cookbook and this great StackOverflow answer. Load Random // Create a value that is the random package val random = new scala . util . Random Create A Start And End // Create a start and end value pair val start = - 10 val end = 10 Generate Random Integer Between The Start And End Values // Then generate a random integer between 0 and the different between end and start + 1 //(to make it inclusive), then shift the value into the desired range by added the start value start + random . nextInt ( ( end - start ) + 1 ) 1","tags":"Scala","url":"http://chrisalbon.com/scala/random_integer_between_two_values.html","loc":"http://chrisalbon.com/scala/random_integer_between_two_values.html"},{"title":"Replacing Parts Of Strings","text":"This tutorial was inspired by the awesome Scala Cookbook . Create A String // Create a string value val text : String = \"Lt. Steve Miller will be leading the attack.\" Create A Regex Pattern // Create a regex pattern for a name val find_steve = \"Steve Miller\" . r Replace Anything That Matches That Pattern With Something Else // Replace all instances of the pattern with a different name find_steve . replaceAllIn ( text , \"Peter Jackson\" ) Lt. Peter Jackson will be leading the attack. Replace First Match // Replace first instance of the pattern with a different name find_steve . replaceFirstIn ( text , \"Peter Jackson\" ) Lt. Peter Jackson will be leading the attack.","tags":"Scala","url":"http://chrisalbon.com/scala/replacing_parts_of_strings.html","loc":"http://chrisalbon.com/scala/replacing_parts_of_strings.html"},{"title":"Search Strings","text":"If you want a deeper guide to Scala, when I was learning I used Programming Scala and Scala for Data Science . Create A String // Create a value called text that is a string val text : String = \"This is a sentence that we want to split along every space\" Array(This, is, a, sentence, that, we, want, to, split, along, every, space) Split Up A String By Commas // Create a value called csv_row that is a string and contains one row of data val csv_row : String = \"Billy, Miller, 22, Baker, High School\" // Split up that row by commas csv_row . split ( \",\" ) Array(Billy, \" Miller\", \" 22\", \" Baker\", \" High School\")","tags":"Scala","url":"http://chrisalbon.com/scala/search_strings.html","loc":"http://chrisalbon.com/scala/search_strings.html"},{"title":"Search Strings Using Regex","text":"This tutorial was inspired by the awesome Scala Cookbook . Create A String // Create a string value val attack_order : String = \"Our 382 troops will attack their east flank at dawn. They have 28 troops there.\" Create A Regex Pattern // Create a value that is a regex pattern val find_numbers = \"[0-9]+\" . r Find First Match // Apply the regex to find the first match, output the result, otherwise output \"None\" find_numbers . findFirstIn ( attack_order ). getOrElse ( \"None\" ) 382 Find All Matches // Apply the regex to find all matches and output to an array find_numbers . findAllIn ( attack_order ). toArray Array(382, 28)","tags":"Scala","url":"http://chrisalbon.com/scala/search_strings_with_regex.html","loc":"http://chrisalbon.com/scala/search_strings_with_regex.html"},{"title":"Set Operations On Sequences","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Preliminaries import scala.collection.mutable.ArrayBuffer Create Two Array Sequences // Create two arrays of ages val student_ages = ArrayBuffer ( 42 , 25 , 28 , 38 , 58 , 63 , 23 , 458 , 2569 , 584 , 25 , 25 ) val teacher_ages = ArrayBuffer ( 23 , 25 , 25 , 38 , 58 , 32 , 23 , 23 , 125 , 23 , 23 , 21 , 26 ) Concatenate Two Sequences // Join two sequences end to end student_ages ++ teacher_ages ArrayBuffer(42, 25, 28, 38, 58, 63, 23, 458, 2569, 584, 25, 25, 23, 25, 25, 38, 58, 32, 23, 23, 125, 23, 23, 21, 26) Intersection (Shared Elements) Of Two Sequences // Create the interaction of two sequences teacher_ages . intersect ( student_ages ) ArrayBuffer(23, 25, 25, 38, 58) Union (All Elements) Of Two Sequences // Create the union of two sequences teacher_ages . union ( student_ages ) ArrayBuffer(23, 25, 25, 38, 58, 32, 23, 23, 125, 23, 23, 21, 26, 42, 25, 28, 38, 58, 63, 23, 458, 2569, 584, 25, 25) Unique Elements In Union Of Two Sequences // Create the union of two sequences then keep only the unique values teacher_ages . union ( student_ages ). distinct ArrayBuffer(23, 25, 38, 58, 32, 125, 21, 26, 42, 28, 63, 458, 2569, 584) Relative Complement Of Two Sequences // Elements in student_ages but not in teacher_ages student_ages diff teacher_ages ArrayBuffer(42, 28, 63, 458, 2569, 584, 25)","tags":"Scala","url":"http://chrisalbon.com/scala/set_operations_on_sequences.html","loc":"http://chrisalbon.com/scala/set_operations_on_sequences.html"},{"title":"Sorting Sequences","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create Two Vectors // Create two vectors val ages = Vector ( 23 , 42 , 12 , 34 ) val lastName = Vector ( \"Jackson\" , \"Dillan\" , \"Bower\" , \"Stein\" ) Sort Alphabetically // View the sequence alphabetically lastName . sorted Vector(Bower, Dillan, Jackson, Stein) Sort Ascending // View the sequence in ascending order ages . sorted Vector(12, 23, 34, 42) Sort Descending // View the sequence sorted using i > j ages . sortWith ( _ > _ ) Vector(42, 34, 23, 12) Sort By Length // Voew the sequence sorted by descending length lastName . sortWith ( _ . length > _ . length ) Vector(Jackson, Dillan, Bower, Stein)","tags":"Scala","url":"http://chrisalbon.com/scala/sorting_sequences.html","loc":"http://chrisalbon.com/scala/sorting_sequences.html"},{"title":"Split Strings","text":"If you want a deeper guide to Scala, when I was learning I used Programming Scala and Scala for Data Science . Split Up A String By Spaces // Create a value called text that is a string val text : String = \"This is a sentence that we want to split along every space\" // Split up the value along every space text . split ( \" \" ) Array(This, is, a, sentence, that, we, want, to, split, along, every, space) Split Up A String By Commas // Create a value called csv_row that is a string and contains one row of data val csv_row : String = \"Billy, Miller, 22, Baker, High School\" // Split up that row by commas csv_row . split ( \",\" ) Array(Billy, \" Miller\", \" 22\", \" Baker\", \" High School\")","tags":"Scala","url":"http://chrisalbon.com/scala/split_strings.html","loc":"http://chrisalbon.com/scala/split_strings.html"},{"title":"Try, Catch, Finally","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create Some Operation That Will Cause An Exception \"Sixteen\" . toFloat Name : java . lang . NumberFormatException Message : For input string : \"Sixteen\" StackTrace : at sun . misc . FloatingDecimal . readJavaFormatString ( FloatingDecimal . java : 2043 ) at sun . misc . FloatingDecimal . parseFloat ( FloatingDecimal . java : 122 ) at java . lang . Float . parseFloat ( Float . java : 451 ) at scala . collection . immutable . StringLike $class . toFloat ( StringLike . scala : 280 ) at scala . collection . immutable . StringOps . toFloat ( StringOps . scala : 29 ) Try, Catch, Finally // Try try { // The bad operation \"Sixteen\" . toFloat // Catch any problems } catch { // If it is an exception, print something case e : Exception => println ( \"Something went wrong\" ) } finally { // Regardless of if there is an error or not, print this println ( \"We are finally done.\" ) } Something went wrong We are finally done. ()","tags":"Scala","url":"http://chrisalbon.com/scala/try_catch_finally.html","loc":"http://chrisalbon.com/scala/try_catch_finally.html"},{"title":"Variables And Values","text":"If you want a deeper guide to Scala, when I was learning I used Programming Scala and Scala for Data Science . Values Are Immutable // Create a value called greeting that is a string with the word \"Hello\" val greeting : String = \"Hello\" // View the value greeting Hello Variables Are Mutable // Create a variable called age that is a \"short\" number (between -32768 to 32767) with the number 12 var age : Short = 12 // View the variable age 12","tags":"Scala","url":"http://chrisalbon.com/scala/variables_and_values.html","loc":"http://chrisalbon.com/scala/variables_and_values.html"},{"title":"Zip Together Two Lists","text":"If you want to learn more, check out Scala Cookbook and Programming in Scala . Create Two Vectors // Create two vectors val firstName = Vector ( \"Steve\" , \"Bob\" , \"Jack\" , \"Jill\" ) val lastName = Vector ( \"Jackson\" , \"Dillan\" , \"Bower\" , \"Stein\" ) Zip Together Vectors // Create a new variable that zips the sequences val fullNames = firstName zip lastName // View the zipped sequences and convert to a map fullNames Vector((Steve,Jackson), (Bob,Dillan), (Jack,Bower), (Jill,Stein))","tags":"Scala","url":"http://chrisalbon.com/scala/zip_together_two_lists.html","loc":"http://chrisalbon.com/scala/zip_together_two_lists.html"},{"title":"Titanic Competition With Random Forest","text":"This was my first attempt at a Kaggle submission and conducted mostly to understand the Kaggle competition process. Preliminaries import pandas as pd import numpy as np from sklearn import preprocessing from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV , cross_val_score import csv as csv Get The Data You can get the data on Kaggle's site . # Load the data train = pd . read_csv ( 'data/train.csv' ) test = pd . read_csv ( 'data/test.csv' ) Data Cleaning # Create a list of the features we will eventually want for our model features = [ 'Age' , 'SibSp' , 'Parch' , 'Fare' , 'male' , 'embarked_Q' , 'embarked_S' , 'Pclass_2' , 'Pclass_3' ] Sex Here we convert the gender labels ( male , female ) into a dummy variable ( 1 , 0 ). # Create an encoder sex_encoder = preprocessing . LabelEncoder () # Fit the encoder to the train data so it knows that male = 1 sex_encoder . fit ( train [ 'Sex' ]) # Apply the encoder to the training data train [ 'male' ] = sex_encoder . transform ( train [ 'Sex' ]) # Apply the encoder to the training data test [ 'male' ] = sex_encoder . transform ( test [ 'Sex' ]) Embarked # Convert the Embarked training feature into dummies using one-hot # and leave one first category to prevent perfect collinearity train_embarked_dummied = pd . get_dummies ( train [ \"Embarked\" ], prefix = 'embarked' , drop_first = True ) # Convert the Embarked test feature into dummies using one-hot # and leave one first category to prevent perfect collinearity test_embarked_dummied = pd . get_dummies ( test [ \"Embarked\" ], prefix = 'embarked' , drop_first = True ) # Concatenate the dataframe of dummies with the main dataframes train = pd . concat ([ train , train_embarked_dummied ], axis = 1 ) test = pd . concat ([ test , test_embarked_dummied ], axis = 1 ) Social Class # Convert the Pclass training feature into dummies using one-hot # and leave one first category to prevent perfect collinearity train_Pclass_dummied = pd . get_dummies ( train [ \"Pclass\" ], prefix = 'Pclass' , drop_first = True ) # Convert the Pclass test feature into dummies using one-hot # and leave one first category to prevent perfect collinearity test_Pclass_dummied = pd . get_dummies ( test [ \"Pclass\" ], prefix = 'Pclass' , drop_first = True ) # Concatenate the dataframe of dummies with the main dataframes train = pd . concat ([ train , train_Pclass_dummied ], axis = 1 ) test = pd . concat ([ test , test_Pclass_dummied ], axis = 1 ) Impute Missing Values A number of values of the Age feature are missing and will prevent the random forest to train. We get around this we will fill in missing values with the mean value of age (a useful fiction). Age # Create an imputer object age_imputer = preprocessing . Imputer ( missing_values = 'NaN' , strategy = 'mean' , axis = 0 ) # Fit the imputer object on the training data age_imputer . fit ( train [ 'Age' ] . reshape ( - 1 , 1 )) # Apply the imputer object to the training and test data train [ 'Age' ] = age_imputer . transform ( train [ 'Age' ] . reshape ( - 1 , 1 )) test [ 'Age' ] = age_imputer . transform ( test [ 'Age' ] . reshape ( - 1 , 1 )) Fare # Create an imputer object fare_imputer = preprocessing . Imputer ( missing_values = 'NaN' , strategy = 'mean' , axis = 0 ) # Fit the imputer object on the training data fare_imputer . fit ( train [ 'Fare' ] . reshape ( - 1 , 1 )) # Apply the imputer object to the training and test data train [ 'Fare' ] = fare_imputer . transform ( train [ 'Fare' ] . reshape ( - 1 , 1 )) test [ 'Fare' ] = fare_imputer . transform ( test [ 'Fare' ] . reshape ( - 1 , 1 )) Search For Optimum Parameters # Create a dictionary containing all the candidate values of the parameters parameter_grid = dict ( n_estimators = list ( range ( 1 , 5001 , 1000 )), criterion = [ 'gini' , 'entropy' ], max_features = list ( range ( 1 , len ( features ), 2 )), max_depth = [ None ] + list ( range ( 5 , 25 , 1 ))) # Creata a random forest object random_forest = RandomForestClassifier ( random_state = 0 , n_jobs =- 1 ) # Create a gridsearch object with 5-fold cross validation, and uses all cores (n_jobs=-1) clf = GridSearchCV ( estimator = random_forest , param_grid = parameter_grid , cv = 5 , verbose = 1 , n_jobs =- 1 ) # Nest the gridsearchCV in a 3-fold CV for model evaluation cv_scores = cross_val_score ( clf , train [ features ], train [ 'Survived' ]) # Print results print ( 'Accuracy scores:' , cv_scores ) print ( 'Mean of score:' , np . mean ( cv_scores )) print ( 'Variance of scores:' , np . var ( cv_scores )) Retrain The Random Forest With The Optimum Parameters # Retrain the model on the whole dataset clf . fit ( train [ features ], train [ 'Survived' ]) # Predict who survived in the test dataset predictions = clf . predict ( test [ features ]) Create The Kaggle Submission # Grab the passenger IDs ids = test [ 'PassengerId' ] . values # Create a csv submission_file = open ( \"submission.csv\" , \"w\" ) # Write to that csv open_file_object = csv . writer ( submission_file ) # Write the header of the csv open_file_object . writerow ([ \"PassengerId\" , \"Survived\" ]) # Write the rows of the csv open_file_object . writerows ( zip ( ids , predictions )) # Close the file submission_file . close ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/titanic_competition_with_random_forest.html","loc":"http://chrisalbon.com/machine-learning/titanic_competition_with_random_forest.html"},{"title":"Group Pandas Data By Hour Of The Day","text":"Preliminaries # Import libraries import pandas as pd import numpy as np Create Data # Create a time series of 2000 elements, one very five minutes starting on 1/1/2000 time = pd . date_range ( '1/1/2000' , periods = 2000 , freq = '5min' ) # Create a pandas series with a random values between 0 and 100, using 'time' as the index series = pd . Series ( np . random . randint ( 100 , size = 2000 ), index = time ) View Data # View the first few rows of the data series [ 0 : 10 ] 2000-01-01 00:00:00 40 2000-01-01 00:05:00 13 2000-01-01 00:10:00 99 2000-01-01 00:15:00 72 2000-01-01 00:20:00 4 2000-01-01 00:25:00 36 2000-01-01 00:30:00 24 2000-01-01 00:35:00 20 2000-01-01 00:40:00 83 2000-01-01 00:45:00 44 Freq: 5T, dtype: int64 Group Data By Time Of The Day # Group the data by the index's hour value, then aggregate by the average series . groupby ( series . index . hour ) . mean () 0 50.380952 1 49.380952 2 49.904762 3 53.273810 4 47.178571 5 46.095238 6 49.047619 7 44.297619 8 53.119048 9 48.261905 10 45.166667 11 54.214286 12 50.714286 13 56.130952 14 50.916667 15 42.428571 16 46.880952 17 56.892857 18 54.071429 19 47.607143 20 50.940476 21 50.511905 22 44.550000 23 50.250000 dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/group_pandas_data_by_hour_of_the_day.html","loc":"http://chrisalbon.com/python/group_pandas_data_by_hour_of_the_day.html"},{"title":"Naive Bayes Classifier From Scratch","text":"Naive bayes is simple classifier known for doing well when only a small number of observations is available. In this tutorial we will create a gaussian naive bayes classifier from scratch and use it to predict the class of a previously unseen data point. This tutorial is based on an example on Wikipedia's naive bayes classifier page , I have implemented it in Python and tweaked some notation to improve explanation. Preliminaries import pandas as pd import numpy as np Create Data Our dataset is contains data on eight individuals. We will use the dataset to construct a classifier that takes in the height, weight, and foot size of an individual and outputs a prediction for their gender. # Create an empty dataframe data = pd . DataFrame () # Create our target variable data [ 'Gender' ] = [ 'male' , 'male' , 'male' , 'male' , 'female' , 'female' , 'female' , 'female' ] # Create our feature variables data [ 'Height' ] = [ 6 , 5.92 , 5.58 , 5.92 , 5 , 5.5 , 5.42 , 5.75 ] data [ 'Weight' ] = [ 180 , 190 , 170 , 165 , 100 , 150 , 130 , 150 ] data [ 'Foot_Size' ] = [ 12 , 11 , 12 , 10 , 6 , 8 , 7 , 9 ] # View the data data Gender Height Weight Foot_Size 0 male 6.00 180 12 1 male 5.92 190 11 2 male 5.58 170 12 3 male 5.92 165 10 4 female 5.00 100 6 5 female 5.50 150 8 6 female 5.42 130 7 7 female 5.75 150 9 The dataset above is used to construct our classifier. Below we will create a new person for whom we know their feature values but not their gender. Our goal is to predict their gender. # Create an empty dataframe person = pd . DataFrame () # Create some feature values for this single row person [ 'Height' ] = [ 6 ] person [ 'Weight' ] = [ 130 ] person [ 'Foot_Size' ] = [ 8 ] # View the data person Height Weight Foot_Size 0 6 130 8 Bayes Theorem Bayes theorem is a famous equation that allows us to make predictions based on data. Here is the classic version of the Bayes theorem: $$\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}$$ This might be too abstract, so let us replace some of the variables to make it more concrete. In a bayes classifier, we are interested in finding out the class (e.g. male or female, spam or ham) of an observation given the data: $$p(\\text{class} \\mid \\mathbf {\\text{data}} )={\\frac {p(\\mathbf {\\text{data}} \\mid \\text{class}) * p(\\text{class})}{p(\\mathbf {\\text{data}} )}}$$ where: $\\text{class}$ is a particular class (e.g. male) $\\mathbf {\\text{data}}$ is an observation's data $p(\\text{class} \\mid \\mathbf {\\text{data}} )$ is called the posterior $p(\\text{data|class})$ is called the likelihood $p(\\text{class})$ is called the prior $p(\\mathbf {\\text{data}} )$ is called the marginal probability In a bayes classifier, we calculate the posterior (technically we only calculate the numerator of the posterior, but ignore that for now) for every class for each observation. Then, classify the observation based on the class with the largest posterior value. In our example, we have one observation to predict and two possible classes (e.g. male and female), therefore we will calculate two posteriors: one for male and one for female. $$p(\\text{person is male} \\mid \\mathbf {\\text{person's data}} )={\\frac {p(\\mathbf {\\text{person's data}} \\mid \\text{person is male}) * p(\\text{person is male})}{p(\\mathbf {\\text{person's data}} )}}$$ $$p(\\text{person is female} \\mid \\mathbf {\\text{person's data}} )={\\frac {p(\\mathbf {\\text{person's data}} \\mid \\text{person is female}) * p(\\text{person is female})}{p(\\mathbf {\\text{person's data}} )}}$$ Gaussian Naive Bayes Classifier A gaussian naive bayes is probably the most popular type of bayes classifier. To explain what the name means, let us look at what the bayes equations looks like when we apply our two classes (male and female) and three feature variables (height, weight, and footsize): $${\\displaystyle {\\text{posterior (male)}}={\\frac {P({\\text{male}})\\,p({\\text{height}}\\mid{\\text{male}})\\,p({\\text{weight}}\\mid{\\text{male}})\\,p({\\text{foot size}}\\mid{\\text{male}})}{\\text{marginal probability}}}}$$ $${\\displaystyle {\\text{posterior (female)}}={\\frac {P({\\text{female}})\\,p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})}{\\text{marginal probability}}}}$$ Now let us unpack the top equation a bit: $P({\\text{male}})$ is the prior probabilities. It is, as you can see, simply the probability an observation is male. This is just the number of males in the dataset divided by the total number of people in the dataset. $p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})$ is the likelihood. Notice that we have unpacked $\\mathbf {\\text{person's data}}$ so it is now every feature in the dataset. The \"gaussian\" and \"naive\" come from two assumptions present in this likelihood: If you look each term in the likelihood you will notice that we assume each feature is uncorrelated from each other. That is, foot size is independent of weight or height etc.. This is obviously not true, and is a \"naive\" assumption - hence the name \"naive bayes.\" Second, we assume have that the value of the features (e.g. the height of women, the weight of women) are normally (gaussian) distributed. This means that $p(\\text{height}\\mid\\text{female})$ is calculated by inputing the required parameters into the probability density function of the normal distribution: $$ p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e&#94;{ -\\frac{(\\text{observation's height}-\\text{average height of females in the data})&#94;2}{2\\text{variance of female height in the data}} } $$ $\\text{marginal probability}$ is probably one of the most confusing parts of bayesian approaches. In toy examples (including ours) it is completely possible to calculate the marginal probability. However, in many real-world cases, it is either extremely difficult or impossible to find the value of the marginal probability (explaining why is beyond the scope of this tutorial). This is not as much of a problem for our classifier as you might think. Why? Because we don't care what the true posterior value is, we only care which class has a the highest posterior value. And because the marginal probability is the same for all classes 1) we can ignore the denominator, 2) calculate only the posterior's numerator for each class, and 3) pick the largest numerator. That is, we can ignore the posterior's denominator and make a prediction solely on the relative values of the posterior's numerator. Okay! Theory over. Now let us start calculating all the different parts of the bayes equations. Calculate Priors Priors can be either constants or probability distributions. In our example is the simply the probability of being a gender. Calculating this is simple: # Number of males n_male = data [ 'Gender' ][ data [ 'Gender' ] == 'male' ] . count () # Number of males n_female = data [ 'Gender' ][ data [ 'Gender' ] == 'female' ] . count () # Total rows total_ppl = data [ 'Gender' ] . count () # Number of males divided by the total rows P_male = n_male / total_ppl # Number of females divided by the total rows P_female = n_female / total_ppl Calculate Likelihood Remember that each term (e.g. $p(\\text{height}\\mid\\text{female})$) in our likelihood is assumed to be a normal pdf. For example: $$ p(\\text{height}\\mid\\text{female})=\\frac{1}{\\sqrt{2\\pi\\text{variance of female height in the data}}}\\,e&#94;{ -\\frac{(\\text{observation's height}-\\text{average height of females in the data})&#94;2}{2\\text{variance of female height in the data}} } $$ This means that for each class (e.g. female) and feature (e.g. height) combination we need to calculate the variance and mean value from the data. Pandas makes this easy: # Group the data by gender and calculate the means of each feature data_means = data . groupby ( 'Gender' ) . mean () # View the values data_means Height Weight Foot_Size Gender female 5.4175 132.50 7.50 male 5.8550 176.25 11.25 # Group the data by gender and calculate the variance of each feature data_variance = data . groupby ( 'Gender' ) . var () # View the values data_variance Height Weight Foot_Size Gender female 0.097225 558.333333 1.666667 male 0.035033 122.916667 0.916667 Now we can create all the variables we need. The code below might look complex but all we are doing is creating a variable out of each cell in both of the tables above. # Means for male male_height_mean = data_means [ 'Height' ][ data_variance . index == 'male' ] . values [ 0 ] male_weight_mean = data_means [ 'Weight' ][ data_variance . index == 'male' ] . values [ 0 ] male_footsize_mean = data_means [ 'Foot_Size' ][ data_variance . index == 'male' ] . values [ 0 ] # Variance for male male_height_variance = data_variance [ 'Height' ][ data_variance . index == 'male' ] . values [ 0 ] male_weight_variance = data_variance [ 'Weight' ][ data_variance . index == 'male' ] . values [ 0 ] male_footsize_variance = data_variance [ 'Foot_Size' ][ data_variance . index == 'male' ] . values [ 0 ] # Means for female female_height_mean = data_means [ 'Height' ][ data_variance . index == 'female' ] . values [ 0 ] female_weight_mean = data_means [ 'Weight' ][ data_variance . index == 'female' ] . values [ 0 ] female_footsize_mean = data_means [ 'Foot_Size' ][ data_variance . index == 'female' ] . values [ 0 ] # Variance for female female_height_variance = data_variance [ 'Height' ][ data_variance . index == 'female' ] . values [ 0 ] female_weight_variance = data_variance [ 'Weight' ][ data_variance . index == 'female' ] . values [ 0 ] female_footsize_variance = data_variance [ 'Foot_Size' ][ data_variance . index == 'female' ] . values [ 0 ] Finally, we need to create a function to calculate the probability density of each of the terms of the likelihood (e.g. $p(\\text{height}\\mid\\text{female})$). # Create a function that calculates p(x | y): def p_x_given_y ( x , mean_y , variance_y ): # Input the arguments into a probability density function p = 1 / ( np . sqrt ( 2 * np . pi * variance_y )) * np . exp (( - ( x - mean_y ) ** 2 ) / ( 2 * variance_y )) # return p return p Apply Bayes Classifier To New Data Point Alright! Our bayes classifier is ready. Remember that since we can ignore the marginal probability (the demoninator), what we are actually calculating is this: $${\\displaystyle {\\text{numerator of the posterior}}={P({\\text{female}})\\,p({\\text{height}}\\mid{\\text{female}})\\,p({\\text{weight}}\\mid{\\text{female}})\\,p({\\text{foot size}}\\mid{\\text{female}})}{}}$$ To do this, we just need to plug in the values of the unclassified person (height = 6), the variables of the dataset (e.g. mean of female height), and the function ( p_x_given_y ) we made above: # Numerator of the posterior if the unclassified observation is a male P_male * \\ p_x_given_y ( person [ 'Height' ][ 0 ], male_height_mean , male_height_variance ) * \\ p_x_given_y ( person [ 'Weight' ][ 0 ], male_weight_mean , male_weight_variance ) * \\ p_x_given_y ( person [ 'Foot_Size' ][ 0 ], male_footsize_mean , male_footsize_variance ) 6.1970718438780782e-09 # Numerator of the posterior if the unclassified observation is a female P_female * \\ p_x_given_y ( person [ 'Height' ][ 0 ], female_height_mean , female_height_variance ) * \\ p_x_given_y ( person [ 'Weight' ][ 0 ], female_weight_mean , female_weight_variance ) * \\ p_x_given_y ( person [ 'Foot_Size' ][ 0 ], female_footsize_mean , female_footsize_variance ) 0.00053779091836300176 Because the numerator of the posterior for female is greater than male, then we predict that the person is female.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/naive_bayes_classifier_from_scratch.html","loc":"http://chrisalbon.com/machine-learning/naive_bayes_classifier_from_scratch.html"},{"title":"Nested Cross Validation","text":"Often we want to tune the parameters of a model (for example, C in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation: Set the parameter you want to tune to some value. Split your data into K 'folds' (sections). Train your model using K-1 folds using the parameter value. Test your model on the remaining fold. Repeat steps 3 and 4 so that every fold is the test data once. Repeat steps 1 to 5 for every possible value of the parameter. Report the parameter that produced the best result. However, as Cawley and Talbot point out in their 2010 paper, since we used the test set to both select the values of the parameter and evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a different test set to get an unbiased evaluation of that selected model. One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation. Preliminaries # Load required packages from sklearn import datasets from sklearn.model_selection import GridSearchCV , cross_val_score from sklearn.preprocessing import StandardScaler import numpy as np from sklearn.svm import SVC Get Data The data for this tutorial is beast cancer data with 30 features and a binary target variable . # Load the data dataset = datasets . load_breast_cancer () # Create X from the features X = dataset . data # Create y from the target y = dataset . target Standardize Data # Create a scaler object sc = StandardScaler () # Fit the scaler to the feature data and transform X_std = sc . fit_transform ( X ) Create Inner Cross Validation (For Parameter Tuning) This is our inner cross validation. We will use this to hunt for the best parameters for C , the penalty for misclassifying a data point. GridSearchCV will conduct steps 1-6 listed at the top of this tutorial. # Create a list of 10 candidate values for the C parameter C_candidates = dict ( C = np . logspace ( - 4 , 4 , 10 )) # Create a gridsearch object with the support vector classifier and the C value candidates clf = GridSearchCV ( estimator = SVC (), param_grid = C_candidates ) The code below isn't necessary for parameter tuning using nested cross validation, however to demonstrate that our inner cross validation grid search can find the best value for the parameter C , we will run it once here: # Fit the cross validated grid search on the data clf . fit ( X_std , y ) # Show the best value for C clf . best_estimator_ . C 2.7825594022071258 Create Outer Cross Validation (For Model Evaluation) With our inner cross validation constructed, we can use cross_val_score to evaluate the model with a second (outer) cross validation. The code below splits the data into three folds, running the inner cross validation on two of the folds (merged together) and then evaluating the model on the third fold. This is repeated three times so that every fold is used for testing once. cross_val_score ( clf , X_std , y ) array([ 0.94736842, 0.97894737, 0.98412698]) Each the values above is an unbiased evaluation of the model's accuracy, once for each of the three test folds. Averaged together, they would represent the average accuracy of the model found in the inner cross validated grid search.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/nested_cross_validation.html","loc":"http://chrisalbon.com/machine-learning/nested_cross_validation.html"},{"title":"Feature Selection Using Random Forest","text":"Often in data science we have hundreds or even millions of features and we want a way to create a model that only includes the most important features. This has three benefits. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called \"feature selection.\" Random Forests are often used for feature selection in a data science workflow. The reason is because the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity ). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features. In this tutorial we will: Prepare the dataset Train a random forest classifier Identify the most important features Create a new 'limited featured' dataset containing only those features Train a second classifier on this new dataset Compare the accuracy of the 'full featured' classifier to the accuracy of the 'limited featured' classifier Note: There are other definitions of importance, however in this tutorial we limit our discussion to gini importance. Preliminaries import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score Create The Data The dataset used in this tutorial is the famous iris dataset . The Iris target data contains 50 samples from three species of Iris, y and four feature variables, X . # Load the iris dataset iris = datasets . load_iris () # Create a list of feature names feat_labels = [ 'Sepal Length' , 'Sepal Width' , 'Petal Length' , 'Petal Width' ] # Create X from the features X = iris . data # Create y from output y = iris . target View The Data # View the features X [ 0 : 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) # View the target data y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Split The Data Into Training And Test Sets # Split the data into 40% test and 60% training X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.4 , random_state = 0 ) Train A Random Forest Classifier # Create a random forest classifier clf = RandomForestClassifier ( n_estimators = 10000 , random_state = 0 , n_jobs =- 1 ) # Train the classifier clf . fit ( X_train , y_train ) # Print the name and gini importance of each feature for feature in zip ( feat_labels , clf . feature_importances_ ): print ( feature ) ('Sepal Length', 0.11024282328064565) ('Sepal Width', 0.016255033655398394) ('Petal Length', 0.45028123999239533) ('Petal Width', 0.42322090307156124) The scores above are the importance scores for each variable. There are two things to note. First, all the importance scores add up to 100%. Second, Petal Length and Petal Width are far more important than the other two features. Combined, Petal Length and Petal Width have an importance of ~0.86! Clearly these are the most importance features. Identify And Select Most Important Features # Create a selector object that will use the random forest classifier to identify # features that have an importance of more than 0.15 sfm = SelectFromModel ( clf , threshold = 0.15 ) # Train the selector sfm . fit ( X_train , y_train ) SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False), prefit=False, threshold=0.15) # Print the names of the most important features for feature_list_index in sfm . get_support ( indices = True ): print ( feat_labels [ feature_list_index ]) Petal Length Petal Width Create A Data Subset With Only The Most Important Features # Transform the data to create a new dataset containing only the most important features # Note: We have to apply the transform to both the training X and test X data. X_important_train = sfm . transform ( X_train ) X_important_test = sfm . transform ( X_test ) Train A New Random Forest Classifier Using Only Most Important Features # Create a new random forest classifier for the most important features clf_important = RandomForestClassifier ( n_estimators = 10000 , random_state = 0 , n_jobs =- 1 ) # Train the new classifier on the new dataset containing the most important features clf_important . fit ( X_important_train , y_train ) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) Compare The Accuracy Of Our Full Feature Classifier To Our Limited Feature Classifier # Apply The Full Featured Classifier To The Test Data y_pred = clf . predict ( X_test ) # View The Accuracy Of Our Full Feature (4 Features) Model accuracy_score ( y_test , y_pred ) 0.93333333333333335 # Apply The Full Featured Classifier To The Test Data y_important_pred = clf_important . predict ( X_important_test ) # View The Accuracy Of Our Limited Feature (2 Features) Model accuracy_score ( y_test , y_important_pred ) 0.8833333333333333 As can be seen by the accuracy scores, our original model which contained all four features is 93.3% accurate while the our 'limited' model which contained only two features is 88.3% accurate. Thus, for a small cost in accuracy we halved the number of features in the model.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/feature_selection_using_random_forest.html","loc":"http://chrisalbon.com/machine-learning/feature_selection_using_random_forest.html"},{"title":"Logistic Regression With L1 Regularization","text":"L1 regularization (also called least absolute deviations) is a powerful tool in data science. There are many tutorials out there explaining L1 regularization and I will not try to do that here. Instead, this tutorial is show the effect of the regularization parameter C on the coefficients and model accuracy. Preliminaries import numpy as np from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Create The Data The dataset used in this tutorial is the famous iris dataset . The Iris target data contains 50 samples from three species of Iris, y and four feature variables, X . The dataset contains three categories (three species of Iris), however for the sake of simplicity it is easier if the target data is binary. Therefore we will remove the data from the last species of Iris. # Load the iris dataset iris = datasets . load_iris () # Create X from the features X = iris . data # Create y from output y = iris . target # Remake the variable, keeping all data where the category is not 2. X = X [ y != 2 ] y = y [ y != 2 ] View The Data # View the features X [ 0 : 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) # View the target data y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) Split The Data Into Training And Test Sets # Split the data into test and training sets, with 30% of samples being put into the test set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Standardize Features Because the regularization penalty is comprised of the sum of the absolute value of the coefficients, we need to scale the data so the coefficients are all based on the same scale. # Create a scaler object sc = StandardScaler () # Fit the scaler to the training data and transform X_train_std = sc . fit_transform ( X_train ) # Apply the scaler to the test data X_test_std = sc . transform ( X_test ) Run Logistic Regression With A L1 Penalty With Various Regularization Strengths The usefulness of L1 is that it can push feature coefficients to 0, creating a method for feature selection. In the code below we run a logistic regression with a L1 penalty four times, each time decreasing the value of C . We should expect that as C decreases, more coefficients become 0. C = [ 10 , 1 , . 1 , . 001 ] for c in C : clf = LogisticRegression ( penalty = 'l1' , C = c ) clf . fit ( X_train , y_train ) print ( 'C:' , c ) print ( 'Coefficient of each feature:' , clf . coef_ ) print ( 'Training accuracy:' , clf . score ( X_train , y_train )) print ( 'Test accuracy:' , clf . score ( X_test , y_test )) print ( '' ) C : 10 Coefficient of each feature : [[- 0.0855264 - 3.75409972 4.40427765 0 . ]] Training accuracy : 1.0 Test accuracy : 1.0 C : 1 Coefficient of each feature : [[ 0 . - 2.28800472 2.5766469 0 . ]] Training accuracy : 1.0 Test accuracy : 1.0 C : 0.1 Coefficient of each feature : [[ 0 . - 0.82310456 0.97171847 0 . ]] Training accuracy : 1.0 Test accuracy : 1.0 C : 0.001 Coefficient of each feature : [[ 0 . 0 . 0 . 0 .]] Training accuracy : 0.5 Test accuracy : 0.5 Notice that as C decreases the model coefficients become smaller (for example from 4.36276075 when C=10 to 0.0.97175097 when C=0.1 ), until at C=0.001 all the coefficients are zero. This is the effect of the regularization penalty becoming more prominent.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/logistic_regression_with_l1_regularization.html","loc":"http://chrisalbon.com/machine-learning/logistic_regression_with_l1_regularization.html"},{"title":"Pipelines With Parameter Optimization","text":"Preliminaries # Import required packages import numpy as np from sklearn import linear_model , decomposition , datasets from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV , cross_val_score from sklearn.preprocessing import StandardScaler Load Data # Load the breast cancer data dataset = datasets . load_breast_cancer () # Create X from the dataset's features X = dataset . data # Create y from the dataset's output y = dataset . target Create Pipelines # Create an scaler object sc = StandardScaler () # Create a pca object pca = decomposition . PCA () # Create a logistic regression object with an L2 penalty logistic = linear_model . LogisticRegression () # Create a pipeline of three steps. First, standardize the data. # Second, tranform the data with PCA. # Third, train a logistic regression on the data. pipe = Pipeline ( steps = [( 'sc' , sc ), ( 'pca' , pca ), ( 'logistic' , logistic )]) Create Parameter Space # Create a list of a sequence of integers from 1 to 30 (the number of features in X + 1) n_components = list ( range ( 1 , X . shape [ 1 ] + 1 , 1 )) # Create a list of values of the regularization parameter C = np . logspace ( - 4 , 4 , 50 ) # Create a list of options for the regularization penalty penalty = [ 'l1' , 'l2' ] # Create a dictionary of all the parameter options # Note has you can access the parameters of steps of a pipeline by using '__' parameters = dict ( pca__n_components = n_components , logistic__C = C , logistic__penalty = penalty ) Conduct Parameter Optmization With Pipeline # Create a grid search object clf = GridSearchCV ( pipe , parameters ) # Fit the grid search clf . fit ( X , y ) # View The Best Parameters print ( 'Best Penalty:' , clf . best_estimator_ . get_params ()[ 'logistic__penalty' ]) print ( 'Best C:' , clf . best_estimator_ . get_params ()[ 'logistic__C' ]) print ( 'Best Number Of Components:' , clf . best_estimator_ . get_params ()[ 'pca__n_components' ]) Use Cross Validation To Evaluate Model # Fit the grid search using 3-Fold cross validation cross_val_score ( clf , X , y )","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html","loc":"http://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html"},{"title":"Convert Pandas Categorical Data For Scikit-Learn","text":"Preliminaries # Import required packages from sklearn import preprocessing import pandas as pd Create DataFrame raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) Fit The Label Encoder # Create a label (category) encoder object le = preprocessing . LabelEncoder () # Fit the encoder to the pandas column le . fit ( df [ 'score' ]) LabelEncoder() View The Labels # View the labels (if you want) list ( le . classes_ ) ['normal', 'strong', 'weak'] Transform Categories Into Integers # Apply the fitted encoder to the pandas column le . transform ( df [ 'score' ]) array([1, 2, 0, 2, 1]) Transform Integers Into Categories # Convert some integers into their category names list ( le . inverse_transform ([ 2 , 2 , 1 ])) ['weak', 'weak', 'strong']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/convert_pandas_categorical_column_into_integers_for_scikit-learn.html","loc":"http://chrisalbon.com/machine-learning/convert_pandas_categorical_column_into_integers_for_scikit-learn.html"},{"title":"Impute Missing Values With Means","text":"Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most 'naive' imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it. Preliminaries import pandas as pd import numpy as np from sklearn.preprocessing import Imputer Create Data # Create an empty dataset df = pd . DataFrame () # Create two variables called x0 and x1. Make the first value of x1 a missing value df [ 'x0' ] = [ 0.3051 , 0.4949 , 0.6974 , 0.3769 , 0.2231 , 0.341 , 0.4436 , 0.5897 , 0.6308 , 0.5 ] df [ 'x1' ] = [ np . nan , 0.2654 , 0.2615 , 0.5846 , 0.4615 , 0.8308 , 0.4962 , 0.3269 , 0.5346 , 0.6731 ] # View the dataset df x0 x1 0 0.3051 NaN 1 0.4949 0.2654 2 0.6974 0.2615 3 0.3769 0.5846 4 0.2231 0.4615 5 0.3410 0.8308 6 0.4436 0.4962 7 0.5897 0.3269 8 0.6308 0.5346 9 0.5000 0.6731 Fit Imputer # Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0) mean_imputer = Imputer ( missing_values = 'NaN' , strategy = 'mean' , axis = 0 ) # Train the imputor on the df dataset mean_imputer = mean_imputer . fit ( df ) Apply Imputer # Apply the imputer to the df dataset imputed_df = mean_imputer . transform ( df . values ) View Data # View the data imputed_df array([[ 0.3051 , 0.49273333], [ 0.4949 , 0.2654 ], [ 0.6974 , 0.2615 ], [ 0.3769 , 0.5846 ], [ 0.2231 , 0.4615 ], [ 0.341 , 0.8308 ], [ 0.4436 , 0.4962 ], [ 0.5897 , 0.3269 ], [ 0.6308 , 0.5346 ], [ 0.5 , 0.6731 ]]) Notice that 0.49273333 is the imputed value, replacing the np.NaN value.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/impute_missing_values_with_means.html","loc":"http://chrisalbon.com/machine-learning/impute_missing_values_with_means.html"},{"title":"SVC Parameters When Using RBF Kernel","text":"In this tutorial we will visually explore the effects of the two parameters from the support vector classifier (SVC) when using the radial basis function kernel (RBF). This tutorial draws heavily on the code used in Sebastian Raschka's book Python Machine Learning . Preliminaries # Import packages to visualize the classifer from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt import warnings # Import packages to do the classifying import numpy as np from sklearn.svm import SVC Create Function To Visualize Classification Regions You can ignore the code below. It is used to visualize the the decision regions of the classifier. However it is unimportant to this tutorial to understand how the function works. def versiontuple ( v ): return tuple ( map ( int , ( v . split ( \".\" )))) def plot_decision_regions ( X , y , classifier , test_idx = None , resolution = 0.02 ): # setup marker generator and color map markers = ( 's' , 'x' , 'o' , '&#94;' , 'v' ) colors = ( 'red' , 'blue' , 'lightgreen' , 'gray' , 'cyan' ) cmap = ListedColormap ( colors [: len ( np . unique ( y ))]) # plot the decision surface x1_min , x1_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 x2_min , x2_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx1 , xx2 = np . meshgrid ( np . arange ( x1_min , x1_max , resolution ), np . arange ( x2_min , x2_max , resolution )) Z = classifier . predict ( np . array ([ xx1 . ravel (), xx2 . ravel ()]) . T ) Z = Z . reshape ( xx1 . shape ) plt . contourf ( xx1 , xx2 , Z , alpha = 0.4 , cmap = cmap ) plt . xlim ( xx1 . min (), xx1 . max ()) plt . ylim ( xx2 . min (), xx2 . max ()) for idx , cl in enumerate ( np . unique ( y )): plt . scatter ( x = X [ y == cl , 0 ], y = X [ y == cl , 1 ], alpha = 0.8 , c = cmap ( idx ), marker = markers [ idx ], label = cl ) # highlight test samples if test_idx : # plot all samples if not versiontuple ( np . __version__ ) >= versiontuple ( '1.9.0' ): X_test , y_test = X [ list ( test_idx ), :], y [ list ( test_idx )] warnings . warn ( 'Please update to NumPy 1.9.0 or newer' ) else : X_test , y_test = X [ test_idx , :], y [ test_idx ] plt . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = '' , alpha = 1.0 , linewidths = 1 , marker = 'o' , s = 55 , label = 'test set' ) Generate Data Here we are generating some non-linearly separable data that we will train our classifier on. This data would be akin to your training dataset. There are two classes in our y vector: blue x's and red squares. np . random . seed ( 0 ) X_xor = np . random . randn ( 200 , 2 ) y_xor = np . logical_xor ( X_xor [:, 0 ] > 0 , X_xor [:, 1 ] > 0 ) y_xor = np . where ( y_xor , 1 , - 1 ) plt . scatter ( X_xor [ y_xor == 1 , 0 ], X_xor [ y_xor == 1 , 1 ], c = 'b' , marker = 'x' , label = '1' ) plt . scatter ( X_xor [ y_xor == - 1 , 0 ], X_xor [ y_xor == - 1 , 1 ], c = 'r' , marker = 's' , label = '-1' ) plt . xlim ([ - 3 , 3 ]) plt . ylim ([ - 3 , 3 ]) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show () Classify Using a Linear Kernel The most basic way to use a SVC is with a linear kernel, which means the decision boundary is a straight line (or hyperplane in higher dimensions). Linear kernels are rarely used in practice, however I wanted to show it here since it is the most basic version of SVC. As can been seen below, it is not very good at classifying (which can be seen by all the blue X's in the red region) because the data is not linear. # Create a SVC classifier using a linear kernel svm = SVC ( kernel = 'linear' , C = 1 , random_state = 0 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Classify Using a RBF Kernel Radial Basis Function is a commonly used kernel in SVC: $$K(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {||\\mathbf {x} -\\mathbf {x'} ||&#94;{2}}{2\\sigma &#94;{2}}}\\right)$$ where $||\\mathbf {x} -\\mathbf {x'} ||&#94;{2}$ is the squared Euclidean distance between two data points $\\mathbf {x}$ and $\\mathbf {x'}$. If this doesn't make sense, Sebastian's book has a full description. However, for this tutorial, it is only important to know that an SVC classifier using an RBF kernel has two parameters: gamma and C . Gamma gamma is a parameter of the RBF kernel and can be thought of as the 'spread' of the kernel and therefore the decision region. When gamma is low, the 'curve' of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points. We will see this very clearly below. C C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance). Gamma In the four charts below, we apply the same SVC-RBF classifier to the same data while holding C constant. The only difference between each chart is that each time we will increase the value of gamma . By doing so, we can visually see the effect of gamma on the decision boundary. Gamma = 0.01 In the case of our SVC classifier and data, when using a low gamma like 0.01, the decision boundary is not very 'curvy', rather it is just one big sweeping arch. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 1.0 You can see a big difference when we increase the gamma to 1. Now the decision boundary is starting to better cover the spread of the data. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 1 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 10.0 At gamma = 10 the spread of the kernel is less pronounced. The decision boundary starts to be highly effected by individual data points (i.e. variance). # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 10 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 100.0 With high gamma , the decision boundary is almost entirely dependent on individual data points, creating \"islands\". This data is clearly overfitted. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 100 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C - The Penalty Parameter Now we will repeat the process for C : we will use the same classifier, same data, and hold gamma constant. The only thing we will change is the C , the penalty for misclassification. C = 1 With C = 1 , the classifier is clearly tolerant of misclassified data point. There are many red points in the blue region and blue points in the red region. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 10 At C = 10 , the classifier is less tolerant to misclassified data points and therefore the decision boundary is more severe. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 10 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 1000 When C = 1000 , the classifier starts to become very intolerant to misclassified data points and thus the decision boundary becomes less biased and has more variance (i.e. more dependent on the individual data points). # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 10000 At C = 10000 , the classifier \"works really hard\" to not misclassify data points and we see signs of overfitting. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 10000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 100000 At C = 100000 , the classifier is heavily penalized for any misclassified data points and therefore the margins are small. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 100000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show ()","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/svc_parameters_using_rbf_kernel.html","loc":"http://chrisalbon.com/machine-learning/svc_parameters_using_rbf_kernel.html"},{"title":"Run Project Jupyter Notebooks On Amazon EC2","text":"This is tutorial on running Project Jupyter Notebook on an Amazon EC2 instance. It is based on a tutorial by Piyush Agarwal which did not work for me immediately, but I tweaked a few things and got it working. Note: This is not a beginner's tutorial. I don't explain some of the steps fully and don't explain some concepts. There are other tutorials out there for that. Create an AWS account An EC2 instance requires an AWS account. You can make an account here . Navigate to EC2 Log into AWS and go to the EC2 main page. Then you will see a 'Launch Instance' button. Launch a new instance Select Ubuntu Select t2.micro Check out your new instance Create a new security group Create and download a new key pair View connect instructions Set permissions on key pair chmod 400 tutorial.pem Open terminal Connect using ssh ssh -i \"tutorial.pem\" ubuntu@ec2-52-39-239-66.us-west-2.compute.amazonaws.com Are you sure you want to continue connecting (yes/no)? Download Anaconda to instance Visit Anaconda's download page and right click to get the url of the latest version of the Linux 64-bit version. In my case this url was: https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh Now, back in the terminal, tell the EC2 instance to download that file. Note: You aren't downloading the file to your computer, you are downloading it to the EC2 instance and installing it from there. wget https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh Install Anaconda bash Anaconda3-4.2.0-Linux-x86_64.sh Press enter a few times Type 'yes' to agree 'Press ENTER to confirm the location' Do you wish the installer to prepend the Anaconda3 install location to PATH in your /home/ubuntu/.bashrc ? [yes|no] [no] >>> yes If it doesn't allow you to answer yes, you can manually set it through the prompt they show: You may wish to edit your .bashrc or prepend the Anaconda3 install location: $ export PATH = /home/ubuntu/anaconda3/bin: $PATH Check that Anaconda is the preferred environment which python /home/ubuntu/anaconda3/bin/python Create a password for jupyter notebook ipython from IPython.lib import passwd passwd() Enter password: [Create password and press enter] Verify password: [Press enter] 'sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d' exit Create config profile jupyter notebook --generate-config Create certificates for https mkdir certs cd certs sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem Answer questions Configure jupyter cd ~/.jupyter/ vi jupyter_notebook_config.py Insert this at the beginning of the document: c = get_config() # Kernel config c.IPKernelApp.pylab = 'inline' # if you want plotting support always in your notebook # Notebook config c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' #location of your certificate file c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False #so that the ipython notebook does not opens up a browser by default c.NotebookApp.password = u'sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d' #the encrypted password we generated above # Set the port to 8888, the port we set up in the AWS EC2 set-up c.NotebookApp.port = 8888 Remember to replace sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d with your password! Press esc Press shift-z Create folder for notebooks cd ~ mkdir Notebooks cd Notebooks Create new screen This command allows you to create a separate screen for just your Jupyter process logs while you continue to do other work on the ec2 instance. screen Start Jupyter notebook jupyter notebook Detach from screen Command: Ctrl-a and then d Other useful commands: Create new window: Ctrl-a c . Switch windows: Ctrl-a n Reattach to Screen: screen -r See running screen processes screen -ls Visit Jupyter notebook in browser Your EC2 instance will have a long url, like this: ec2-52-39-239-66.us-west-2.compute.amazonaws.com Visit that URL in your browser: (make sure to include the https at the beginning, or you'll have access errors.) https://ec2-52-39-239-66.us-west-2.compute.amazonaws.com:8888/","tags":"Cloud Computing","url":"http://chrisalbon.com/cloud-computing/run_project_jupyter_on_amazon_ec2.html","loc":"http://chrisalbon.com/cloud-computing/run_project_jupyter_on_amazon_ec2.html"},{"title":"Perceptron In Scikit","text":"A perceptron learner was one of the earliest machine learning techniques and still from the foundation of many modern neural networks. In this tutorial we use a perceptron learner to classify the famous iris dataset . This tutorial was inspired by Python Machine Learning by Sebastian Raschka . Preliminaries # Load required libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Perceptron from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np Load The Iris Data # Load the iris dataset iris = datasets . load_iris () # Create our X and y data X = iris . data y = iris . target View The Iris Data # View the first five observations of our y data y [: 5 ] array([0, 0, 0, 0, 0]) # View the first five observations of our x data. # Notice that there are four independent variables (features) X [: 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) Split The Iris Data Into Training And Test # Split the data into 70% training data and 30% test data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 ) Preprocess The X Data By Scaling # Train the scaler, which standarizes all the features to have mean=0 and unit variance sc = StandardScaler () sc . fit ( X_train ) StandardScaler(copy=True, with_mean=True, with_std=True) # Apply the scaler to the X training data X_train_std = sc . transform ( X_train ) # Apply the SAME scaler to the X test data X_test_std = sc . transform ( X_test ) Train A Perceptron Learner # Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1 ppn = Perceptron ( n_iter = 40 , eta0 = 0.1 , random_state = 0 ) # Train the perceptron ppn . fit ( X_train_std , y_train ) Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True, n_iter=40, n_jobs=1, penalty=None, random_state=0, shuffle=True, verbose=0, warm_start=False) Apply The Trained Learner To Test Data # Apply the trained perceptron on the X data to make predicts for the y test data y_pred = ppn . predict ( X_test_std ) Compare The Predicted Y With The True Y # View the predicted y test data y_pred array([0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1]) # View the true y test data y_test array([0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1]) Examine Accuracy Metric # View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations) print ( 'Accuracy: %.2f ' % accuracy_score ( y_test , y_pred )) Accuracy : 0.87","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/perceptron_in_scikit.html","loc":"http://chrisalbon.com/machine-learning/perceptron_in_scikit.html"},{"title":"Mine Twitter's Stream For Hashtags Or Words","text":"This is a script which monitor's Twitter for tweets containing certain hashtags, words, or phrases. When one of those appears, it saves that tweet, and the user's information to a csv file. A similar version of this script is available on GitHub here . The main difference between the code presented here and the repo is that here I am added extensive comments in the code explaining what is happening. Also, the code below runs as a Jupyter notebook. To get the code below to run, you need to added your own Twitter API credentials. Preliminaries #Import libraries from tweepy.streaming import StreamListener from tweepy import OAuthHandler from tweepy import Stream import time import csv import sys Create A Twitter Stream Miner # Create a streamer object class StdOutListener ( StreamListener ): # Define a function that is initialized when the miner is called def __init__ ( self , api = None ): # That sets the api self . api = api # Create a file with 'data_' and the current time self . filename = 'data' + '_' + time . strftime ( '%Y%m %d -%H%M%S' ) + '.csv' # Create a new file with that filename csvFile = open ( self . filename , 'w' ) # Create a csv writer csvWriter = csv . writer ( csvFile ) # Write a single row with the headers of the columns csvWriter . writerow ([ 'text' , 'created_at' , 'geo' , 'lang' , 'place' , 'coordinates' , 'user.favourites_count' , 'user.statuses_count' , 'user.description' , 'user.location' , 'user.id' , 'user.created_at' , 'user.verified' , 'user.following' , 'user.url' , 'user.listed_count' , 'user.followers_count' , 'user.default_profile_image' , 'user.utc_offset' , 'user.friends_count' , 'user.default_profile' , 'user.name' , 'user.lang' , 'user.screen_name' , 'user.geo_enabled' , 'user.profile_background_color' , 'user.profile_image_url' , 'user.time_zone' , 'id' , 'favorite_count' , 'retweeted' , 'source' , 'favorited' , 'retweet_count' ]) # When a tweet appears def on_status ( self , status ): # Open the csv file created previously csvFile = open ( self . filename , 'a' ) # Create a csv writer csvWriter = csv . writer ( csvFile ) # If the tweet is not a retweet if not 'RT @' in status . text : # Try to try : # Write the tweet's information to the csv file csvWriter . writerow ([ status . text , status . created_at , status . geo , status . lang , status . place , status . coordinates , status . user . favourites_count , status . user . statuses_count , status . user . description , status . user . location , status . user . id , status . user . created_at , status . user . verified , status . user . following , status . user . url , status . user . listed_count , status . user . followers_count , status . user . default_profile_image , status . user . utc_offset , status . user . friends_count , status . user . default_profile , status . user . name , status . user . lang , status . user . screen_name , status . user . geo_enabled , status . user . profile_background_color , status . user . profile_image_url , status . user . time_zone , status . id , status . favorite_count , status . retweeted , status . source , status . favorited , status . retweet_count ]) # If some error occurs except Exception as e : # Print the error print ( e ) # and continue pass # Close the csv file csvFile . close () # Return nothing return # When an error occurs def on_error ( self , status_code ): # Print the error code print ( 'Encountered error with status code:' , status_code ) # If the error code is 401, which is the error for bad credentials if status_code == 401 : # End the stream return False # When a deleted tweet appears def on_delete ( self , status_id , user_id ): # Print message print ( \"Delete notice\" ) # Return nothing return # When reach the rate limit def on_limit ( self , track ): # Print rate limiting error print ( \"Rate limited, continuing\" ) # Continue mining tweets return True # When timed out def on_timeout ( self ): # Print timeout message print ( sys . stderr , 'Timeout...' ) # Wait 10 seconds time . sleep ( 10 ) # Return nothing return Create A Wrapper For The Miner # Create a mining function def start_mining ( queries ): ''' Inputs list of strings. Returns tweets containing those strings. ''' #Variables that contains the user credentials to access Twitter API consumer_key = \"YOUR_CREDENTIALS\" consumer_secret = \"YOUR_CREDENTIALS\" access_token = \"YOUR_CREDENTIALS\" access_token_secret = \"YOUR_CREDENTIALS\" # Create a listener l = StdOutListener () # Create authorization info auth = OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_token , access_token_secret ) # Create a stream object with listener and authorization stream = Stream ( auth , l ) # Run the stream object using the user defined queries stream . filter ( track = queries ) Run The Stream Miner # Start the miner start_mining ([ 'python' , '#Python' ]) Encountered error with status code: 401","tags":"Python","url":"http://chrisalbon.com/python/mine_a_twitter_hashtags_and_words.html","loc":"http://chrisalbon.com/python/mine_a_twitter_hashtags_and_words.html"},{"title":"Effect Of Alpha On Lasso Regression","text":"Often we want conduct a process called regularization , wherein we penalize the number of features in a model in order to only keep the most important features. This can be particularly important when you have a dataset with 100,000+ features. Lasso regression is a common modeling technique to do regularization. The math behind it is pretty interesting, but practically, what you need to know is that Lasso regression comes with a parameter, alpha , and the higher the alpha , the most feature coefficients are zero. That is, when alpha is 0 , Lasso regression produces the same coefficients as a linear regression. When alpha is very very large, all coefficients are zero. In this tutorial, I run three lasso regressions, with varying levels of alpha, and show the resulting effect on the coefficients. Preliminaries from sklearn.linear_model import Lasso from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_boston import pandas as pd Load Data boston = load_boston () scaler = StandardScaler () X = scaler . fit_transform ( boston [ \"data\" ]) Y = boston [ \"target\" ] names = boston [ \"feature_names\" ] Run Three Lasso Regressions, Varying Alpha Levels # Create a function called lasso, def lasso ( alphas ): ''' Takes in a list of alphas. Outputs a dataframe containing the coefficients of lasso regressions from each alpha. ''' # Create an empty data frame df = pd . DataFrame () # Create a column of feature names df [ 'Feature Name' ] = names # For each alpha value in the list of alpha values, for alpha in alphas : # Create a lasso regression with that alpha value, lasso = Lasso ( alpha = alpha ) # Fit the lasso regression lasso . fit ( X , Y ) # Create a column name for that alpha value column_name = 'Alpha = %f ' % alpha # Create a column of coefficient values df [ column_name ] = lasso . coef_ # Return the datafram return df # Run the function called, Lasso lasso ([ . 0001 , . 5 , 10 ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Feature Name Alpha = 0.000100 Alpha = 0.500000 Alpha = 10.000000 0 CRIM -0.920130 -0.106977 -0.0 1 ZN 1.080498 0.000000 0.0 2 INDUS 0.142027 -0.000000 -0.0 3 CHAS 0.682235 0.397399 0.0 4 NOX -2.059250 -0.000000 -0.0 5 RM 2.670814 2.973323 0.0 6 AGE 0.020680 -0.000000 -0.0 7 DIS -3.104070 -0.169378 0.0 8 RAD 2.656950 -0.000000 -0.0 9 TAX -2.074110 -0.000000 -0.0 10 PTRATIO -2.061921 -1.599574 -0.0 11 B 0.856553 0.545715 0.0 12 LSTAT -3.748470 -3.668884 -0.0 Notice that as the alpha value increases, more features have a coefficient of 0.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/effect_of_alpha_on_lasso_regression.html","loc":"http://chrisalbon.com/machine-learning/effect_of_alpha_on_lasso_regression.html"},{"title":"Generate Tweets Using Markov Chains","text":"Preliminaries import markovify Load Corpus The corpus I am using is just one I found online. The corpus you choose is central to generating realistic text. # Get raw text as string with open ( \"brown.txt\" ) as f : text = f . read () Build Markov Chain # Build the model. text_model = markovify . Text ( text ) Generate One Tweet # Print three randomly-generated sentences of no more than 140 characters for i in range ( 3 ): print ( text_model . make_short_sentence ( 140 )) Within a month, calls were still productive and most devotees of baseball attended the dozens of them. Even death, therefore, has a leather bolo drawn through a local rajah in 1949. He had a rather sharp and confident.","tags":"Python","url":"http://chrisalbon.com/python/generate_tweets_using_markov_chain.html","loc":"http://chrisalbon.com/python/generate_tweets_using_markov_chain.html"},{"title":"Loading Features From Dictionaries","text":"Preliminaries from sklearn.feature_extraction import DictVectorizer Create A Dictionary staff = [{ 'name' : 'Steve Miller' , 'age' : 33. }, { 'name' : 'Lyndon Jones' , 'age' : 12. }, { 'name' : 'Baxter Morth' , 'age' : 18. }] Convert Dictionary To Feature Matrix # Create an object for our dictionary vectorizer vec = DictVectorizer () # Fit then transform the staff dictionary with vec, then output an array vec . fit_transform ( staff ) . toarray () array([[ 33., 0., 0., 1.], [ 12., 0., 1., 0.], [ 18., 1., 0., 0.]]) View Feature Names # Get Feature Names vec . get_feature_names () ['age', 'name=Baxter Morth', 'name=Lyndon Jones', 'name=Steve Miller']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/loading_features_from_dictionaries.html","loc":"http://chrisalbon.com/machine-learning/loading_features_from_dictionaries.html"},{"title":"Nested For Loops Using List Comprehension","text":"# Create two lists squads = [ \"1st Squad\" , '2nd Squad' , '3rd Squad' ] regiments = [ \"51st Regiment\" , '15th Regiment' , '12th Regiment' ] # Create a tuple for each regiment in regiments, for each squad in sqauds [( regiment , squad ) for regiment in regiments for squad in squads ] [('51st Regiment', '1st Squad'), ('51st Regiment', '2nd Squad'), ('51st Regiment', '3rd Squad'), ('15th Regiment', '1st Squad'), ('15th Regiment', '2nd Squad'), ('15th Regiment', '3rd Squad'), ('12th Regiment', '1st Squad'), ('12th Regiment', '2nd Squad'), ('12th Regiment', '3rd Squad')]","tags":"Python","url":"http://chrisalbon.com/python/nested_for_loops_using_list_comprehension.html","loc":"http://chrisalbon.com/python/nested_for_loops_using_list_comprehension.html"},{"title":"Preprocessing Categorical Features","text":"Often, machine learning methods (e.g. logistic regression, SVM with a linear kernel, etc) will require that categorical variables be converted into dummy variables (also called OneHot encoding). For example, a single feature Fruit would be converted into three features, Apples , Oranges , and Bananas , one for each category in the categorical feature. There are common ways to preprocess categorical features: using pandas or scikit-learn. Preliminaries from sklearn import preprocessing from sklearn.pipeline import Pipeline import pandas as pd Create Data raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'city' : [ 'San Francisco' , 'Baltimore' , 'Miami' , 'Douglas' , 'Boston' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'city' ]) df first_name last_name age city 0 Jason Miller 42 San Francisco 1 Molly Jacobson 52 Baltimore 2 Tina Ali 36 Miami 3 Jake Milner 24 Douglas 4 Amy Cooze 73 Boston Convert Nominal Categorical Feature Into Dummy Variables Using Pandas # Create dummy variables for every unique category in df.city pd . get_dummies ( df [ \"city\" ]) Baltimore Boston Douglas Miami San Francisco 0 0.0 0.0 0.0 0.0 1.0 1 1.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 1.0 0.0 3 0.0 0.0 1.0 0.0 0.0 4 0.0 1.0 0.0 0.0 0.0 Convert Nominal Categorical Data Into Dummy (OneHot) Features Using Scikit # Convert strings categorical names to integers integerized_data = preprocessing . LabelEncoder () . fit_transform ( df [ \"city\" ]) # View data integerized_data array([4, 0, 3, 2, 1]) # Convert integer categorical representations to OneHot encodings preprocessing . OneHotEncoder () . fit_transform ( integerized_data . reshape ( - 1 , 1 )) . toarray () array([[ 0., 0., 0., 0., 1.], [ 1., 0., 0., 0., 0.], [ 0., 0., 0., 1., 0.], [ 0., 0., 1., 0., 0.], [ 0., 1., 0., 0., 0.]]) Note that the output of pd.get_dummies() and the scikit methods produces the same output matrix.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/preprocessing_categorical_features.html","loc":"http://chrisalbon.com/machine-learning/preprocessing_categorical_features.html"},{"title":"Store API Credentials For Open Source Projects","text":"One issue which repeated comes up is how to manage private API credentials when the project is available on GitHub. This is the method I use for my own projects. I store all credentials in a JSON file and tell gitignore to not upload that file. Then when I am running that code locally, load the API credentials from the JSON file. Preliminaries import json Step 1: Create a JSON with the API credentials credentials = { 'access_secret' : '392n39d93' , 'access_token' : 'sdf424f' , 'consumer_key' : 'sdf3223' , 'consumer_secret' : 'dsf2344' } with open ( 'credentials.json' , 'w' ) as f : json . dump ( credentials , f , ensure_ascii = False ) Step 2: Add File To gitignore Follow the instructions here . Here is an example of a good gitignore file. Step 3: Retrieve The Credentials From The JSON File This step should be the one done inside your project or script. Load JSON File # Import API Keys with open ( 'credentials.json' ) as creds : credentials = json . load ( creds ) Retrieve The Credentials credentials [ 'consumer_key' ] 'sdf3223'","tags":"Python","url":"http://chrisalbon.com/python/store_api_credentials_for_open_source_projects.html","loc":"http://chrisalbon.com/python/store_api_credentials_for_open_source_projects.html"},{"title":"Cross Validation Pipeline","text":"The code below does a lot in only a few lines. To help explain things, here are the steps that code is doing: Split the raw data into three folds. Select one for testing and two for training. Preprocess the data by scaling the training features. Train a support vector classifier on the training data. Apply the classifier to the test data. Record the accuracy score. Repeat steps 1-5 two more times, once for each fold. Calculate the mean score for all the folds. Preliminaries from sklearn.datasets import load_iris from sklearn.pipeline import make_pipeline from sklearn import preprocessing from sklearn import cross_validation from sklearn import svm Load Data For this tutorial we will use the famous iris dataset . The iris data contains four measurements of 150 iris flowers and their species. We will use a support vector classifier to predict the species of the iris flowers. # Load the iris test data iris = load_iris () # View the iris data features for the first three rows iris . data [ 0 : 3 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2]]) # View the iris data target for first three rows. '0' means it flower is of the setosa species. iris . target [ 0 : 3 ] array([0, 0, 0]) Create Classifier Pipeline Now we create a pipeline for the data. First, the pipeline preprocesses the data by scaling the feature variable's values to mean zero and unit variance. Second, the pipeline trains a support classifier on the data with C=1 . C is the cost function for the margins. The higher the C, the less tolerant the model is for observations being on the wrong side of the hyperplane. # Create a pipeline that scales the data then trains a support vector classifier classifier_pipeline = make_pipeline ( preprocessing . StandardScaler (), svm . SVC ( C = 1 )) Cross Validation Scikit provides a great helper function to make it easy to do cross validation. Specifically, the code below splits the data into three folds, then executes the classifier pipeline on the iris data. Important note from the scikit docs : For integer/None inputs, if y is binary or multiclass, StratifiedKFold used. If the estimator is a classifier or if y is neither binary nor multiclass, KFold is used. # KFold/StratifiedKFold cross validation with 3 folds (the default) # applying the classifier pipeline to the feature and target data scores = cross_validation . cross_val_score ( classifier_pipeline , iris . data , iris . target , cv = 3 ) Evaluate Model Here is the output of our 3 KFold cross validation. Each value is the accuracy score of the support vector classifier when leaving out a different fold. There are three values because there are three folds. A higher accuracy score, the better. scores array([ 0.98039216, 0.90196078, 0.97916667]) To get an good measure of the model's accuracy, we calculate the mean of the three scores. This is our measure of model accuracy. scores . mean () 0.95383986928104569","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cross_validation_pipeline.html","loc":"http://chrisalbon.com/machine-learning/cross_validation_pipeline.html"},{"title":"Saving Machine Learning Models","text":"In scikit there are two main ways to save a model for future use: a pickle string and a pickled model as a file. Preliminaries from sklearn.linear_model import LogisticRegression from sklearn import datasets import pickle from sklearn.externals import joblib Load Data # Load the iris data iris = datasets . load_iris () # Create a matrix, X, of features and a vector, y. X , y = iris . data , iris . target Train Model # Train a naive logistic regression model clf = LogisticRegression ( random_state = 0 ) clf . fit ( X , y ) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) Save To String Using Pickle # Save the trained model as a pickle string. saved_model = pickle . dumps ( clf ) # View the pickled model saved_model b '\\x80\\x03csklearn.linear_model.logistic\\nLogisticRegression\\nq\\x00)\\x81q\\x01}q\\x02(X\\x07\\x00\\x00\\x00penaltyq\\x03X\\x02\\x00\\x00\\x00l2q\\x04X\\x0b\\x00\\x00\\x00multi_classq\\x05X\\x03\\x00\\x00\\x00ovrq\\x06X\\x08\\x00\\x00\\x00max_iterq\\x07KdX\\x08\\x00\\x00\\x00classes_q\\x08cnumpy.core.multiarray\\n_reconstruct\\nq\\tcnumpy\\nndarray\\nq\\nK\\x00\\x85q\\x0bC\\x01bq\\x0c\\x87q\\rRq\\x0e(K\\x01K\\x03\\x85q\\x0fcnumpy\\ndtype\\nq\\x10X\\x02\\x00\\x00\\x00i8q\\x11K\\x00K\\x01\\x87q\\x12Rq\\x13(K\\x03X\\x01\\x00\\x00\\x00<q\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq\\x15b\\x89C\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00q\\x16tq\\x17bX\\x07\\x00\\x00\\x00n_iter_q\\x18h\\th\\nK\\x00\\x85q\\x19h\\x0c\\x87q\\x1aRq\\x1b(K\\x01K\\x01\\x85q\\x1ch\\x10X\\x02\\x00\\x00\\x00i4q\\x1dK\\x00K\\x01\\x87q\\x1eRq\\x1f(K\\x03h\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq b\\x89C\\x04\\x07\\x00\\x00\\x00q!tq\"bX\\x06\\x00\\x00\\x00n_jobsq#K\\x01X\\x11\\x00\\x00\\x00intercept_scalingq$K\\x01X\\x03\\x00\\x00\\x00tolq%G?\\x1a6\\xe2\\xeb\\x1cC-X\\x07\\x00\\x00\\x00verboseq&K\\x00X\\x04\\x00\\x00\\x00dualq\\'\\x89X\\x0c\\x00\\x00\\x00random_stateq(K\\x00X\\x05\\x00\\x00\\x00coef_q)h\\th\\nK\\x00\\x85q*h\\x0c\\x87q+Rq,(K\\x01K\\x03K\\x04\\x86q-h\\x10X\\x02\\x00\\x00\\x00f8q.K\\x00K\\x01\\x87q/Rq0(K\\x03h\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq1b\\x88C`\\x9a\\x1c\\x904+\\x8f\\xda?v5\\xf6\\x7f9\\xaa\\xda?FVL\\xe5\\x05R\\xfb\\xbf\\xf6\\xad\\xd9&#94;ya\\xf7?\\x89\\x86\\x10B\\x03\\x9d\\xf9\\xbf\\x7f\\xa7x\\xf5\\\\\\x8c\\xf8\\xbf\\x8b$8y\\xdd\\x18\\x02\\xc0\\xac\\x8f\\xee\\xd9+|\\xe2?\\\\\\x10\\xf2\\xcc\\x8c\\xc4\\x03@\\xda\\xb0;l,w\\xf0\\xbf8_\\xe7W*+\\xf6\\xbf\\xefT`-lq\\x04@q2tq3bX\\n\\x00\\x00\\x00intercept_q4h\\th\\nK\\x00\\x85q5h\\x0c\\x87q6Rq7(K\\x01K\\x03\\x85q8h0\\x89C\\x18\\xd4\\x86D\\x03\\xb1\\xff\\xd0?\\xa2\\xcc=I\\xe5]\\xf1?\\x84\\'\\xad\\x8dxo\\xf3\\xbfq9tq:bX\\n\\x00\\x00\\x00warm_startq;\\x89X\\x01\\x00\\x00\\x00Cq<G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00X\\r\\x00\\x00\\x00fit_interceptq=\\x88X\\x06\\x00\\x00\\x00solverq>X\\t\\x00\\x00\\x00liblinearq?X\\x0c\\x00\\x00\\x00class_weightq@Nub.' # Load the pickled model clf_from_pickle = pickle . loads ( saved_model ) # Use the loaded pickled model to make predictions clf_from_pickle . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Save To Pickled File Using joblib # Save the model as a pickle in a file joblib . dump ( clf , 'filename.pkl' ) ['filename.pkl', 'filename.pkl_01.npy', 'filename.pkl_02.npy', 'filename.pkl_03.npy', 'filename.pkl_04.npy'] # Load the model from the file clf_from_joblib = joblib . load ( 'filename.pkl' ) # Use the loaded model to make predictions clf_from_joblib . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/saving_machine_learning_models.html","loc":"http://chrisalbon.com/machine-learning/saving_machine_learning_models.html"},{"title":"Geolocate A City Or Country","text":"This tutorial creates a function that attempts to take a city and country and return its latitude and longitude. But when the city is unavailable (which is often be the case), the returns the latitude and longitude of the center of the country. Preliminaries from geopy.geocoders import Nominatim geolocator = Nominatim () import numpy as np Create Geolocation Function def geolocate ( city = None , country = None ): ''' Inputs city and country, or just country. Returns the lat/long coordinates of either the city if possible, if not, then returns lat/long of the center of the country. ''' # If the city exists, if city != None : # Try try : # To geolocate the city and country loc = geolocator . geocode ( str ( city + ',' + country )) # And return latitude and longitude return ( loc . latitude , loc . longitude ) # Otherwise except : # Return missing value return np . nan # If the city doesn't exist else : # Try try : # Geolocate the center of the country loc = geolocator . geocode ( country ) # And return latitude and longitude return ( loc . latitude , loc . longitude ) # Otherwise except : # Return missing value return np . nan Geolocate A City And Country # Geolocate a city and country geolocate ( city = 'Austin' , country = 'USA' ) (30.2711286, -97.7436994) Geolocate Just A Country # Geolocate just a country geolocate ( country = 'USA' ) (39.7837304, -100.4458824)","tags":"Python","url":"http://chrisalbon.com/python/geolocate_a_city_or_country.html","loc":"http://chrisalbon.com/python/geolocate_a_city_or_country.html"},{"title":"Preprocessing Iris Data","text":"Preliminaries from sklearn import datasets import numpy as np from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler Load Data # Load the iris data iris = datasets . load_iris () # Create a variable for the feature data X = iris . data # Create a variable for the target data y = iris . target Split Data For Cross Validation # Random split the data into four new datasets, training features, training outcome, test features, # and test outcome. Set the size of the test data to be 30% of the full dataset. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 42 ) Standardize Feature Data # Load the standard scaler sc = StandardScaler () # Compute the mean and standard deviation based on the training data sc . fit ( X_train ) # Scale the training data to be of mean 0 and of unit variance X_train_std = sc . transform ( X_train ) # Scale the test data to be of mean 0 and of unit variance X_test_std = sc . transform ( X_test ) # Feature Test Data, non-standardized X_test [ 0 : 5 ] array([[ 6.1, 2.8, 4.7, 1.2], [ 5.7, 3.8, 1.7, 0.3], [ 7.7, 2.6, 6.9, 2.3], [ 6. , 2.9, 4.5, 1.5], [ 6.8, 2.8, 4.8, 1.4]]) # Feature Test Data, standardized. X_test_std [ 0 : 5 ] array([[ 0.3100623 , -0.49582097, 0.48403749, -0.05143998], [-0.17225683, 1.92563026, -1.26851205, -1.26670948], [ 2.23933883, -0.98011121, 1.76924049, 1.43388941], [ 0.18948252, -0.25367584, 0.36720086, 0.35364985], [ 1.15412078, -0.49582097, 0.54245581, 0.21861991]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/preprocessing_iris_data.html","loc":"http://chrisalbon.com/machine-learning/preprocessing_iris_data.html"},{"title":"Random Forest Classifier Example","text":"This tutorial is based on Yhat's 2013 tutorial on Random Forests in Python . If you want a good summary of the theory and uses of random forests, I suggest you check out their guide. In the tutorial below, I annotate, correct, and expand on a short code example of random forests they present at the end of the article. Specifically, I 1) update the code so it runs in the latest version of pandas and Python, 2) write detailed comments explaining what is happening in each step, and 3) expand the code in a number of ways. Let's get started! A Note About The Data The data for this tutorial is famous. Called, the iris dataset , it contains four variables measuring various parts of iris flowers of three related species, and then a fourth variable with the species name. The reason it is so famous in machine learning and statistics communities is because the data requires very little preprocessing (i.e. no missing values, all features are floating numbers, etc.). Preliminaries # Load the library with the iris dataset from sklearn.datasets import load_iris # Load scikit's random forest classifier library from sklearn.ensemble import RandomForestClassifier # Load pandas import pandas as pd # Load numpy import numpy as np # Set random seed np . random . seed ( 0 ) Load Data # Create an object called iris with the iris data iris = load_iris () # Create a dataframe with the four feature variables df = pd . DataFrame ( iris . data , columns = iris . feature_names ) # View the top 5 rows df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 # Add a new column with the species names, this is what we are going to try to predict df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) # View the top 5 rows df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Create Training And Test Data # Create a new column that for each row, generates a random number between 0 and 1, and # if that value is less than or equal to .75, then sets the value of that cell as True # and false otherwise. This is a quick and dirty way of randomly assigning some rows to # be used as the training data and some as the test data. df [ 'is_train' ] = np . random . uniform ( 0 , 1 , len ( df )) <= . 75 # View the top 5 rows df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species is_train 0 5.1 3.5 1.4 0.2 setosa True 1 4.9 3.0 1.4 0.2 setosa True 2 4.7 3.2 1.3 0.2 setosa True 3 4.6 3.1 1.5 0.2 setosa True 4 5.0 3.6 1.4 0.2 setosa True # Create two new dataframes, one with the training rows, one with the test rows train , test = df [ df [ 'is_train' ] == True ], df [ df [ 'is_train' ] == False ] # Show the number of observations for the test and training dataframes print ( 'Number of observations in the training data:' , len ( train )) print ( 'Number of observations in the test data:' , len ( test )) Number of observations in the training data: 118 Number of observations in the test data: 32 Preprocess Data # Create a list of the feature column's names features = df . columns [: 4 ] # View features features Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object') # train['species'] contains the actual species names. Before we can use it, # we need to convert each species name into a digit. So, in this case there # are three species, which have been coded as 0, 1, or 2. y = pd . factorize ( train [ 'species' ])[ 0 ] # View target y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Train The Random Forest Classifier # Create a random forest Classifier. By convention, clf means 'Classifier' clf = RandomForestClassifier ( n_jobs = 2 , random_state = 0 ) # Train the Classifier to take the training features and learn how they relate # to the training y (the species) clf . fit ( train [ features ], y ) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2, oob_score=False, random_state=0, verbose=0, warm_start=False) Huzzah! We have done it! We have officially trained our random forest Classifier! Now let's play with it. The Classifier model itself is stored in the clf variable. Apply Classifier To Test Data If you have been following along, you will know we only trained our classifier on part of the data, leaving the rest out. This is, in my humble opinion, the most important part of machine learning. Why? Because by leaving out a portion of the data, we have a set of data to test the accuracy of our model! Let's do that now. # Apply the Classifier we trained to the test data (which, remember, it has never seen before) clf . predict ( test [ features ]) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) What are you looking at above? Remember that we coded each of the three species of plant as 0, 1, or 2. What the list of numbers above is showing you is what species our model predicts each plant is based on the the sepal length, sepal width, petal length, and petal width. How confident is the classifier about each plant? We can see that too. # View the predicted probabilities of the first 10 observations clf . predict_proba ( test [ features ])[ 0 : 10 ] array([[ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 0.9, 0.1, 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ]]) There are three species of plant, thus [ 1. , 0. , 0. ] tells us that the classifier is certain that the plant is the first class. Taking another example, [ 0.9, 0.1, 0. ] tells us that the classifier gives a 90% probability the plant belongs to the first class and a 10% probability the plant belongs to the second class. Because 90 is greater than 10, the classifier predicts the plant is the first class. Evaluate Classifier Now that we have predicted the species of all plants in the test data, we can compare our predicted species with the that plant's actual species. # Create actual english names for the plants for each predicted plant class preds = iris . target_names [ clf . predict ( test [ features ])] # View the PREDICTED species for the first five observations preds [ 0 : 5 ] array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype='<U10') # View the ACTUAL species for the first five observations test [ 'species' ] . head () 7 setosa 8 setosa 10 setosa 13 setosa 17 setosa Name: species, dtype: category Categories (3, object): [setosa, versicolor, virginica] That looks pretty good! At least for the first five observations. Now let's use look at all the data. Create a confusion matrix A confusion matrix can be, no pun intended, a little confusing to interpret at first, but it is actually very straightforward. The columns are the species we predicted for the test data and the rows are the actual species for the test data. So, if we take the top row, we can wee that we predicted all 13 setosa plants in the test data perfectly. However, in the next row, we predicted 5 of the versicolor plants correctly, but mis-predicted two of the versicolor plants as virginica. The short explanation of how to interpret a confusion matrix is: anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly. # Create confusion matrix pd . crosstab ( test [ 'species' ], preds , rownames = [ 'Actual Species' ], colnames = [ 'Predicted Species' ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Predicted Species setosa versicolor virginica Actual Species setosa 13 0 0 versicolor 0 5 2 virginica 0 0 12 View Feature Importance While we don't get regression coefficients like with OLS, we do get a score telling us how important each feature was in classifying. This is one of the most powerful parts of random forests, because we can clearly see that petal width was more important in classification than sepal width. # View a list of the features and their importance scores list ( zip ( train [ features ], clf . feature_importances_ )) [('sepal length (cm)', 0.11185992930506346), ('sepal width (cm)', 0.016341813006098178), ('petal length (cm)', 0.36439533040889194), ('petal width (cm)', 0.5074029272799464)]","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html","loc":"http://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html"},{"title":"Remove Stop Words","text":"Authors: Chris Albon Preliminaries # Load library from nltk.corpus import stopwords # You will have to download the set of stop words the first time import nltk nltk . download ( 'stopwords' ) [nltk_data] Downloading package stopwords to [nltk_data] /Users/chrisalbon/nltk_data... [nltk_data] Package stopwords is already up-to-date! True Create Word Tokens # Create word tokens tokenized_words = [ 'i' , 'am' , 'going' , 'to' , 'go' , 'to' , 'the' , 'store' , 'and' , 'park' ] Load Stop Words # Load stop words stop_words = stopwords . words ( 'english' ) # Show stop words stop_words [: 5 ] ['i', 'me', 'my', 'myself', 'we'] Remove Stop Words # Remove stop words [ word for word in tokenized_words if word not in stop_words ] ['going', 'go', 'store', 'park']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/remove_stop_words.html","loc":"http://chrisalbon.com/machine-learning/remove_stop_words.html"},{"title":"Stemming Words","text":"Authors: Chris Albon Preliminaries # Load library from nltk.stem.porter import PorterStemmer Create Text Data # Create word tokens tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] Stem Words Stemming reduces a word to its stem by identifying and removing affixes (e.g. gerunds) while keeping the root meaning of the word. NLTK's PorterStemmer implements the widely used Porter stemming algorithm. # Create stemmer porter = PorterStemmer () # Apply stemmer [ porter . stem ( word ) for word in tokenized_words ] ['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/stemming_words.html","loc":"http://chrisalbon.com/machine-learning/stemming_words.html"},{"title":"Tag Parts Of Speech","text":"Authors: Chris Albon Preliminaries # Load libraries from nltk import pos_tag from nltk import word_tokenize Create Text Data # Create text text_data = \"Chris loved outdoor running\" Tag Parts Of Speech # Use pre-trained part of speech tagger text_tagged = pos_tag ( word_tokenize ( text_data )) # Show parts of speech text_tagged [('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')] Common Penn Treebank Parts Of Speech Tags The output is a list of tuples with the word and the tag of the part of speech. NLTK uses the Penn Treebank parts for speech tags. Tag Part Of Speech NNP Proper noun, singular NN Noun, singular or mass RB Adverb VBD Verb, past tense VBG Verb, gerund or present participle JJ Adjective PRP Personal pronoun","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/tag_parts_of_speech.html","loc":"http://chrisalbon.com/machine-learning/tag_parts_of_speech.html"},{"title":"Remove Punctuation","text":"Authors: Chris Albon Preliminaries # Load libraries import string import numpy as np Create Text Data # Create text text_data = [ 'Hi!!!! I. Love. This. Song....' , '10000% Agree!!!! #LoveIT' , 'Right?!?!' ] Remove Punctuation # Create function using string.punctuation to remove all punctuation def remove_punctuation ( sentence : str ) -> str : return sentence . translate ( str . maketrans ( '' , '' , string . punctuation )) # Apply function [ remove_punctuation ( sentence ) for sentence in text_data ] ['Hi I Love This Song', '10000 Agree LoveIT', 'Right']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/remove_punctuation.html","loc":"http://chrisalbon.com/machine-learning/remove_punctuation.html"},{"title":"Tokenize Text","text":"Preliminaries # Load library from nltk.tokenize import word_tokenize , sent_tokenize Create Text Data # Create text string = \"The science of today is the technology of tomorrow. Tomorrow is today.\" Tokenize Words # Tokenize words word_tokenize ( string ) ['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow', '.', 'Tomorrow', 'is', 'today', '.'] Tokenize Sentences # Tokenize sentences sent_tokenize ( string ) ['The science of today is the technology of tomorrow.', 'Tomorrow is today.']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/tokenize_text.html","loc":"http://chrisalbon.com/machine-learning/tokenize_text.html"},{"title":"Parse HTML","text":"Preliminaries # Load library from bs4 import BeautifulSoup Create HTML # Create some HTML code html = \"<div class='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\" Parse HTML # Parse html soup = BeautifulSoup ( html , \"lxml\" ) # Find the div with the class \"full_name\", show text soup . find ( \"div\" , { \"class\" : \"full_name\" }) . text 'Masego Azra'","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/parse_html.html","loc":"http://chrisalbon.com/machine-learning/parse_html.html"},{"title":"Construct A Dictionary From Multiple Lists","text":"Create Two Lists # Create a list of theofficer's name officer_names = [ 'Sodoni Dogla' , 'Chris Jefferson' , 'Jessica Billars' , 'Michael Mulligan' , 'Steven Johnson' ] # Create a list of the officer's army officer_armies = [ 'Purple Army' , 'Orange Army' , 'Green Army' , 'Red Army' , 'Blue Army' ] Construct A Dictionary From The Two Lists # Create a dictionary that is the zip of the two lists dict ( zip ( officer_names , officer_armies )) {'Chris Jefferson': 'Orange Army', 'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'}","tags":"Python","url":"http://chrisalbon.com/python/construct_a_dictionary_from_multiple_lists_python.html","loc":"http://chrisalbon.com/python/construct_a_dictionary_from_multiple_lists_python.html"},{"title":"Converting A Dictionary Into A Matrix","text":"Preliminaries # Load library from sklearn.feature_extraction import DictVectorizer Create Dictionary # Our dictionary of data data_dict = [{ 'Red' : 2 , 'Blue' : 4 }, { 'Red' : 4 , 'Blue' : 3 }, { 'Red' : 1 , 'Yellow' : 2 }, { 'Red' : 2 , 'Yellow' : 2 }] Feature Matrix From Dictionary # Create DictVectorizer object dictvectorizer = DictVectorizer ( sparse = False ) # Convert dictionary into feature matrix features = dictvectorizer . fit_transform ( data_dict ) # View feature matrix features array([[ 4., 2., 0.], [ 3., 4., 0.], [ 0., 1., 2.], [ 0., 2., 2.]]) View column names # View feature matrix column names dictvectorizer . get_feature_names () ['Blue', 'Red', 'Yellow']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/converting_a_dictionary_into_a_matrix.html","loc":"http://chrisalbon.com/machine-learning/converting_a_dictionary_into_a_matrix.html"},{"title":"Create Interaction Features","text":"Preliminaries # Load libraries from sklearn.preprocessing import PolynomialFeatures import numpy as np Create Feature Matrix # Create feature matrix X = np . array ([[ 2 , 3 ], [ 2 , 3 ], [ 2 , 3 ]]) Add Interaction Features # Create PolynomialFeatures object with interaction_only set to True interaction = PolynomialFeatures ( degree = 2 , interaction_only = True , include_bias = False ) # Transform feature matrix interaction . fit_transform ( X ) array([[ 2., 3., 6.], [ 2., 3., 6.], [ 2., 3., 6.]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/create_interaction_features.html","loc":"http://chrisalbon.com/machine-learning/create_interaction_features.html"},{"title":"Cross Validation With Parameter Tuning Using Grid Search","text":"In machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model's parameters that maximize the accuracy of the model. In this tutorial we work through an example which combines cross validation and parameter tuning using scikit-learn. Note: This tutorial is based on examples given in the scikit-learn documentation . I have combined a few examples in the documentation, simplified the code, and added extensive explanations/code comments. Preliminaries import numpy as np from sklearn.grid_search import GridSearchCV from sklearn import datasets , svm import matplotlib.pyplot as plt Create Two Datasets In the code below, we load the digits dataset , which contains 64 feature variables. Each feature denotes the darkness of a pixel in an 8 by 8 image of a handwritten digit. We can see these features for the first observation: # Load the digit data digits = datasets . load_digits () # View the features of the first observation digits . data [ 0 : 1 ] array([[ 0., 0., 5., 13., 9., 1., 0., 0., 0., 0., 13., 15., 10., 15., 5., 0., 0., 3., 15., 2., 0., 11., 8., 0., 0., 4., 12., 0., 0., 8., 8., 0., 0., 5., 8., 0., 0., 9., 8., 0., 0., 4., 11., 0., 1., 12., 7., 0., 0., 2., 14., 5., 10., 12., 0., 0., 0., 0., 6., 13., 10., 0., 0., 0.]]) The target data is a vector containing the image's true digit. For example, the first observation is a handwritten digit for '0'. # View the target of the first observation digits . target [ 0 : 1 ] array([0]) To demonstrate cross validation and parameter tuning, first we are going to divide the digit data into two datasets called data1 and data2 . data1 contains the first 1000 rows of the digits data, while data2 contains the remaining ~800 rows. Note that this split is separate to the cross validation we will conduct and is done purely to demonstrate something at the end of the tutorial. In other words, don't worry about data2 for now, we will come back to it. # Create dataset 1 data1_features = digits . data [: 1000 ] data1_target = digits . target [: 1000 ] # Create dataset 2 data2_features = digits . data [ 1000 :] data2_target = digits . target [ 1000 :] Create Parameter Candidates Before looking for which combination of parameter values produces the most accurate model, we must specify the different candidate values we want to try. In the code below we have a number of candidate parameter values, including four different values for C ( 1, 10, 100, 1000 ), two values for gamma ( 0.001, 0.0001 ), and two kernels ( linear, rbf ). The grid search will try all combinations of parameter values and select the set of parameters which provides the most accurate model. parameter_candidates = [ { 'C' : [ 1 , 10 , 100 , 1000 ], 'kernel' : [ 'linear' ]}, { 'C' : [ 1 , 10 , 100 , 1000 ], 'gamma' : [ 0.001 , 0.0001 ], 'kernel' : [ 'rbf' ]}, ] Conduct Grid Search To Find Parameters Producing Highest Score Now we are ready to conduct the grid search using scikit-learn's GridSearchCV which stands for grid search cross validation. By default, the GridSearchCV 's cross validation uses 3-fold KFold or StratifiedKFold depending on the situation. # Create a classifier object with the classifier and parameter candidates clf = GridSearchCV ( estimator = svm . SVC (), param_grid = parameter_candidates , n_jobs =- 1 ) # Train the classifier on data1's feature and target data clf . fit ( data1_features , data1_target ) GridSearchCV(cv=None, error_score='raise', estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params={}, iid=True, n_jobs=-1, param_grid=[{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}, {'kernel': ['rbf'], 'gamma': [0.001, 0.0001], 'C': [1, 10, 100, 1000]}], pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0) Success! We have our results! First, let's look at the accuracy score when we apply the model to the data1 's test data. # View the accuracy score print ( 'Best score for data1:' , clf . best_score_ ) Best score for data1: 0.942 Which parameters are the best? We can tell scikit-learn to display them: # View the best parameters for the model found using grid search print ( 'Best C:' , clf . best_estimator_ . C ) print ( 'Best Kernel:' , clf . best_estimator_ . kernel ) print ( 'Best Gamma:' , clf . best_estimator_ . gamma ) Best C: 10 Best Kernel: rbf Best Gamma: 0.001 This tells us that the most accurate model uses C=10 , the rbf kernel, and gamma=0.001 . Sanity Check Using Second Dataset Remember the second dataset we created? Now we will use it to prove that those parameters are actually used by the model. First, we apply the classifier we just trained to the second dataset. Then we will train a new support vector classifier from scratch using the parameters found using the grid search. We should get the same results for both models. # Apply the classifier trained using data1 to data2, and view the accuracy score clf . score ( data2_features , data2_target ) 0.96988707653701378 # Train a new classifier using the best parameters found by the grid search svm . SVC ( C = 10 , kernel = 'rbf' , gamma = 0.001 ) . fit ( data1_features , data1_target ) . score ( data2_features , data2_target ) 0.96988707653701378 Success!","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html","loc":"http://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html"},{"title":"Detecting Outliers","text":"Preliminaries # Load libraries import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.datasets import make_blobs Create Data # Create simulated data X , _ = make_blobs ( n_samples = 10 , n_features = 2 , centers = 1 , random_state = 1 ) # Replace the first observation's values with extreme values X [ 0 , 0 ] = 10000 X [ 0 , 1 ] = 10000 Detect Outliers EllipticEnvelope assumes the data is normally distributed and based on that assumption \"draws\" an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1 ) and any observation outside the ellipse as an outlier (labeled as -1 ). A major limitation of this approach is the need to specify a contamination parameter which is the proportion of observations that are outliers, a value that we don't know. # Create detector outlier_detector = EllipticEnvelope ( contamination =. 1 ) # Fit detector outlier_detector . fit ( X ) # Predict outliers outlier_detector . predict ( X ) array([-1, 1, 1, 1, 1, 1, 1, 1, 1, 1])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/detecting_outliers.html","loc":"http://chrisalbon.com/machine-learning/detecting_outliers.html"},{"title":"Discretize Features","text":"Preliminaries # Load libraries from sklearn.preprocessing import Binarizer import numpy as np Create Data # Create feature age = np . array ([[ 6 ], [ 12 ], [ 20 ], [ 36 ], [ 65 ]]) Option 1: Binarize Feature # Create binarizer binarizer = Binarizer ( 18 ) # Transform feature binarizer . fit_transform ( age ) array([[0], [0], [1], [1], [1]]) Option 2: Break Up Feature Into Bins # Bin feature np . digitize ( age , bins = [ 20 , 30 , 64 ]) array([[0], [0], [1], [2], [3]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/discretize_features.html","loc":"http://chrisalbon.com/machine-learning/discretize_features.html"},{"title":"Encoding Ordinal Categorical Features","text":"Preliminaries # Load library import pandas as pd Create Feature Matrix # Create features df = pd . DataFrame ({ 'Score' : [ 'Low' , 'Low' , 'Medium' , 'Medium' , 'High' ]}) # View data frame df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Score 0 Low 1 Low 2 Medium 3 Medium 4 High Create Scale Map # Create mapper scale_mapper = { 'Low' : 1 , 'Medium' : 2 , 'High' : 3 } Map Scale To Features # Map feature values to scale df [ 'Scale' ] = df [ 'Score' ] . replace ( scale_mapper ) # View data frame df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Score Scale 0 Low 1 1 Low 1 2 Medium 2 3 Medium 2 4 High 3","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/encoding_ordinal_categorical_features.html","loc":"http://chrisalbon.com/machine-learning/encoding_ordinal_categorical_features.html"},{"title":"Exiting A Loop","text":"Create A List # Create a list: armies = [ 'Red Army' , 'Blue Army' , 'Green Army' ] Breaking Out Of A For Loop for army in armies : print ( army ) if army == 'Blue Army' : print ( 'Blue Army Found! Stopping.' ) break Red Army Blue Army Blue Army Found! Stopping. Notice that the loop stopped after the conditional if statement was satisfied. Exiting If Loop Completed A loop will exit when completed, but using an else statement we can add an action at the conclusion of the loop if it hasn't been exited earlier. for army in armies : print ( army ) if army == 'Orange Army' : break else : print ( 'Looped Through The Whole List, No Orange Army Found' ) Red Army Blue Army Green Army Looped Through The Whole List, No Orange Army Found","tags":"Python","url":"http://chrisalbon.com/python/exiting_a_loop_python.html","loc":"http://chrisalbon.com/python/exiting_a_loop_python.html"},{"title":"Group Observations Using K-Means Clustering","text":"Preliminaries # Load libraries from sklearn.datasets import make_blobs from sklearn.cluster import KMeans import pandas as pd Create Data # Make simulated feature matrix X , _ = make_blobs ( n_samples = 50 , n_features = 2 , centers = 3 , random_state = 1 ) # Create DataFrame df = pd . DataFrame ( X , columns = [ 'feature_1' , 'feature_2' ]) Train Clusterer # Make k-means clusterer clusterer = KMeans ( 3 , random_state = 1 ) # Fit clusterer clusterer . fit ( X ) KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10, n_jobs=1, precompute_distances='auto', random_state=1, tol=0.0001, verbose=0) Create Feature Based On Predicted Cluster # Predict values df [ 'group' ] = clusterer . predict ( X ) # First few observations df . head ( 5 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } feature_1 feature_2 group 0 -9.877554 -3.336145 0 1 -7.287210 -8.353986 2 2 -6.943061 -7.023744 2 3 -7.440167 -8.791959 2 4 -6.641388 -8.075888 2","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/group_observations_using_clustering.html","loc":"http://chrisalbon.com/machine-learning/group_observations_using_clustering.html"},{"title":"Handling Imbalanced Classes With Downsampling","text":"In downsampling, we randomly sample without replacement from the majority class (i.e. the class with more observations) to create a new subset of observation equal in size to the minority class. Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris () # Create feature matrix X = iris . data # Create target vector y = iris . target Make Iris Dataset Imbalanced # Remove first 40 observations X = X [ 40 :,:] y = y [ 40 :] # Create binary target vector indicating if class 0 y = np . where (( y == 0 ), 0 , 1 ) # Look at the imbalanced target vector y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) Downsample Majority Class To Match Minority Class # Indicies of each class' observations i_class0 = np . where ( y == 0 )[ 0 ] i_class1 = np . where ( y == 1 )[ 0 ] # Number of observations in each class n_class0 = len ( i_class0 ) n_class1 = len ( i_class1 ) # For every observation of class 0, randomly sample from class 1 without replacement i_class1_downsampled = np . random . choice ( i_class1 , size = n_class0 , replace = False ) # Join together class 0's target vector with the downsampled class 1's target vector np . hstack (( y [ i_class0 ], y [ i_class1_downsampled ])) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_with_downsampling.html","loc":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_with_downsampling.html"},{"title":"Handling Imbalanced Classes With Upsampling","text":"In upsampling, for every observation in the majority class, we randomly select an observation from the minority class with replacement. The end result is the same number of observations from the minority and majority classes. Preliminaries # Load libraries import numpy as np from sklearn.datasets import load_iris Load Iris Dataset # Load iris data iris = load_iris () # Create feature matrix X = iris . data # Create target vector y = iris . target Make Iris Dataset Imbalanced # Remove first 40 observations X = X [ 40 :,:] y = y [ 40 :] # Create binary target vector indicating if class 0 y = np . where (( y == 0 ), 0 , 1 ) # Look at the imbalanced target vector y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) Upsampling Minority Class To Match Majority # Indicies of each class' observations i_class0 = np . where ( y == 0 )[ 0 ] i_class1 = np . where ( y == 1 )[ 0 ] # Number of observations in each class n_class0 = len ( i_class0 ) n_class1 = len ( i_class1 ) # For every observation in class 1, randomly sample from class 0 with replacement i_class0_upsampled = np . random . choice ( i_class0 , size = n_class1 , replace = True ) # Join together class 0's upsampled target vector with class 1's target vector np . concatenate (( y [ i_class0_upsampled ], y [ i_class1 ])) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_with_upsampling.html","loc":"http://chrisalbon.com/machine-learning/handling_imbalanced_classes_with_upsampling.html"},{"title":"Handling Outliers","text":"Preliminaries # Load library import pandas as pd Create Data # Create DataFrame houses = pd . DataFrame () houses [ 'Price' ] = [ 534433 , 392333 , 293222 , 4322032 ] houses [ 'Bathrooms' ] = [ 2 , 3.5 , 2 , 116 ] houses [ 'Square_Feet' ] = [ 1500 , 2500 , 1500 , 48000 ] houses .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Price Bathrooms Square_Feet 0 534433 2.0 1500 1 392333 3.5 2500 2 293222 2.0 1500 3 4322032 116.0 48000 Option 1: Drop # Drop observations greater than some value houses [ houses [ 'Bathrooms' ] < 20 ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Price Bathrooms Square_Feet 0 534433 2.0 1500 1 392333 3.5 2500 2 293222 2.0 1500 Option 2: Mark # Load library import numpy as np # Create feature based on boolean condition houses [ 'Outlier' ] = np . where ( houses [ 'Bathrooms' ] < 20 , 0 , 1 ) # Show data houses .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Price Bathrooms Square_Feet Outlier 0 534433 2.0 1500 0 1 392333 3.5 2500 0 2 293222 2.0 1500 0 3 4322032 116.0 48000 1 Option 3: Rescale # Log feature houses [ 'Log_Of_Square_Feet' ] = [ np . log ( x ) for x in houses [ 'Square_Feet' ]] # Show data houses .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Price Bathrooms Square_Feet Outlier Log_Of_Square_Feet 0 534433 2.0 1500 0 7.313220 1 392333 3.5 2500 0 7.824046 2 293222 2.0 1500 0 7.313220 3 4322032 116.0 48000 1 10.778956","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/handling_outliers.html","loc":"http://chrisalbon.com/machine-learning/handling_outliers.html"},{"title":"Imputing Missing Class Labels","text":"Preliminaries # Load libraries import numpy as np from sklearn.preprocessing import Imputer Create Feature Matrix With Missing Values # Create feature matrix with categorical feature X = np . array ([[ 0 , 2.10 , 1.45 ], [ 1 , 1.18 , 1.33 ], [ 0 , 1.22 , 1.27 ], [ 0 , - 0.21 , - 1.19 ], [ np . nan , 0.87 , 1.31 ], [ np . nan , - 0.67 , - 0.22 ]]) Fill Missing Values' Class With Most Frequent Class # Create Imputer object imputer = Imputer ( strategy = 'most_frequent' , axis = 0 ) # Fill missing values with most frequent class imputer . fit_transform ( X ) array([[ 0. , 2.1 , 1.45], [ 1. , 1.18, 1.33], [ 0. , 1.22, 1.27], [ 0. , -0.21, -1.19], [ 0. , 0.87, 1.31], [ 0. , -0.67, -0.22]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/imputing_missing_class_labels.html","loc":"http://chrisalbon.com/machine-learning/imputing_missing_class_labels.html"},{"title":"Imputing Missing Class Labels Using k-Nearest Neighbors","text":"Preliminaries # Load libraries import numpy as np from sklearn.neighbors import KNeighborsClassifier Create Feature Matrix # Create feature matrix with categorical feature X = np . array ([[ 0 , 2.10 , 1.45 ], [ 1 , 1.18 , 1.33 ], [ 0 , 1.22 , 1.27 ], [ 1 , - 0.21 , - 1.19 ]]) Create Feature Matrix With Missing Values # Create feature matrix with missing values in the categorical feature X_with_nan = np . array ([[ np . nan , 0.87 , 1.31 ], [ np . nan , - 0.67 , - 0.22 ]]) Train k-Nearest Neighbor Classifier # Train KNN learner clf = KNeighborsClassifier ( 3 , weights = 'distance' ) trained_model = clf . fit ( X [:, 1 :], X [:, 0 ]) Predict Missing Values' Class # Predict missing values' class imputed_values = trained_model . predict ( X_with_nan [:, 1 :]) # Join column of predicted class with their other features X_with_imputed = np . hstack (( imputed_values . reshape ( - 1 , 1 ), X_with_nan [:, 1 :])) # Join two feature matrices np . vstack (( X_with_imputed , X )) array([[ 0. , 0.87, 1.31], [ 1. , -0.67, -0.22], [ 0. , 2.1 , 1.45], [ 1. , 1.18, 1.33], [ 0. , 1.22, 1.27], [ 1. , -0.21, -1.19]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/imputing_missing_class_labels_using_k-nearest_neighbors.html","loc":"http://chrisalbon.com/machine-learning/imputing_missing_class_labels_using_k-nearest_neighbors.html"},{"title":"Iterating Over Dictionary Keys","text":"Create A Dictionary Officers = { 'Michael Mulligan' : 'Red Army' , 'Steven Johnson' : 'Blue Army' , 'Jessica Billars' : 'Green Army' , 'Sodoni Dogla' : 'Purple Army' , 'Chris Jefferson' : 'Orange Army' } Officers {'Chris Jefferson': 'Orange Army', 'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'} Use Dictionary Comprehension # Display all dictionary entries where the key doesn't start with 'Chris' { keys : Officers [ keys ] for keys in Officers if not keys . startswith ( 'Chris' )} {'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'} Notice that the entry for 'Chris Jefferson' is not returned.","tags":"Python","url":"http://chrisalbon.com/python/iterating_over_dictionary_keys_python.html","loc":"http://chrisalbon.com/python/iterating_over_dictionary_keys_python.html"},{"title":"Looping Over Two Lists","text":"# Create a list of length 3: armies = [ 'Red Army' , 'Blue Army' , 'Green Army' ] # Create a list of length 4: units = [ 'Red Infantry' , 'Blue Armor' , 'Green Artillery' , 'Orange Aircraft' ] # For each element in the first list, for army , unit in zip ( armies , units ): # Display the corresponding index element of the second list: print ( army , 'has the following options:' , unit ) Red Army has the following options: Red Infantry Blue Army has the following options: Blue Armor Green Army has the following options: Green Artillery Notice that the fourth item of the second list, orange aircraft , did not display.","tags":"Python","url":"http://chrisalbon.com/python/looping_over_two_lists_using_Python.html","loc":"http://chrisalbon.com/python/looping_over_two_lists_using_Python.html"},{"title":"Normalizing Observations","text":"Preliminaries # Load libraries from sklearn.preprocessing import Normalizer import numpy as np Create Feature Matrix # Create feature matrix X = np . array ([[ 0.5 , 0.5 ], [ 1.1 , 3.4 ], [ 1.5 , 20.2 ], [ 1.63 , 34.4 ], [ 10.9 , 3.3 ]]) Normalize Observations Normalizer rescales the values on individual observations to have unit norm (the sum of their lengths is one). # Create normalizer normalizer = Normalizer ( norm = 'l2' ) # Transform feature matrix normalizer . transform ( X ) array([[ 0.70710678, 0.70710678], [ 0.30782029, 0.95144452], [ 0.07405353, 0.99725427], [ 0.04733062, 0.99887928], [ 0.95709822, 0.28976368]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/normalizing_observations.html","loc":"http://chrisalbon.com/machine-learning/normalizing_observations.html"},{"title":"One-Hot Encode Features With Multiple Labels","text":"Preliminaries # Load libraries from sklearn.preprocessing import MultiLabelBinarizer import numpy as np Create Data # Create NumPy array y = [( 'Texas' , 'Florida' ), ( 'California' , 'Alabama' ), ( 'Texas' , 'Florida' ), ( 'Delware' , 'Florida' ), ( 'Texas' , 'Alabama' )] One-hot Encode Data # Create MultiLabelBinarizer object one_hot = MultiLabelBinarizer () # One-hot encode data one_hot . fit_transform ( y ) array([[0, 0, 0, 1, 1], [1, 1, 0, 0, 0], [0, 0, 0, 1, 1], [0, 0, 1, 1, 0], [1, 0, 0, 0, 1]]) View Column Headers # View classes one_hot . classes_ array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'], dtype=object)","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/one-hot_encode_features_with_multiple_labels.html","loc":"http://chrisalbon.com/machine-learning/one-hot_encode_features_with_multiple_labels.html"},{"title":"One-Hot Encode Nominal Categorical Features","text":"Preliminaries # Load libraries from sklearn.preprocessing import LabelBinarizerr import numpy as np import pandas as pd Create Data With One Class Label # Create NumPy array x = np . array ([[ 'Texas' ], [ 'California' ], [ 'Texas' ], [ 'Delaware' ], [ 'Texas' ]]) One-hot Encode Data (Method 1) # Create LabelBinzarizer object one_hot = LabelBinarizer () # One-hot encode data one_hot . fit_transform ( x ) array([[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 1, 0], [0, 0, 1]]) View Column Headers # View classes one_hot . classes_ array(['California', 'Delaware', 'Texas'], dtype='<U10') One-hot Encode Data (Method 2) # Dummy feature pd . get_dummies ( x [:, 0 ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } California Delaware Texas 0 0 0 1 1 1 0 0 2 0 0 1 3 0 1 0 4 0 0 1","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/one-hot_encode_nominal_categorical_features.html","loc":"http://chrisalbon.com/machine-learning/one-hot_encode_nominal_categorical_features.html"},{"title":"Replace Characters","text":"Preliminaries # Import library import re Create Text # Create text text_data = [ 'Interrobang. By Aishwarya Henriette' , 'Parking And Going. By Karl Gautier' , 'Today Is The night. By Jarek Prakash' ] Replace Character (Method 1) # Remove periods remove_periods = [ string . replace ( '.' , '' ) for string in text_data ] # Show text remove_periods ['Interrobang By Aishwarya Henriette', 'Parking And Going By Karl Gautier', 'Today Is The night By Jarek Prakash'] Replace Character (Method 2) # Create function def replace_letters_with_X ( string : str ) -> str : return re . sub ( r '[a-zA-Z]' , 'X' , string ) # Apply function [ replace_letters_with_X ( string ) for string in remove_periods ] ['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX', 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX', 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/replace_characters.html","loc":"http://chrisalbon.com/machine-learning/replace_characters.html"},{"title":"Rescale A Feature","text":"Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np . array ([[ - 500.5 ], [ - 100.1 ], [ 0 ], [ 100.1 ], [ 900.9 ]]) Rescale Feature Using Min-Max # Create scaler minmax_scale = preprocessing . MinMaxScaler ( feature_range = ( 0 , 1 )) # Scale feature x_scale = minmax_scale . fit_transform ( x ) # Show feature x_scale array([[ 0. ], [ 0.28571429], [ 0.35714286], [ 0.42857143], [ 1. ]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/rescale_a_feature.html","loc":"http://chrisalbon.com/machine-learning/rescale_a_feature.html"},{"title":"Standardize A Feature","text":"Preliminaries # Load libraries from sklearn import preprocessing import numpy as np Create Feature # Create feature x = np . array ([[ - 500.5 ], [ - 100.1 ], [ 0 ], [ 100.1 ], [ 900.9 ]]) Standardize Feature # Create scaler scaler = preprocessing . StandardScaler () # Transform the feature standardized = scaler . fit_transform ( x ) # Show feature standardized array([[ 0. ], [ 0.28571429], [ 0.35714286], [ 0.42857143], [ 1. ]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/standardize_a_feature.html","loc":"http://chrisalbon.com/machine-learning/standardize_a_feature.html"},{"title":"Strip Whitespace","text":"Authors: Chris Albon Create Text # Create text text_data = [ ' Interrobang. By Aishwarya Henriette ' , 'Parking And Going. By Karl Gautier' , ' Today Is The night. By Jarek Prakash ' ] Remove Whitespace # Strip whitespaces strip_whitespace = [ string . strip () for string in text_data ] # Show text strip_whitespace ['Interrobang. By Aishwarya Henriette', 'Parking And Going. By Karl Gautier', 'Today Is The night. By Jarek Prakash']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/stripe_whitespace.html","loc":"http://chrisalbon.com/machine-learning/stripe_whitespace.html"},{"title":"K-Nearest Neighbors Classification","text":"K-nearest neighbors classifier (KNN) is a simple and powerful classification learner. KNN has three basic parts: $y_i$: The class of an observation (what we are trying to predict in the test data). $X_i$: The predictors/IVs/attributes of an observation. $K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its \"neighborhood\". For example, K=2 means that each observation's has a neighorhood comprising of the two other observations closest to it. Imagine we have an observation where we know its independent variables $x_{test}$ but do not know its class $y_{test}$. The KNN learner finds the K other observations that are closest to $x_{test}$ and uses their known classes to assign a classes to $x_{test}$. Preliminaries import pandas as pd from sklearn import neighbors import numpy as np % matplotlib inline import seaborn Create Dataset Here we create three variables, test_1 and test_2 are our independent variables, 'outcome' is our dependent variable. We will use this data to train our learner. training_data = pd . DataFrame () training_data [ 'test_1' ] = [ 0.3051 , 0.4949 , 0.6974 , 0.3769 , 0.2231 , 0.341 , 0.4436 , 0.5897 , 0.6308 , 0.5 ] training_data [ 'test_2' ] = [ 0.5846 , 0.2654 , 0.2615 , 0.4538 , 0.4615 , 0.8308 , 0.4962 , 0.3269 , 0.5346 , 0.6731 ] training_data [ 'outcome' ] = [ 'win' , 'win' , 'win' , 'win' , 'win' , 'loss' , 'loss' , 'loss' , 'loss' , 'loss' ] training_data . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } test_1 test_2 outcome 0 0.3051 0.5846 win 1 0.4949 0.2654 win 2 0.6974 0.2615 win 3 0.3769 0.4538 win 4 0.2231 0.4615 win Plot the data This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes. seaborn . lmplot ( 'test_1' , 'test_2' , data = training_data , fit_reg = False , hue = \"outcome\" , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) <seaborn.axisgrid.FacetGrid at 0x11008aeb8> Convert Data Into np.arrays The scikit-learn library requires the data be formatted as a numpy array. Here are doing that reformatting. X = training_data . as_matrix ( columns = [ 'test_1' , 'test_2' ]) y = np . array ( training_data [ 'outcome' ]) Train The Learner This is our big moment. We train a KNN learner using the parameters that an observation's neighborhood is its three closest neighors. weights = 'uniform' can be thought of as the voting system used. For example, uniform means that all neighbors get an equally weighted \"vote\" about an observation's class while weights = 'distance' would tell the learner to weigh each observation's \"vote\" by its distance from the observation we are classifying. clf = neighbors . KNeighborsClassifier ( 3 , weights = 'uniform' ) trained_model = clf . fit ( X , y ) View The Model's Score How good is our trained model compared to our training data? trained_model . score ( X , y ) 0.80000000000000004 Our model is 80% accurate! Note: that in any real world example we'd want to compare the trained model to some holdout test data. But since this is a toy example I used the training data . Apply The Learner To A New Data Point Now that we have trained our model, we can predict the class any new observation, $y_{test}$. Let us do that now! # Create a new observation with the value of the first independent variable, 'test_1', as .4 # and the second independent variable, test_1', as .6 x_test = np . array ([[ . 4 , . 6 ]]) # Apply the learner to the new, unclassified observation. trained_model . predict ( x_test ) array(['loss'], dtype=object) Huzzah! We can see that the learner has predicted that the new observation's class is loss . We can even look at the probabilities the learner assigned to each class: trained_model . predict_proba ( x_test ) array([[ 0.66666667, 0.33333333]]) According to this result, the model predicted that the observation was loss with a ~67% probability and win with a ~33% probability. Because the observation had a greater probability of being loss , it predicted that class for the observation. Notes The choice of K has major affects on the classifer created. The greater the K, more linear (high bias and low variance) the decision boundary. There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/k-nearest_neighbors_using_scikit_pandas.html","loc":"http://chrisalbon.com/machine-learning/k-nearest_neighbors_using_scikit_pandas.html"},{"title":"Load An Excel File Into Pandas","text":"Preliminaries # Load library import pandas as pd Load Excel File # Create URL to Excel file (alternatively this can be a filepath) url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.xlsx' # Load the first sheet of the Excel file into a data frame df = pd . read_excel ( url , sheetname = 0 , header = 1 ) # View the first ten rows df . head ( 10 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 5 2015-01-01 00:00:00 0 0 5 2015-01-01 00:00:01 0 1 9 2015-01-01 00:00:02 0 2 6 2015-01-01 00:00:03 0 3 6 2015-01-01 00:00:04 0 4 9 2015-01-01 00:00:05 0 5 7 2015-01-01 00:00:06 0 6 1 2015-01-01 00:00:07 0 7 6 2015-01-01 00:00:08 0 8 9 2015-01-01 00:00:09 0 9 5 2015-01-01 00:00:10 0","tags":"Python","url":"http://chrisalbon.com/python/load_excel_file_into_pandas.html","loc":"http://chrisalbon.com/python/load_excel_file_into_pandas.html"},{"title":"Load A JSON File Into Pandas","text":"Preliminaries # Load library import pandas as pd Load JSON File # Create URL to JSON file (alternatively this can be a filepath) url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json' # Load the first sheet of the JSON file into a data frame df = pd . read_json ( url , orient = 'columns' ) # View the first ten rows df . head ( 10 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } category datetime integer 0 0 2015-01-01 00:00:00 5 1 0 2015-01-01 00:00:01 5 10 0 2015-01-01 00:00:10 5 11 0 2015-01-01 00:00:11 5 12 0 2015-01-01 00:00:12 8 13 0 2015-01-01 00:00:13 9 14 0 2015-01-01 00:00:14 8 15 0 2015-01-01 00:00:15 8 16 0 2015-01-01 00:00:16 2 17 0 2015-01-01 00:00:17 1","tags":"Python","url":"http://chrisalbon.com/python/load_json_file_into_pandas.html","loc":"http://chrisalbon.com/python/load_json_file_into_pandas.html"},{"title":"Loading scikit-learn's Boston Housing Dataset","text":"Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Boston Housing Dataset The Boston housing dataset is a famous dataset from the 1970s. It contains 506 observations on housing prices around Boston. It is often used in regression examples and contains 15 features. # Load digits dataset boston = datasets . load_boston () # Create feature matrix X = boston . data # Create target vector y = boston . target # View the first observation's feature values X [ 0 ] array([ 6.32000000e-03, 1.80000000e+01, 2.31000000e+00, 0.00000000e+00, 5.38000000e-01, 6.57500000e+00, 6.52000000e+01, 4.09000000e+00, 1.00000000e+00, 2.96000000e+02, 1.53000000e+01, 3.96900000e+02, 4.98000000e+00]) As you can see, the features are not standardized. This is more easily seen if we display the values as decimals: # Display each feature value of the first observation as floats [ '{:f}' . format ( x ) for x in X [ 0 ]] ['0.006320', '18.000000', '2.310000', '0.000000', '0.538000', '6.575000', '65.200000', '4.090000', '1.000000', '296.000000', '15.300000', '396.900000', '4.980000'] Therefore, it is often beneficial and/or required to standardize the value of the features.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/loading_scikit-learns_boston_housing-dataset.html","loc":"http://chrisalbon.com/machine-learning/loading_scikit-learns_boston_housing-dataset.html"},{"title":"Loading scikit-learn's Digits Dataset","text":"Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Digits Dataset Digits is a dataset of handwritten digits. Each feature is the intensity of one pixel of an 8 x 8 image. # Load digits dataset digits = datasets . load_digits () # Create feature matrix X = digits . data # Create target vector y = digits . target # View the first observation's feature values X [ 0 ] array([ 0., 0., 5., 13., 9., 1., 0., 0., 0., 0., 13., 15., 10., 15., 5., 0., 0., 3., 15., 2., 0., 11., 8., 0., 0., 4., 12., 0., 0., 8., 8., 0., 0., 5., 8., 0., 0., 9., 8., 0., 0., 4., 11., 0., 1., 12., 7., 0., 0., 2., 14., 5., 10., 12., 0., 0., 0., 0., 6., 13., 10., 0., 0., 0.]) The observation's feature values are presented as a vector. However, by using the images method we can load the the same feature values as a matrix and then visualize the actual handwritten character: # View the first observation's feature values as a matrix digits . images [ 0 ] array([[ 0., 0., 5., 13., 9., 1., 0., 0.], [ 0., 0., 13., 15., 10., 15., 5., 0.], [ 0., 3., 15., 2., 0., 11., 8., 0.], [ 0., 4., 12., 0., 0., 8., 8., 0.], [ 0., 5., 8., 0., 0., 9., 8., 0.], [ 0., 4., 11., 0., 1., 12., 7., 0.], [ 0., 2., 14., 5., 10., 12., 0., 0.], [ 0., 0., 6., 13., 10., 0., 0., 0.]]) # Visualize the first observation's feature values as an image plt . gray () plt . matshow ( digits . images [ 0 ]) plt . show () <matplotlib.figure.Figure at 0x1068494a8>","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/loading_scikit-learns_digits-dataset.html","loc":"http://chrisalbon.com/machine-learning/loading_scikit-learns_digits-dataset.html"},{"title":"Loading scikit-learn's Iris Dataset","text":"Preliminaries # Load libraries from sklearn import datasets import matplotlib.pyplot as plt Load Iris Dataset The Iris flower dataset is one of the most famous databases for classification. It contains three classes (i.e. three species of flowers) with 50 observations per class. # Load digits dataset iris = datasets . load_iris () # Create feature matrix X = iris . data # Create target vector y = iris . target # View the first observation's feature values X [ 0 ] array([ 5.1, 3.5, 1.4, 0.2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/loading_scikit-learns_iris-dataset.html","loc":"http://chrisalbon.com/machine-learning/loading_scikit-learns_iris-dataset.html"},{"title":"Bessel's Correction","text":"Bessel's correction is the reason we use $n-1$ instead of $n$ in the calculations of sample variance and sample standard deviation. Sample variance: $$ s&#94;2 = \\frac {1}{n-1} \\sum_{i=1}&#94;n \\left(x_i - \\overline{x} \\right)&#94; 2 $$ When we calculate sample variance, we are attempting to estimate the population variance, an unknown value. To make this estimate, we estimate this unknown population variance from the mean of the squared deviations of samples from the overall sample mean. A negative sideffect of this estimation technique is that, because we are taking a sample, we are a more likely to observe observations with a smaller deviation because they are more common (e.g. they are the center of the distribution). The means that by definiton we will underestimate the population variance. Friedrich Bessel figured out that by multiplying a biased (uncorrected) sample variance $s_n&#94;2 = \\frac {1}{n} \\sum_{i=1}&#94;n \\left(x_i - \\overline{x} \\right)&#94; 2$ by $\\frac{n}{n-1}$ we will be able to reduce that bias and thus be able to make an accurate estimate of the population variance and standard deviation. The end result of that multiplication is the unbiased sample variance.","tags":"Statistics","url":"http://chrisalbon.com/statistics/bessels_correction.html","loc":"http://chrisalbon.com/statistics/bessels_correction.html"},{"title":"Linear Regression","text":"Sources: scikit-learn , DrawMyData . The purpose of this tutorial is to give a brief introduction into the logic of statistical model building used in machine learning. If you want to read more about the theory behind this tutorial, check out An Introduction To Statistical Learning . Let us get started. Preliminary import pandas as pd from sklearn import linear_model import random import numpy as np % matplotlib inline Load Data With those libraries added, let us load the dataset (the dataset is avaliable in his site's GitHub repo). # Load the data df = pd . read_csv ( '../data/simulated_data/battledeaths_n300_cor99.csv' ) # Shuffle the data's rows (This is only necessary because of the way I created # the data using DrawMyData. This would not normally be necessary with a real analysis). df = df . sample ( frac = 1 ) Explore Data Let us take a look at the first few rows of the data just to get an idea about it. # View the first few rows df . head () friendly_battledeaths enemy_battledeaths 7 8.2051 9.6154 286 88.7179 86.1538 164 14.3590 8.8462 180 38.9744 36.5385 89 93.0769 93.0769 Now let us plot the data so we can see it's structure. # Plot the two variables against eachother df . plot ( x = 'friendly_battledeaths' , y = 'enemy_battledeaths' , kind = 'scatter' ) <matplotlib.axes._subplots.AxesSubplot at 0x1145cdb00> Break Data Up Into Training And Test Datasets Now for the real work. To judge how how good our model is, we need something to test it against. We can accomplish this using a technique called cross-validation. Cross-validation can get much more complicated and powerful, but in this example we are going do the most simple version of this technique. Steps Divide the dataset into two datasets: A 'training' dataset that we will use to train our model and a 'test' dataset that we will use to judge the accuracy of that model. Train the model on the 'training' data. Apply that model to the test data's X variable, creating the model's guesses for the test data's Ys. Compare how close the model's predictions for the test data's Ys were to the actual test data Ys. # Create our predictor/independent variable # and our response/dependent variable X = df [ 'friendly_battledeaths' ] y = df [ 'enemy_battledeaths' ] # Create our test data from the first 30 observations X_test = X [ 0 : 30 ] . reshape ( - 1 , 1 ) y_test = y [ 0 : 30 ] # Create our training data from the remaining observations X_train = X [ 30 :] . reshape ( - 1 , 1 ) y_train = y [ 30 :] Train The Linear Model Let us train the model using our training data. # Create an object that is an ols regression ols = linear_model . LinearRegression () # Train the model using our training data model = ols . fit ( X_train , y_train ) View The Results Here are some basic outputs of the model, notably the coefficient and the R-squared score. # View the training model's coefficient model . coef_ array([ 0.97696721]) # View the R-Squared score model . score ( X_test , y_test ) 0.98573393818904709 Now that we have used the training data to train a model, called model , we can apply it to the test data's Xs to make predictions of the test data's Ys. Previously we used X_train and y_train to train a linear regression model, which we stored as a variable called model . The code model.predict(X_test) applies the trained model to the X_test data, data the model has never seen before to make predicted values of Y. This can easily be seen by simply running the code: # Run the model on X_test and show the first five results list ( model . predict ( X_test )[ 0 : 5 ]) [7.4633347104887342, 86.121700007313791, 13.475493202059415, 37.523931774900845, 90.380300060086256] This array of values is the model's best guesses for the values of the test data's Ys. Compare them to the actual test data Y values: # View the first five test Y values list ( y_test )[ 0 : 5 ] [9.6153999999999993, 86.153800000000004, 8.8461999999999996, 36.538499999999999, 93.076899999999995] The difference between the model's predicted values and the actual values is how is we judge as model's accuracy, because a perfectly accurate model would have residuals of zero. However, to judge a model, we want a single statistic (number) that we can use as a measure. We want this measure to capture the difference between the predicted values and the actual values across all observations in the data. The most common statistic used for quantitative Ys is the residual sum of squares : $$ RSS = \\sum_{i=1}&#94;{n}(y_{i}-f(x_{i}))&#94;{2} $$ Don't let the mathematical notation throw you off: $f(x_{i})$ is the model we trained: model.predict(X_test) $y_{i}$ is the test data's y: y_test $&#94;{2}$ is the exponent: **2 $\\sum_{i=1}&#94;{n}$ is the summation: .sum() In the residual sum of squares, for each observation we find the difference between the model's predicted Y and the actual Y, then square that difference to make all the values positive. Then we add all those squared differences together to get a single number. The final result is a statistic representing how far the model's predictions were from the real values. # Apply the model we created using the training data # to the test data, and calculate the RSS. (( y_test - model . predict ( X_test )) ** 2 ) . sum () 313.6087355571951 Note: You can also use Mean Squared Error, which is RSS divided by the degrees of freedom. But I find it helpful to think in terms of RSS. # Calculate the MSE np . mean (( model . predict ( X_test ) - y_test ) ** 2 ) 10.45362451857317","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/linear_regression.html","loc":"http://chrisalbon.com/machine-learning/linear_regression.html"},{"title":"Sort A List Of Names By Last Name","text":"Create a list of names commander_names = [ \"Alan Brooke\" , \"George Marshall\" , \"Frank Jack Fletcher\" , \"Conrad Helfrich\" , \"Albert Kesselring\" ] Sort Alphabetically By Last Name To complete the sort, we will combine three operations: lambda x: x.split(\" \") , which is a function that takes a string x and breaks it up along each blank space. This outputs a list. [-1] , which takes the last element of a list. sorted() , which sorts a list. # Sort a variable called 'commander_names' by the last elements of each name. sorted ( commander_names , key = lambda x : x . split ( \" \" )[ - 1 ]) ['Alan Brooke', 'Frank Jack Fletcher', 'Conrad Helfrich', 'Albert Kesselring', 'George Marshall']","tags":"Python","url":"http://chrisalbon.com/python/sort_a_list_by_last_name.html","loc":"http://chrisalbon.com/python/sort_a_list_by_last_name.html"},{"title":"Sort A List Of Strings By Length","text":"Create a list of names commander_names = [ \"Alan Brooke\" , \"George Marshall\" , \"Frank Jack Fletcher\" , \"Conrad Helfrich\" , \"Albert Kesselring\" ] Sort Alphabetically By Length To complete the sort, we will combine two operations: lambda x: len(x) , which returns the length of each string. sorted() , which sorts a list. # Sort a variable called 'commander_names' by the length of each string sorted ( commander_names , key = lambda x : len ( x )) ['Alan Brooke', 'George Marshall', 'Conrad Helfrich', 'Albert Kesselring', 'Frank Jack Fletcher']","tags":"Python","url":"http://chrisalbon.com/python/sort_a_list_of_strings_by_length.html","loc":"http://chrisalbon.com/python/sort_a_list_of_strings_by_length.html"},{"title":"Rename Column Headers In Pandas","text":"Originally from rgalbo on StackOverflow . Preliminaries # Import required modules import pandas as pd Create example data # Create a values as dictionary of lists raw_data = { '0' : [ 'first_name' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], '1' : [ 'last_name' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], '2' : [ 'age' , 52 , 36 , 24 , 73 ], '3' : [ 'preTestScore' , 24 , 31 , 2 , 3 ]} # Create a dataframe df = pd . DataFrame ( raw_data ) # View a dataframe df 0 1 2 3 0 first_name last_name age preTestScore 1 Molly Jacobson 52 24 2 Tina Ali 36 31 3 Jake Milner 24 2 4 Amy Cooze 73 3 Replace the header value with the first row's values # Create a new variable called 'header' from the first row of the dataset header = df . iloc [ 0 ] 0 first_name 1 last_name 2 age 3 preTestScore Name: 0, dtype: object # Replace the dataframe with a new one which does not contain the first row df = df [ 1 :] # Rename the dataframe's column values with the header variable df . rename ( columns = header ) first_name last_name age preTestScore 1 Molly Jacobson 52 24 2 Tina Ali 36 31 3 Jake Milner 24 2 4 Amy Cooze 73 3","tags":"Python","url":"http://chrisalbon.com/python/pandas_rename_column_headers.html","loc":"http://chrisalbon.com/python/pandas_rename_column_headers.html"},{"title":"Add A Column","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Delete Column %% sql -- Edit the table ALTER TABLE criminals -- Add a column called 'state' that contains text with the default value being 'CA' ADD COLUMN state text DEFAULT 'CA' [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor state 412 James Smith 15 M Santa Rosa 1 CA 234 Bill James 22 M Santa Rosa 0 CA 632 Stacy Miller 23 F Santa Rosa 0 CA 621 Betty Bob None F Petaluma 1 CA 162 Jaden Ado 49 M None 0 CA 901 Gordon Ado 32 F Santa Rosa 0 CA 512 Bill Byson 21 M Santa Rosa 0 CA 411 Bob Iton None M San Francisco 0 CA","tags":"SQL","url":"http://chrisalbon.com/sql/add_a_column.html","loc":"http://chrisalbon.com/sql/add_a_column.html"},{"title":"What Is The Probability An Economy Class Seat Is An Aisle Seat?","text":"There are two types of people in the world, aisle seaters and window seaters. I am an aisle seater, nothing is worse than limited bathroom access on a long flight. The first thing I do when I get my ticket is check to see if I have a window seat. If not, I immediately head over to the airline counter and try to get one. Last flight, on Turkish Airlines, I ran into a curious situation. I recieved my boarding pass with my seat number, 18C, but the ticket did not specify if C was an aisle seat or not. Making matters worse, the airline counter was swamped with a few dozen people. So I asked myself: given only the seat letter, C, what is the probability that it is an aisle seat? Later, on the flight, I decided to find out. Preliminaries # Import required modules import pandas as pd import numpy as np # Set plots to display in the iPython notebook % matplotlib inline Setup possible seat configurations I am a pretty frequently flyer on a variety of airlines and aircraft. There are a variety of seating configurations out there, but typically they follow some basic rules: No window cluster of seats has more than three seats. On small flights with three seats, the single seat is on the left side. No flight has more than nine rows. Based on these rules, here are the \"typical\" seating configurations from aircraft with between two and nine seats per row. A '1' codifies that a seat is an aisle seat, a '0' codifies that it is a non-aisle seat (i.e. window or middle), and 'np.nan' denotes that the aircraft has less than nine seats (this is so all the list lengths are the same). # An aircraft with two seats per row rows2 = [ 1 , 1 , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan ] # An aircraft with three seats per row rows3 = [ 1 , 1 , 0 , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan ,] # An aircraft with four seats per row rows4 = [ 0 , 1 , 1 , 0 , np . nan , np . nan , np . nan , np . nan , np . nan ] # An aircraft with five seats per row rows5 = [ 0 , 1 , 1 , 0 , 0 , np . nan , np . nan , np . nan , np . nan ] # An aircraft with six seats per row rows6 = [ 0 , 1 , 1 , 1 , 1 , 0 , np . nan , np . nan , np . nan ] # An aircraft with seven seats per row rows7 = [ 0 , 1 , 1 , 0 , 1 , 1 , 0 , np . nan , np . nan ] # An aircraft with eight seats per row rows8 = [ 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , np . nan ] # An aircraft with nine seats per row rows9 = [ 0 , 0 , 1 , 1 , 0 , 1 , 1 , 0 , 0 ] For example, in an aircraft with five seats per row, rows5 , the seating arrangement would be: window aisle aisle middle window no seat no seat no seat no seat Next, I'm take advantage of pandas row summation options, but to do this I need to wrangle the data into a pandas dataframe. Essentially I am using the pandas dataframe as a matrix. # Create a list variable of all possible aircraft configurations seating_map = [ rows2 , rows3 , rows4 , rows5 , rows6 , rows7 , rows8 , rows9 ] # Create a dataframe from the seating_map variable df = pd . DataFrame ( seating_map , columns = [ 'A' , 'B' , 'C' , 'D' , 'E' , 'F' , 'G' , 'H' , 'I' ], index = [ 'rows2' , 'rows3' , 'rows4' , 'rows5' , 'rows6' , 'rows7' , 'rows8' , 'rows9' ]) Here is all the data we need to construct our probabilities. The columns represent individual seat letters (A, B, etc.) while the rows represent the number of seats-per-row in the aircraft. # View the dataframe df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } A B C D E F G H I rows2 1 1 NaN NaN NaN NaN NaN NaN NaN rows3 1 1 0.0 NaN NaN NaN NaN NaN NaN rows4 0 1 1.0 0.0 NaN NaN NaN NaN NaN rows5 0 1 1.0 0.0 0.0 NaN NaN NaN NaN rows6 0 1 1.0 1.0 1.0 0.0 NaN NaN NaN rows7 0 1 1.0 0.0 1.0 1.0 0.0 NaN NaN rows8 0 0 1.0 1.0 1.0 1.0 0.0 0.0 NaN rows9 0 0 1.0 1.0 0.0 1.0 1.0 0.0 0.0 Calculate aisle probability Because each aircraft seats-per-row configuration (i.e. row) is binary (1 if aisle, 0 if non-aisle), the probability that a seat is an aisle is simply the mean value of each seat letter (i.e. column). # Create a list wherein each element is the mean value of a column aisle_probability = [ df [ 'A' ] . mean (), df [ 'B' ] . mean (), df [ 'C' ] . mean (), df [ 'D' ] . mean (), df [ 'E' ] . mean (), df [ 'F' ] . mean (), df [ 'G' ] . mean (), df [ 'H' ] . mean (), df [ 'I' ] . mean ()] # Display the variable aisle_probability [0.25, 0.75, 0.8571428571428571, 0.5, 0.6, 0.75, 0.3333333333333333, 0.0, 0.0] So there you have it, the probability that each seat letter is an aisle. However, we can make the presentation a little more intituative. Visualize seat letter probabilities The most obvious visualization to convey the probabilities would be seat letters on the x-axis and probabilities on the y-axis. Panda's plot function makes that easy. # Create a list of strings to use as the x-axis labels seats = [ 'Seat A' , 'Seat B' , 'Seat C' , 'Seat D' , 'Seat E' , 'Seat F' , 'Seat G' , 'Seat H' , 'Seat I' ] # Plot the probabilities, using 'seats' as the index as a bar chart pd . Series ( aisle_probability , index = seats ) . plot ( kind = 'bar' , # set y to range between 0 and 1 ylim = [ 0 , 1 ], # set the figure size figsize = [ 10 , 6 ], # set the figure title title = 'Probabilty of being an Aisle Seat in Economy Class' ) <matplotlib.axes._subplots.AxesSubplot at 0x10f1231d0> So there we have it! If given a boarding pass with seat C you have a 86% probability of being in an aisle seat! I hope this was helpful!","tags":"Blog","url":"http://chrisalbon.com/blog/aisle_seat_probabilities.html","loc":"http://chrisalbon.com/blog/aisle_seat_probabilities.html"},{"title":"All Combinations For A List Of Objects","text":"Preliminary # Import combinations with replacements from itertools from itertools import combinations_with_replacement Create a list of objects # Create a list of objects to combine list_of_objects = [ 'warplanes' , 'armor' , 'infantry' ] Find all combinations (with replacement) for the list # Create an empty list object to hold the results of the loop combinations = [] # Create a loop for every item in the length of list_of_objects, that, for i in list ( range ( len ( list_of_objects ))): # Finds every combination (with replacement) for each object in the list combinations . append ( list ( combinations_with_replacement ( list_of_objects , i + 1 ))) # View the results combinations [[('warplanes',), ('armor',), ('infantry',)], [('warplanes', 'warplanes'), ('warplanes', 'armor'), ('warplanes', 'infantry'), ('armor', 'armor'), ('armor', 'infantry'), ('infantry', 'infantry')], [('warplanes', 'warplanes', 'warplanes'), ('warplanes', 'warplanes', 'armor'), ('warplanes', 'warplanes', 'infantry'), ('warplanes', 'armor', 'armor'), ('warplanes', 'armor', 'infantry'), ('warplanes', 'infantry', 'infantry'), ('armor', 'armor', 'armor'), ('armor', 'armor', 'infantry'), ('armor', 'infantry', 'infantry'), ('infantry', 'infantry', 'infantry')]] # Flatten the list of lists into just a list combinations = [ i for row in combinations for i in row ] # View the results combinations [('warplanes',), ('armor',), ('infantry',), ('warplanes', 'warplanes'), ('warplanes', 'armor'), ('warplanes', 'infantry'), ('armor', 'armor'), ('armor', 'infantry'), ('infantry', 'infantry'), ('warplanes', 'warplanes', 'warplanes'), ('warplanes', 'warplanes', 'armor'), ('warplanes', 'warplanes', 'infantry'), ('warplanes', 'armor', 'armor'), ('warplanes', 'armor', 'infantry'), ('warplanes', 'infantry', 'infantry'), ('armor', 'armor', 'armor'), ('armor', 'armor', 'infantry'), ('armor', 'infantry', 'infantry'), ('infantry', 'infantry', 'infantry')]","tags":"Python","url":"http://chrisalbon.com/python/all_combinations_of_a_list_of_objects.html","loc":"http://chrisalbon.com/python/all_combinations_of_a_list_of_objects.html"},{"title":"Apply Operations Over Items In A List","text":"Method 1: map() # Create a list of casualties from battles battleDeaths = [ 482 , 93 , 392 , 920 , 813 , 199 , 374 , 237 , 244 ] # Create a function that updates all battle deaths by adding 100 def updated ( x ): return x + 100 # Create a list that applies updated() to all elements of battleDeaths list ( map ( updated , battleDeaths )) [582, 193, 492, 1020, 913, 299, 474, 337, 344] Method 2: for x in y # Create a list of deaths casualties = [ 482 , 93 , 392 , 920 , 813 , 199 , 374 , 237 , 244 ] # Create a variable where we will put the updated casualty numbers casualtiesUpdated = [] # Create a function that for each item in casualties, adds 10 for x in casualties : casualtiesUpdated . append ( x + 100 ) # View casualties variables casualtiesUpdated [582, 193, 492, 1020, 913, 299, 474, 337, 344] Method 3: lambda functions # Map the lambda function x() over casualties list ( map (( lambda x : x + 100 ), casualties )) [582, 193, 492, 1020, 913, 299, 474, 337, 344]","tags":"Python","url":"http://chrisalbon.com/python/apply_operations_over_items_in_lists.html","loc":"http://chrisalbon.com/python/apply_operations_over_items_in_lists.html"},{"title":"Applying Functions To List Items","text":"Create a list of regiment names regimentNames = [ 'Night Riflemen' , 'Jungle Scouts' , 'The Dragoons' , 'Midnight Revengence' , 'Wily Warriors' ] Using A For Loop Create a for loop goes through the list and capitalizes each # create a variable for the for loop results regimentNamesCapitalized_f = [] # for every item in regimentNames for i in regimentNames : # capitalize the item and add it to regimentNamesCapitalized_f regimentNamesCapitalized_f . append ( i . upper ()) # View the outcome regimentNamesCapitalized_f ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS'] Using Map() Create a lambda function that capitalizes x capitalizer = lambda x : x . upper () Map the capitalizer function to regimentNames, convert the map into a list, and view the variable regimentNamesCapitalized_m = list ( map ( capitalizer , regimentNames )); regimentNamesCapitalized_m ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS'] Using List Comprehension Apply the expression x.upper to each item in the list called regiment names. Then view the output regimentNamesCapitalized_l = [ x . upper () for x in regimentNames ]; regimentNamesCapitalized_l ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS']","tags":"Python","url":"http://chrisalbon.com/python/applying_functions_to_list_items.html","loc":"http://chrisalbon.com/python/applying_functions_to_list_items.html"},{"title":"Arithmetic Basics","text":"Create some simulated variables x = 6 y = 9 x plus y x + y 15 x minus y x - y -3 x times y x * y 54 the remainder of x divided by y x % y 6 x divided by y x / y 0.6666666666666666 x divided by y (floor) (i.e. the quotient) x // y 0 x raised to the y power x ** y 10077696 x plus y, then divide by x ( x + y ) / x 2.5 Classics vs. floor division. This varies between 2.x and 3.x. Classic divison of 3 by 5 3 / 5 0.6 Floor divison of 3 by 5. This means it truncates any remainder down the its \"floor\" 3 // 5 0","tags":"Python","url":"http://chrisalbon.com/python/arithmetic_basics.html","loc":"http://chrisalbon.com/python/arithmetic_basics.html"},{"title":"Assignment Operators","text":"Create some variables a = 2 b = 1 c = 0 d = 3 Assigns values from right side to left side c = a + b c 3 Add right to the left and assign the result to left (c = a + c) c += a c 5 Subtract right from the left and assign the result to left (c = a - c) c -= a c 3 Multiply right with the left and assign the result to left (c = a * c) c *= a c 6 Divide left with the right and assign the result to left (c = c / a) c /= a c 3.0 Takes modulus using two operands and assign the result to left (a = d % a) d %= a d 1 Exponential (power) calculation on operators and assign value to the left (d = d &#94; a) d **= a d 1 Floor division on operators and assign value to the left (d = d // a) d //= a d 0","tags":"Python","url":"http://chrisalbon.com/python/assignment_operators.html","loc":"http://chrisalbon.com/python/assignment_operators.html"},{"title":"Automatically Add Keys To Rows","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data With 'pid' As An Auto-Generated Primary Key %% sql -- Create a table of criminals with pid being a primary key integer that is auto - incremented CREATE TABLE criminals ( pid INTEGER PRIMARY KEY AUTOINCREMENT , name , age , sex , city , minor ); -- Add a single row with a null value for pid INSERT INTO criminals VALUES ( NULL , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 1 James Smith 15 M Santa Rosa 1 Added More Rows With NULL Values For pid %% sql INSERT INTO criminals VALUES ( NULL , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( NULL , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( NULL , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( NULL , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( NULL , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( NULL , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( NULL , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 1 James Smith 15 M Santa Rosa 1 2 Bill James 22 M Santa Rosa 0 3 Stacy Miller 23 F Santa Rosa 0 4 Betty Bob None F Petaluma 1 5 Jaden Ado 49 M None 0 6 Gordon Ado 32 F Santa Rosa 0 7 Bill Byson 21 M Santa Rosa 0 8 Bob Iton None M San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/automatically_add_keys_to_rows.html","loc":"http://chrisalbon.com/sql/automatically_add_keys_to_rows.html"},{"title":"Drilling Down With Beautiful Soup","text":"Preliminaries # Import required modules import requests from bs4 import BeautifulSoup import pandas as pd Download the HTML and create a Beautiful Soup object # Create a variable with the URL to this tutorial url = 'http://en.wikipedia.org/wiki/List_of_A_Song_of_Ice_and_Fire_characters' # Scrape the HTML at the url r = requests . get ( url ) # Turn the HTML into a Beautiful Soup object soup = BeautifulSoup ( r . text , \"lxml\" ) If we looked at the soup object, we'd see that the names we want are in a heirarchical list. In psuedo-code, it looks like: class=toclevel-1 span=toctext class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES To get the CHARACTER NAMES, we are going to need to drill down to grap into loclevel-2 and grab the toctext Setting up where to put the results # Create a variable to score the scraped data in character_name = [] Drilling down with a forloop # for each item in all the toclevel-2 li items # (except the last three because they are not character names), for item in soup . find_all ( 'li' ,{ 'class' : 'toclevel-2' })[: - 3 ]: # find each span with class=toctext, for post in item . find_all ( 'span' ,{ 'class' : 'toctext' }): # add the stripped string of each to character_name, one by one character_name . append ( post . string . strip ()) Results # View all the character names character_name ['Eddard Stark', 'Catelyn Stark', 'Robb Stark', 'Sansa Stark', 'Arya Stark', 'Bran Stark', 'Rickon Stark', 'Jon Snow', 'Benjen Stark', 'Lyanna Stark', 'Roose Bolton', 'Ramsay Bolton', 'Rickard Karstark', 'Alys Karstark', 'Wyman Manderly', 'Hodor', 'Osha', 'Jeyne Poole', 'Jojen and Meera Reed', 'Jeyne Westerling', 'Aegon V Targaryen', 'Aerys II Targaryen', 'Rhaegar Targaryen', 'Viserys Targaryen', 'Daenerys Targaryen', 'Aegon VI Targaryen', 'Jon Connington', 'Jorah Mormont', 'Brynden Rivers', 'Missandei', 'Daario Naharis', 'Grey Worm', 'Maekar I Targaryen', 'House Blackfyre', 'Tywin Lannister', 'Cersei Lannister', 'Jaime Lannister', 'Tyrion Lannister', 'Joffrey Baratheon', 'Myrcella Baratheon', 'Tommen Baratheon', 'Kevan Lannister', 'Lancel Lannister', 'Bronn', 'Gregor Clegane', 'Sandor Clegane', 'Podrick Payne', 'Robert Baratheon', 'Stannis Baratheon', 'Selyse Florent', 'Renly Baratheon', 'Shireen Baratheon', 'Melisandre', 'Davos Seaworth', 'Brienne of Tarth', 'Beric Dondarrion', 'Gendry', 'Edric Storm', 'Jon Arryn', 'Lysa Arryn', 'Robert Arryn', 'Yohn Royce', 'Anya Waynwood', 'Nestor Royce', 'Balon Greyjoy', 'Asha Greyjoy', 'Theon Greyjoy', 'Euron Greyjoy', 'Victarion Greyjoy', 'Aeron Greyjoy', 'Rodrik Harlaw', 'Doran Martell', 'Arianne Martell', 'Quentyn Martell', 'Trystane Martell', 'Elia Martell', 'Oberyn Martell', 'Ellaria Sand', 'The Sand Snakes', 'Areo Hotah', 'Hoster Tully', 'Edmure Tully', 'Brynden Tully', 'Walder Frey', 'Mace Tyrell', 'Loras Tyrell', 'Margaery Tyrell', 'Olenna Tyrell', 'Randyll Tarly', 'Jeor Mormont', 'Maester Aemon', 'Yoren', 'Samwell Tarly', 'Janos Slynt', 'Alliser Thorne', 'Mance Rayder', 'Ygritte', 'Craster', 'Gilly', 'Val', 'Lord of Bones', 'Bowen Marsh', 'Eddison Tollett', 'Tormund Giantsbane', 'Varamyr Sixskins', 'Petyr Baelish', 'Varys', 'Pycelle', 'Barristan Selmy', 'Arys Oakheart', 'Ilyn Payne', 'Qyburn', 'The High Sparrow', 'Meryn Trant', 'Balon Swann', 'Khal Drogo', 'Syrio Forel', \"Jaqen H'ghar\", 'Illyrio Mopatis', 'Thoros of Myr', 'Ser Duncan the Tall', 'Hizdahr zo Loraq', 'Yezzan zo Qaggaz', 'Tycho Nestoris', 'The Waif', 'Meribald', 'Septa Unella'] Quick analysis: Which house has the most main characters? # Create a list object where to store the for loop results houses = [] # For each element in the character_name list, for name in character_name : # split up the names by a blank space and select the last element # this works because it is the last name if they are a house, # but the first name if they only have one name, # Then append each last name to the houses list houses . append ( name . split ( ' ' )[ - 1 ]) # Convert houses into a pandas series (so we can use value_counts()) houses = pd . Series ( houses ) # Count the number of times each name/house name appears houses . value_counts () Stark 9 Targaryen 7 Baratheon 7 Martell 6 Greyjoy 6 Lannister 6 Tyrell 4 Tully 3 Arryn 3 Tarly 2 Bolton 2 Mormont 2 Royce 2 Clegane 2 Payne 2 Karstark 2 Storm 1 Poole 1 Giantsbane 1 Connington 1 Snakes 1 Varys 1 Westerling 1 Rayder 1 Gilly 1 Pycelle 1 Sparrow 1 Drogo 1 Hodor 1 Worm 1 .. Qaggaz 1 Harlaw 1 Forel 1 Slynt 1 Manderly 1 Craster 1 Frey 1 Oakheart 1 Tarth 1 Selmy 1 Trant 1 Qyburn 1 Rivers 1 Tollett 1 Reed 1 Mopatis 1 Dondarrion 1 Florent 1 Waynwood 1 Yoren 1 Baelish 1 Osha 1 Unella 1 Bronn 1 Gendry 1 Myr 1 Thorne 1 Nestoris 1 Tall 1 H'ghar 1 Length: 78, dtype: int64","tags":"Python","url":"http://chrisalbon.com/python/beautiful_soup_drill_down.html","loc":"http://chrisalbon.com/python/beautiful_soup_drill_down.html"},{"title":"Beautiful Soup Basic HTML Scraping","text":"Import the modules # Import required modules import requests from bs4 import BeautifulSoup Scrap the html and turn into a beautiful soup object # Create a variable with the url url = 'http://chrisralbon.com' # Use requests to get the contents r = requests . get ( url ) # Get the text of the contents html_content = r . text # Convert the html content into a beautiful soup object soup = BeautifulSoup ( html_content , 'lxml' ) Select the website's title # View the title tag of the soup object soup . title <title> Chris Albon </title> Website title tag's string # View the string within the title tag soup . title . string 'Chris Albon' First paragraph tag # view the paragraph tag of the soup soup . p <p> I am a <a href= \"./pages/about.html\" > data scientist originally trained as a quantitative political scientist </a> . I specialize in the technical and organizational aspects of applying data science to political and social issues. </p> The parent of the title tag soup . title . parent . name 'head' The first link tag soup . a <a class= \"navbar-brand\" href= \".\" > Chris Albon </a> Find all the link tags and list the first five soup . find_all ( 'a' )[ 0 : 5 ] [ <a class= \"navbar-brand\" href= \".\" > Chris Albon </a> , <a aria-expanded= \"false\" aria-haspopup= \"true\" class= \"dropdown-toggle\" data-toggle= \"dropdown\" href= \"#\" role= \"button\" > About <span class= \"caret\" ></span></a> , <a href= \"./pages/about.html\" > About Chris </a> , <a href= \"https://github.com/chrisalbon\" > GitHub </a> , <a href= \"https://twitter.com/chrisalbon\" > Twitter </a> ] The string inside the first paragraph tag soup . p . string Find all the h2 tags and list the first five soup . find_all ( 'h2' )[ 0 : 5 ] [ <h2 class= \"homepage_category_title\" > Articles </h2> , <h2 class= \"homepage_category_title\" > Projects </h2> , <h2 class= \"homepage_category_title\" > Python </h2> , <h2 class= \"homepage_category_title\" > R Stats </h2> , <h2 class= \"homepage_category_title\" > Regex </h2> ] Find all the links on the page and list the first five soup . find_all ( 'a' )[ 0 : 5 ] [ <a class= \"navbar-brand\" href= \".\" > Chris Albon </a> , <a aria-expanded= \"false\" aria-haspopup= \"true\" class= \"dropdown-toggle\" data-toggle= \"dropdown\" href= \"#\" role= \"button\" > About <span class= \"caret\" ></span></a> , <a href= \"./pages/about.html\" > About Chris </a> , <a href= \"https://github.com/chrisalbon\" > GitHub </a> , <a href= \"https://twitter.com/chrisalbon\" > Twitter </a> ]","tags":"Python","url":"http://chrisalbon.com/python/beautiful_soup_html_basics.html","loc":"http://chrisalbon.com/python/beautiful_soup_html_basics.html"},{"title":"Big-O Notation","text":"Want to learn more? Check out Data Structures and Algorithms in Python Big-O notation is used to classify the worst-case \"speed\" of an algorithm by looking at the order of magnitude of execution time. From best to worst, some common Big-O's are: O(1) O(log n) O(n) O(n log n) O(n 2 ) O(n 3 ) O(2 n ) Below are some examples of a few of these. Create Data # Create a list 100 elements long n = list ( range ( 100 )) O(1) - Constant Time-Complexity The length of n has no bearing on the number of steps required to complete the function. This is the ideal. # Create a function that, regardless of the length of n, def constant ( n ): # returns 1 return 'Number of operations: ' + str ( 1 ) constant ( n ) 'Number of operations: 1' O(log n) - Logarithmic Time-Complexity # Create a function that def logarithmic ( n ): # Creates a list of the operations performed operations = [] # While n is longer than 1, repeat this: while len ( n ) > 1 : # Find half the lengh of n half_length = int ( len ( n ) / 2 ) # Add half the values of n to operations operations = operations + n [ 0 : half_length ] # make n half the length of itself, then restart the loop n = n [ 0 : half_length ] # Return the number of operations return 'Number of operations: ' + str ( len ( operations )) logarithmic ( n ) 'Number of operations: 97' O(n) - Linear Time-Complexity def linear ( n ): # Creates a list of the operations performed operations = [] # For every item in n for item in n : # Add the item to operations operations . append ( item ) # Return the number of operations return 'Number of operations: ' + str ( len ( operations )) linear ( n ) 'Number of operations: 100' O(n 2 ) - Quadratic Time-Complexity def quadratic ( n ): # Creates a list of the operations performed operations = [] # For every item in n, for i in n : # Look at every item item in n for j in n : # and append it to operations operations . append ( j ) # Return the number of operations return 'Number of operations: ' + str ( len ( operations )) quadratic ( n ) 'Number of operations: 10000'","tags":"Algorithms","url":"http://chrisalbon.com/algorithms/big-o_notation.html","loc":"http://chrisalbon.com/algorithms/big-o_notation.html"},{"title":"Binary Search","text":"Create Sorted List sorted_list = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 12 , 14 , 16 , 18 , 20 ] print ( sorted_list ) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20] Create Binary Search Algorithm def binary_search ( sorted_list , target ): '''This function inputs a sorted list and a target value to find and returns ....''' # Create variables for the index of the first and last elements start = 0 end = len ( sorted_list ) - 1 while end >= start : # Create a variable for the index of the middle term middle = start + ( end - start ) // 2 # // is integer division in Python 3.X # If the target value is less than the middle value of the search area if target < sorted_list [ middle ]: # Cut the list in half by making the new end value the old middle value minus 1 # The minus one is because we already ruled the middle value out, so we can ignore it end = middle - 1 # Else, if the target value is greater than the middle value of the search area elif target > sorted_list [ middle ]: # Cut the list in half by making the new start value the old middle value plus 1 # The plus one is because we already ruled the middle value out, so we can ignore it start = middle + 1 # If it's not too high or too low, it must be just right, return the location else : return ( \"Found it at index: {}\" . format ( middle )) # If we've fallen out of the while loop the target value is not in the list return print ( \"Not in list!\" ) Conduct Binary Search # Run binary search binary_search ( sorted_list , 2 ) 'Found it at index: 1' Thanks for Julius for the improved code.","tags":"Algorithms","url":"http://chrisalbon.com/algorithms/binary_search.html","loc":"http://chrisalbon.com/algorithms/binary_search.html"},{"title":"Breaking Up String Variables","text":"Basic name assignment variableName = 'This is a string.' List assignment One , Two , Three = [ 1 , 2 , 3 ] Break up a string into variables firstLetter , secondLetter , thirdLetter , fourthLetter = 'Bark' firstLetter 'B' secondLetter 'a' thirdLetter 'r' fourthLetter 'k' Breaking up a number into separate variables firstNumber , secondNumber , thirdNumber , fourthNumber = '9485' firstNumber '9' secondNumber '4' thirdNumber '8' fourthNumber '5' Assign the first letter of 'spam' into varible a, assign all the remaining letters to variable b a , * b = 'spam' a 's' b ['p', 'a', 'm']","tags":"Python","url":"http://chrisalbon.com/python/breaking_up_string_variables.html","loc":"http://chrisalbon.com/python/breaking_up_string_variables.html"},{"title":"Brute Force D20 Roll Simulator","text":"This snippet is a completely inefficient simulator of a 20 sided dice. To create a \"successful roll\" the snippet has to generate dozens of random numbers. Import random module import random Create a variable with a TRUE value rolling = True Create a while loop that rolls until the first digit is 2 or less and the second digit is 10 or less # while rolling is true while rolling : # create x, a random number between 0 and 99 x = random . randint ( 0 , 99 ) # create y, a random number between 0 and 99 y = random . randint ( 0 , 99 ) # if x is less than 2 and y is between 0 and 10 if x < 2 and 0 < y < 10 : # Print the outcome print ( 'You rolled a {0}{1}.' . format ( x , y )) # And set roll of False rolling = False # Otherwise else : # Try again print ( 'Trying again.' ) Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. You rolled a 16.","tags":"Python","url":"http://chrisalbon.com/python/brute_force_d20_simulator.html","loc":"http://chrisalbon.com/python/brute_force_d20_simulator.html"},{"title":"Cartesian Product","text":"Preliminaries # import pandas as pd import pandas as pd Create Data # Create two lists i = [ 1 , 2 , 3 , 4 , 5 ] j = [ 1 , 2 , 3 , 4 , 5 ] Calculate Cartesian Product (Method 1) # List every single x in i with every single y (i.e. Cartesian product) [( x , y ) for x in i for y in j ] [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5)] Calculate Cartesian Product (Method 2) # An alternative way to do the cartesian product # import itertools import itertools # for two sets, find the the cartisan product for i in itertools . product ([ 1 , 2 , 3 , 4 , 5 ], [ 1 , 2 , 3 , 4 , 5 ]): # and print it print ( i ) (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (5, 1) (5, 2) (5, 3) (5, 4) (5, 5)","tags":"Python","url":"http://chrisalbon.com/python/cartesian_product.html","loc":"http://chrisalbon.com/python/cartesian_product.html"},{"title":"Cleaning Text","text":"Create some raw text # Create a list of three strings. incoming_reports = [ \"We are attacking on their left flank but are losing many men.\" , \"We cannot see the enemy army. Nothing else to report.\" , \"We are ready to attack but are waiting for your orders.\" ] Seperate by word # import word tokenizer from nltk.tokenize import word_tokenize # Apply word_tokenize to each element of the list called incoming_reports tokenized_reports = [ word_tokenize ( report ) for report in incoming_reports ] # View tokenized_reports tokenized_reports [['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men', '.'], ['We', 'can', 'not', 'see', 'the', 'enemy', 'army', '.', 'Nothing', 'else', 'to', 'report', '.'], ['We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders', '.']] # Import regex import re # Import string import string regex = re . compile ( '[ %s ]' % re . escape ( string . punctuation )) #see documentation here: http://docs.python.org/2/library/string.html tokenized_reports_no_punctuation = [] for review in tokenized_reports : new_review = [] for token in review : new_token = regex . sub ( u '' , token ) if not new_token == u '' : new_review . append ( new_token ) tokenized_reports_no_punctuation . append ( new_review ) tokenized_reports_no_punctuation [['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men'], ['We', 'can', 'not', 'see', 'the', 'enemy', 'army', 'Nothing', 'else', 'to', 'report'], ['We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders']] Remove filler words from nltk.corpus import stopwords tokenized_reports_no_stopwords = [] for report in tokenized_reports_no_punctuation : new_term_vector = [] for word in report : if not word in stopwords . words ( 'english' ): new_term_vector . append ( word ) tokenized_reports_no_stopwords . append ( new_term_vector ) tokenized_reports_no_stopwords [['We', 'attacking', 'left', 'flank', 'losing', 'many', 'men'], ['We', 'see', 'enemy', 'army', 'Nothing', 'else', 'report'], ['We', 'ready', 'attack', 'waiting', 'orders']]","tags":"Python","url":"http://chrisalbon.com/python/cleaning_text.html","loc":"http://chrisalbon.com/python/cleaning_text.html"},{"title":"Continue And Break Loops","text":"Import the random module import random Create a while loop # set running to true running = True # while running is true while running : # Create a random integer between 0 and 5 s = random . randint ( 0 , 5 ) # If the integer is less than 3 if s < 3 : # Print this print ( 'It is too small, starting over.' ) # Reset the next interation of the loop # (i.e skip everything below and restart from the top) continue # If the integer is 4 if s == 4 : running = False # Print this print ( 'It is 4! Changing running to false' ) # If the integer is 5, if s == 5 : # Print this print ( 'It is 5! Breaking Loop!' ) # then stop the loop break It is too small, starting over. It is too small, starting over. It is too small, starting over. It is 5! Breaking Loop!","tags":"Python","url":"http://chrisalbon.com/python/continue_and_break_loops.html","loc":"http://chrisalbon.com/python/continue_and_break_loops.html"},{"title":"Convert A String Categorical Variable To A Numeric Variable","text":"Originally from: Data Origami . import modules import pandas as pd Create dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 strong 1 1 2 1 weak 2 1 3 0 normal 3 2 1 1 weak 4 2 2 0 strong Create a function that converts all values of df['score'] into numbers def score_to_numeric ( x ): if x == 'strong' : return 3 if x == 'normal' : return 2 if x == 'weak' : return 1 Apply the function to the score variable df [ 'score_num' ] = df [ 'score' ] . apply ( score_to_numeric ) df patient obs treatment score score_num 0 1 1 0 strong 3 1 1 2 1 weak 1 2 1 3 0 normal 2 3 2 1 1 weak 1 4 2 2 0 strong 3","tags":"Python","url":"http://chrisalbon.com/python/convert_categorical_to_numeric.html","loc":"http://chrisalbon.com/python/convert_categorical_to_numeric.html"},{"title":"Copy Data From One Table To Another","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Table %% sql -- Create a table of criminals_1 CREATE TABLE criminals_1 ( pid , name , age , sex , city , minor ); INSERT INTO criminals_1 VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals_1 VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals_1 VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals_1 VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals_1 VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals_1 VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals_1 VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals_1 VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select all SELECT * -- From the table 'criminals_1' FROM criminals_1 pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Create New Empty Table %% sql -- Create a table called criminals_2 CREATE TABLE criminals_2 ( pid , name , age , sex , city , minor ); [] Copy Contents Of First Table Into Empty Table %% sql -- Insert into the empty table INSERT INTO criminals_2 -- Everything SELECT * -- From the first table FROM criminals_1 ; [] View Previously Empty Table %% sql -- Select everything SELECT * -- From the previously empty table FROM criminals_2 pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0","tags":"SQL","url":"http://chrisalbon.com/sql/copy_data_between_tables.html","loc":"http://chrisalbon.com/sql/copy_data_between_tables.html"},{"title":"Create An Index For A Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Create An Index Using The Column 'pid' As The Unique ID %% sql -- Create a index called uid CREATE INDEX uid -- For the table 'criminals' and the column 'pid' ON criminals ( pid ) [] Note: Use 'CREATE UNIQUE INDEX' if you want the index to not contain any duplicates.","tags":"SQL","url":"http://chrisalbon.com/sql/create_index_for_a_table.html","loc":"http://chrisalbon.com/sql/create_index_for_a_table.html"},{"title":"Creating Lists From Dictionary Keys And Values","text":"Create a dictionary dict = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'fireReports' : [ 4 , 24 , 31 , 2 , 3 ]} Create a list from the dictionary keys # Create a list of keys list ( dict . keys ()) ['fireReports', 'year', 'county'] Create a list from the dictionary values # Create a list of values list ( dict . values ()) [[4, 24, 31, 2, 3], [2012, 2012, 2013, 2014, 2014], ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma']]","tags":"Python","url":"http://chrisalbon.com/python/create_list_from_dictionary_keys_and_values.html","loc":"http://chrisalbon.com/python/create_list_from_dictionary_keys_and_values.html"},{"title":"Create Data Type And Value Rules","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create New Table With Constraints On What Data Can Be Inserted %% sql -- Create a table of criminals CREATE TABLE criminals ( -- With a prisoner ID ( pid ) that is a primary key and cannot be null pid INT PRIMARY KEY NOT NULL , -- With a name variable whose default value is John Doe name TEXT DEFAULT 'John Doe' , -- With an age variable that is an integer and has to be between 0 and 100 age INT CHECK ( 0 < age < 100 ) ); [] Add Data To Table %% sql INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 ); INSERT INTO criminals VALUES ( 632 , 'Bill Steve' , 23 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL ); [] View Table %% sql SELECT * FROM criminals pid name age 412 James Smith 15 234 Bill James 22 632 Bill Steve 23 621 Betty Bob None 162 Jaden Ado 49 901 Gordon Ado 32 512 Bill Byson 21 411 Bob Iton None","tags":"SQL","url":"http://chrisalbon.com/sql/create_rules_constraining_data.html","loc":"http://chrisalbon.com/sql/create_rules_constraining_data.html"},{"title":"Creating Numpy Arrays","text":"# Import Modules import numpy as np # Create a list regimentSize = [ 534 , 5468 , 6546 , 542 , 9856 , 4125 ] # Create a ndarray from the regimentSize list regimentSizeArray = np . array ( regimentSize ); regimentSizeArray array([ 534, 5468, 6546, 542, 9856, 4125]) # What are the number of dimensions of the array? regimentSizeArray . ndim 1 # What is the shape of the array? regimentSizeArray . shape (6,) Nested Lists To Multidimensional Arrays # Create two lists regimentSizePreWar = [ 534 , 5468 , 6546 , 542 , 9856 , 4125 ] regimentSizePostWar = [ 234 , 255 , 267 , 732 , 235 , 723 ] # Create a ndarray from a nested list regimentSizePrePostArray = np . array ([ regimentSizePreWar , regimentSizePostWar ]); regimentSizePrePostArray array([[ 534, 5468, 6546, 542, 9856, 4125], [ 234, 255, 267, 732, 235, 723]]) # What are the number of dimensions of the array? regimentSizePrePostArray . ndim 2 # What is the shape of the array? regimentSizePrePostArray . shape (2, 6)","tags":"Python","url":"http://chrisalbon.com/python/creating_numpy_arrays.html","loc":"http://chrisalbon.com/python/creating_numpy_arrays.html"},{"title":"Convert A CSV Into Python Code To Recreate It","text":"This might seem like a strange bit of code, but it serves a very valuable (though niche) function. I prefer to the code in my tutorials to not rely on outside data to run. That is, dieally the data is created within the code itself, rather than requiring loading an data from a csv file. Obviously this is not reasonable for real analyses, but for tutorials it can make everything simpler and easier. However, this preference to embed the generation of data in the snippets themselves becomes a problem when I want to use data found in existing datasets. So, I created this script to complete one simple task: To take a dataset and generate the python code required to recreate it. Preliminaries # Import the pandas package import pandas as pd Load the external dataset # Load the csv file as a pandas dataframe df_original = pd . read_csv ( 'http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv' ) df = pd . read_csv ( 'http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv' ) Print the code required to create that dataset # Print the code to create the dataframe print ( '==============================' ) print ( 'RUN THE CODE BELOW THIS LINE' ) print ( '==============================' ) print ( 'raw_data =' , df . to_dict ( orient = 'list' )) print ( 'df = pd.DataFrame(raw_data, columns = ' + str ( list ( df_original )) + ')' ) ============================== RUN THE CODE BELOW THIS LINE ============================== raw_data = {'Sepal.Length': [5.0999999999999996, 4.9000000000000004, 4.7000000000000002, 4.5999999999999996, 5.0, 5.4000000000000004, 4.5999999999999996, 5.0, 4.4000000000000004, 4.9000000000000004, 5.4000000000000004, 4.7999999999999998, 4.7999999999999998, 4.2999999999999998, 5.7999999999999998, 5.7000000000000002, 5.4000000000000004, 5.0999999999999996, 5.7000000000000002, 5.0999999999999996, 5.4000000000000004, 5.0999999999999996, 4.5999999999999996, 5.0999999999999996, 4.7999999999999998, 5.0, 5.0, 5.2000000000000002, 5.2000000000000002, 4.7000000000000002, 4.7999999999999998, 5.4000000000000004, 5.2000000000000002, 5.5, 4.9000000000000004, 5.0, 5.5, 4.9000000000000004, 4.4000000000000004, 5.0999999999999996, 5.0, 4.5, 4.4000000000000004, 5.0, 5.0999999999999996, 4.7999999999999998, 5.0999999999999996, 4.5999999999999996, 5.2999999999999998, 5.0, 7.0, 6.4000000000000004, 6.9000000000000004, 5.5, 6.5, 5.7000000000000002, 6.2999999999999998, 4.9000000000000004, 6.5999999999999996, 5.2000000000000002, 5.0, 5.9000000000000004, 6.0, 6.0999999999999996, 5.5999999999999996, 6.7000000000000002, 5.5999999999999996, 5.7999999999999998, 6.2000000000000002, 5.5999999999999996, 5.9000000000000004, 6.0999999999999996, 6.2999999999999998, 6.0999999999999996, 6.4000000000000004, 6.5999999999999996, 6.7999999999999998, 6.7000000000000002, 6.0, 5.7000000000000002, 5.5, 5.5, 5.7999999999999998, 6.0, 5.4000000000000004, 6.0, 6.7000000000000002, 6.2999999999999998, 5.5999999999999996, 5.5, 5.5, 6.0999999999999996, 5.7999999999999998, 5.0, 5.5999999999999996, 5.7000000000000002, 5.7000000000000002, 6.2000000000000002, 5.0999999999999996, 5.7000000000000002, 6.2999999999999998, 5.7999999999999998, 7.0999999999999996, 6.2999999999999998, 6.5, 7.5999999999999996, 4.9000000000000004, 7.2999999999999998, 6.7000000000000002, 7.2000000000000002, 6.5, 6.4000000000000004, 6.7999999999999998, 5.7000000000000002, 5.7999999999999998, 6.4000000000000004, 6.5, 7.7000000000000002, 7.7000000000000002, 6.0, 6.9000000000000004, 5.5999999999999996, 7.7000000000000002, 6.2999999999999998, 6.7000000000000002, 7.2000000000000002, 6.2000000000000002, 6.0999999999999996, 6.4000000000000004, 7.2000000000000002, 7.4000000000000004, 7.9000000000000004, 6.4000000000000004, 6.2999999999999998, 6.0999999999999996, 7.7000000000000002, 6.2999999999999998, 6.4000000000000004, 6.0, 6.9000000000000004, 6.7000000000000002, 6.9000000000000004, 5.7999999999999998, 6.7999999999999998, 6.7000000000000002, 6.7000000000000002, 6.2999999999999998, 6.5, 6.2000000000000002, 5.9000000000000004], 'Petal.Width': [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.29999999999999999, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.10000000000000001, 0.20000000000000001, 0.40000000000000002, 0.40000000000000002, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.20000000000000001, 0.40000000000000002, 0.20000000000000001, 0.5, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.29999999999999999, 0.29999999999999999, 0.20000000000000001, 0.59999999999999998, 0.40000000000000002, 0.29999999999999999, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 1.3999999999999999, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6000000000000001, 1.0, 1.3, 1.3999999999999999, 1.0, 1.5, 1.0, 1.3999999999999999, 1.3, 1.3999999999999999, 1.5, 1.0, 1.5, 1.1000000000000001, 1.8, 1.3, 1.5, 1.2, 1.3, 1.3999999999999999, 1.3999999999999999, 1.7, 1.5, 1.0, 1.1000000000000001, 1.0, 1.2, 1.6000000000000001, 1.5, 1.6000000000000001, 1.5, 1.3, 1.3, 1.3, 1.2, 1.3999999999999999, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1000000000000001, 1.3, 2.5, 1.8999999999999999, 2.1000000000000001, 1.8, 2.2000000000000002, 2.1000000000000001, 1.7, 1.8, 1.8, 2.5, 2.0, 1.8999999999999999, 2.1000000000000001, 2.0, 2.3999999999999999, 2.2999999999999998, 1.8, 2.2000000000000002, 2.2999999999999998, 1.5, 2.2999999999999998, 2.0, 2.0, 1.8, 2.1000000000000001, 1.8, 1.8, 1.8, 2.1000000000000001, 1.6000000000000001, 1.8999999999999999, 2.0, 2.2000000000000002, 1.5, 1.3999999999999999, 2.2999999999999998, 2.3999999999999999, 1.8, 1.8, 2.1000000000000001, 2.3999999999999999, 2.2999999999999998, 1.8999999999999999, 2.2999999999999998, 2.5, 2.2999999999999998, 1.8999999999999999, 2.0, 2.2999999999999998, 1.8], 'Petal.Length': [1.3999999999999999, 1.3999999999999999, 1.3, 1.5, 1.3999999999999999, 1.7, 1.3999999999999999, 1.5, 1.3999999999999999, 1.5, 1.5, 1.6000000000000001, 1.3999999999999999, 1.1000000000000001, 1.2, 1.5, 1.3, 1.3999999999999999, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.8999999999999999, 1.6000000000000001, 1.6000000000000001, 1.5, 1.3999999999999999, 1.6000000000000001, 1.6000000000000001, 1.5, 1.5, 1.3999999999999999, 1.5, 1.2, 1.3, 1.3999999999999999, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6000000000000001, 1.8999999999999999, 1.3999999999999999, 1.6000000000000001, 1.3999999999999999, 1.5, 1.3999999999999999, 4.7000000000000002, 4.5, 4.9000000000000004, 4.0, 4.5999999999999996, 4.5, 4.7000000000000002, 3.2999999999999998, 4.5999999999999996, 3.8999999999999999, 3.5, 4.2000000000000002, 4.0, 4.7000000000000002, 3.6000000000000001, 4.4000000000000004, 4.5, 4.0999999999999996, 4.5, 3.8999999999999999, 4.7999999999999998, 4.0, 4.9000000000000004, 4.7000000000000002, 4.2999999999999998, 4.4000000000000004, 4.7999999999999998, 5.0, 4.5, 3.5, 3.7999999999999998, 3.7000000000000002, 3.8999999999999999, 5.0999999999999996, 4.5, 4.5, 4.7000000000000002, 4.4000000000000004, 4.0999999999999996, 4.0, 4.4000000000000004, 4.5999999999999996, 4.0, 3.2999999999999998, 4.2000000000000002, 4.2000000000000002, 4.2000000000000002, 4.2999999999999998, 3.0, 4.0999999999999996, 6.0, 5.0999999999999996, 5.9000000000000004, 5.5999999999999996, 5.7999999999999998, 6.5999999999999996, 4.5, 6.2999999999999998, 5.7999999999999998, 6.0999999999999996, 5.0999999999999996, 5.2999999999999998, 5.5, 5.0, 5.0999999999999996, 5.2999999999999998, 5.5, 6.7000000000000002, 6.9000000000000004, 5.0, 5.7000000000000002, 4.9000000000000004, 6.7000000000000002, 4.9000000000000004, 5.7000000000000002, 6.0, 4.7999999999999998, 4.9000000000000004, 5.5999999999999996, 5.7999999999999998, 6.0999999999999996, 6.4000000000000004, 5.5999999999999996, 5.0999999999999996, 5.5999999999999996, 6.0999999999999996, 5.5999999999999996, 5.5, 4.7999999999999998, 5.4000000000000004, 5.5999999999999996, 5.0999999999999996, 5.0999999999999996, 5.9000000000000004, 5.7000000000000002, 5.2000000000000002, 5.0, 5.2000000000000002, 5.4000000000000004, 5.0999999999999996], 'Species': ['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica'], 'Sepal.Width': [3.5, 3.0, 3.2000000000000002, 3.1000000000000001, 3.6000000000000001, 3.8999999999999999, 3.3999999999999999, 3.3999999999999999, 2.8999999999999999, 3.1000000000000001, 3.7000000000000002, 3.3999999999999999, 3.0, 3.0, 4.0, 4.4000000000000004, 3.8999999999999999, 3.5, 3.7999999999999998, 3.7999999999999998, 3.3999999999999999, 3.7000000000000002, 3.6000000000000001, 3.2999999999999998, 3.3999999999999999, 3.0, 3.3999999999999999, 3.5, 3.3999999999999999, 3.2000000000000002, 3.1000000000000001, 3.3999999999999999, 4.0999999999999996, 4.2000000000000002, 3.1000000000000001, 3.2000000000000002, 3.5, 3.6000000000000001, 3.0, 3.3999999999999999, 3.5, 2.2999999999999998, 3.2000000000000002, 3.5, 3.7999999999999998, 3.0, 3.7999999999999998, 3.2000000000000002, 3.7000000000000002, 3.2999999999999998, 3.2000000000000002, 3.2000000000000002, 3.1000000000000001, 2.2999999999999998, 2.7999999999999998, 2.7999999999999998, 3.2999999999999998, 2.3999999999999999, 2.8999999999999999, 2.7000000000000002, 2.0, 3.0, 2.2000000000000002, 2.8999999999999999, 2.8999999999999999, 3.1000000000000001, 3.0, 2.7000000000000002, 2.2000000000000002, 2.5, 3.2000000000000002, 2.7999999999999998, 2.5, 2.7999999999999998, 2.8999999999999999, 3.0, 2.7999999999999998, 3.0, 2.8999999999999999, 2.6000000000000001, 2.3999999999999999, 2.3999999999999999, 2.7000000000000002, 2.7000000000000002, 3.0, 3.3999999999999999, 3.1000000000000001, 2.2999999999999998, 3.0, 2.5, 2.6000000000000001, 3.0, 2.6000000000000001, 2.2999999999999998, 2.7000000000000002, 3.0, 2.8999999999999999, 2.8999999999999999, 2.5, 2.7999999999999998, 3.2999999999999998, 2.7000000000000002, 3.0, 2.8999999999999999, 3.0, 3.0, 2.5, 2.8999999999999999, 2.5, 3.6000000000000001, 3.2000000000000002, 2.7000000000000002, 3.0, 2.5, 2.7999999999999998, 3.2000000000000002, 3.0, 3.7999999999999998, 2.6000000000000001, 2.2000000000000002, 3.2000000000000002, 2.7999999999999998, 2.7999999999999998, 2.7000000000000002, 3.2999999999999998, 3.2000000000000002, 2.7999999999999998, 3.0, 2.7999999999999998, 3.0, 2.7999999999999998, 3.7999999999999998, 2.7999999999999998, 2.7999999999999998, 2.6000000000000001, 3.0, 3.3999999999999999, 3.1000000000000001, 3.0, 3.1000000000000001, 3.1000000000000001, 3.1000000000000001, 2.7000000000000002, 3.2000000000000002, 3.2999999999999998, 3.0, 2.5, 3.0, 3.3999999999999999, 3.0], 'Unnamed: 0': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]} df = pd.DataFrame(raw_data, columns = ['Unnamed: 0', 'Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width', 'Species']) If you want to check the results... 1. Enter the code produced from the cell above in this cell raw_data = { 'Petal.Width' : [ 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.29999999999999999 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.40000000000000002 , 0.29999999999999999 , 0.29999999999999999 , 0.29999999999999999 , 0.20000000000000001 , 0.40000000000000002 , 0.20000000000000001 , 0.5 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.29999999999999999 , 0.29999999999999999 , 0.20000000000000001 , 0.59999999999999998 , 0.40000000000000002 , 0.29999999999999999 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 1.3999999999999999 , 1.5 , 1.5 , 1.3 , 1.5 , 1.3 , 1.6000000000000001 , 1.0 , 1.3 , 1.3999999999999999 , 1.0 , 1.5 , 1.0 , 1.3999999999999999 , 1.3 , 1.3999999999999999 , 1.5 , 1.0 , 1.5 , 1.1000000000000001 , 1.8 , 1.3 , 1.5 , 1.2 , 1.3 , 1.3999999999999999 , 1.3999999999999999 , 1.7 , 1.5 , 1.0 , 1.1000000000000001 , 1.0 , 1.2 , 1.6000000000000001 , 1.5 , 1.6000000000000001 , 1.5 , 1.3 , 1.3 , 1.3 , 1.2 , 1.3999999999999999 , 1.2 , 1.0 , 1.3 , 1.2 , 1.3 , 1.3 , 1.1000000000000001 , 1.3 , 2.5 , 1.8999999999999999 , 2.1000000000000001 , 1.8 , 2.2000000000000002 , 2.1000000000000001 , 1.7 , 1.8 , 1.8 , 2.5 , 2.0 , 1.8999999999999999 , 2.1000000000000001 , 2.0 , 2.3999999999999999 , 2.2999999999999998 , 1.8 , 2.2000000000000002 , 2.2999999999999998 , 1.5 , 2.2999999999999998 , 2.0 , 2.0 , 1.8 , 2.1000000000000001 , 1.8 , 1.8 , 1.8 , 2.1000000000000001 , 1.6000000000000001 , 1.8999999999999999 , 2.0 , 2.2000000000000002 , 1.5 , 1.3999999999999999 , 2.2999999999999998 , 2.3999999999999999 , 1.8 , 1.8 , 2.1000000000000001 , 2.3999999999999999 , 2.2999999999999998 , 1.8999999999999999 , 2.2999999999999998 , 2.5 , 2.2999999999999998 , 1.8999999999999999 , 2.0 , 2.2999999999999998 , 1.8 ], 'Sepal.Width' : [ 3.5 , 3.0 , 3.2000000000000002 , 3.1000000000000001 , 3.6000000000000001 , 3.8999999999999999 , 3.3999999999999999 , 3.3999999999999999 , 2.8999999999999999 , 3.1000000000000001 , 3.7000000000000002 , 3.3999999999999999 , 3.0 , 3.0 , 4.0 , 4.4000000000000004 , 3.8999999999999999 , 3.5 , 3.7999999999999998 , 3.7999999999999998 , 3.3999999999999999 , 3.7000000000000002 , 3.6000000000000001 , 3.2999999999999998 , 3.3999999999999999 , 3.0 , 3.3999999999999999 , 3.5 , 3.3999999999999999 , 3.2000000000000002 , 3.1000000000000001 , 3.3999999999999999 , 4.0999999999999996 , 4.2000000000000002 , 3.1000000000000001 , 3.2000000000000002 , 3.5 , 3.6000000000000001 , 3.0 , 3.3999999999999999 , 3.5 , 2.2999999999999998 , 3.2000000000000002 , 3.5 , 3.7999999999999998 , 3.0 , 3.7999999999999998 , 3.2000000000000002 , 3.7000000000000002 , 3.2999999999999998 , 3.2000000000000002 , 3.2000000000000002 , 3.1000000000000001 , 2.2999999999999998 , 2.7999999999999998 , 2.7999999999999998 , 3.2999999999999998 , 2.3999999999999999 , 2.8999999999999999 , 2.7000000000000002 , 2.0 , 3.0 , 2.2000000000000002 , 2.8999999999999999 , 2.8999999999999999 , 3.1000000000000001 , 3.0 , 2.7000000000000002 , 2.2000000000000002 , 2.5 , 3.2000000000000002 , 2.7999999999999998 , 2.5 , 2.7999999999999998 , 2.8999999999999999 , 3.0 , 2.7999999999999998 , 3.0 , 2.8999999999999999 , 2.6000000000000001 , 2.3999999999999999 , 2.3999999999999999 , 2.7000000000000002 , 2.7000000000000002 , 3.0 , 3.3999999999999999 , 3.1000000000000001 , 2.2999999999999998 , 3.0 , 2.5 , 2.6000000000000001 , 3.0 , 2.6000000000000001 , 2.2999999999999998 , 2.7000000000000002 , 3.0 , 2.8999999999999999 , 2.8999999999999999 , 2.5 , 2.7999999999999998 , 3.2999999999999998 , 2.7000000000000002 , 3.0 , 2.8999999999999999 , 3.0 , 3.0 , 2.5 , 2.8999999999999999 , 2.5 , 3.6000000000000001 , 3.2000000000000002 , 2.7000000000000002 , 3.0 , 2.5 , 2.7999999999999998 , 3.2000000000000002 , 3.0 , 3.7999999999999998 , 2.6000000000000001 , 2.2000000000000002 , 3.2000000000000002 , 2.7999999999999998 , 2.7999999999999998 , 2.7000000000000002 , 3.2999999999999998 , 3.2000000000000002 , 2.7999999999999998 , 3.0 , 2.7999999999999998 , 3.0 , 2.7999999999999998 , 3.7999999999999998 , 2.7999999999999998 , 2.7999999999999998 , 2.6000000000000001 , 3.0 , 3.3999999999999999 , 3.1000000000000001 , 3.0 , 3.1000000000000001 , 3.1000000000000001 , 3.1000000000000001 , 2.7000000000000002 , 3.2000000000000002 , 3.2999999999999998 , 3.0 , 2.5 , 3.0 , 3.3999999999999999 , 3.0 ], 'Species' : [ 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' ], 'Unnamed: 0' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 ], 'Sepal.Length' : [ 5.0999999999999996 , 4.9000000000000004 , 4.7000000000000002 , 4.5999999999999996 , 5.0 , 5.4000000000000004 , 4.5999999999999996 , 5.0 , 4.4000000000000004 , 4.9000000000000004 , 5.4000000000000004 , 4.7999999999999998 , 4.7999999999999998 , 4.2999999999999998 , 5.7999999999999998 , 5.7000000000000002 , 5.4000000000000004 , 5.0999999999999996 , 5.7000000000000002 , 5.0999999999999996 , 5.4000000000000004 , 5.0999999999999996 , 4.5999999999999996 , 5.0999999999999996 , 4.7999999999999998 , 5.0 , 5.0 , 5.2000000000000002 , 5.2000000000000002 , 4.7000000000000002 , 4.7999999999999998 , 5.4000000000000004 , 5.2000000000000002 , 5.5 , 4.9000000000000004 , 5.0 , 5.5 , 4.9000000000000004 , 4.4000000000000004 , 5.0999999999999996 , 5.0 , 4.5 , 4.4000000000000004 , 5.0 , 5.0999999999999996 , 4.7999999999999998 , 5.0999999999999996 , 4.5999999999999996 , 5.2999999999999998 , 5.0 , 7.0 , 6.4000000000000004 , 6.9000000000000004 , 5.5 , 6.5 , 5.7000000000000002 , 6.2999999999999998 , 4.9000000000000004 , 6.5999999999999996 , 5.2000000000000002 , 5.0 , 5.9000000000000004 , 6.0 , 6.0999999999999996 , 5.5999999999999996 , 6.7000000000000002 , 5.5999999999999996 , 5.7999999999999998 , 6.2000000000000002 , 5.5999999999999996 , 5.9000000000000004 , 6.0999999999999996 , 6.2999999999999998 , 6.0999999999999996 , 6.4000000000000004 , 6.5999999999999996 , 6.7999999999999998 , 6.7000000000000002 , 6.0 , 5.7000000000000002 , 5.5 , 5.5 , 5.7999999999999998 , 6.0 , 5.4000000000000004 , 6.0 , 6.7000000000000002 , 6.2999999999999998 , 5.5999999999999996 , 5.5 , 5.5 , 6.0999999999999996 , 5.7999999999999998 , 5.0 , 5.5999999999999996 , 5.7000000000000002 , 5.7000000000000002 , 6.2000000000000002 , 5.0999999999999996 , 5.7000000000000002 , 6.2999999999999998 , 5.7999999999999998 , 7.0999999999999996 , 6.2999999999999998 , 6.5 , 7.5999999999999996 , 4.9000000000000004 , 7.2999999999999998 , 6.7000000000000002 , 7.2000000000000002 , 6.5 , 6.4000000000000004 , 6.7999999999999998 , 5.7000000000000002 , 5.7999999999999998 , 6.4000000000000004 , 6.5 , 7.7000000000000002 , 7.7000000000000002 , 6.0 , 6.9000000000000004 , 5.5999999999999996 , 7.7000000000000002 , 6.2999999999999998 , 6.7000000000000002 , 7.2000000000000002 , 6.2000000000000002 , 6.0999999999999996 , 6.4000000000000004 , 7.2000000000000002 , 7.4000000000000004 , 7.9000000000000004 , 6.4000000000000004 , 6.2999999999999998 , 6.0999999999999996 , 7.7000000000000002 , 6.2999999999999998 , 6.4000000000000004 , 6.0 , 6.9000000000000004 , 6.7000000000000002 , 6.9000000000000004 , 5.7999999999999998 , 6.7999999999999998 , 6.7000000000000002 , 6.7000000000000002 , 6.2999999999999998 , 6.5 , 6.2000000000000002 , 5.9000000000000004 ], 'Petal.Length' : [ 1.3999999999999999 , 1.3999999999999999 , 1.3 , 1.5 , 1.3999999999999999 , 1.7 , 1.3999999999999999 , 1.5 , 1.3999999999999999 , 1.5 , 1.5 , 1.6000000000000001 , 1.3999999999999999 , 1.1000000000000001 , 1.2 , 1.5 , 1.3 , 1.3999999999999999 , 1.7 , 1.5 , 1.7 , 1.5 , 1.0 , 1.7 , 1.8999999999999999 , 1.6000000000000001 , 1.6000000000000001 , 1.5 , 1.3999999999999999 , 1.6000000000000001 , 1.6000000000000001 , 1.5 , 1.5 , 1.3999999999999999 , 1.5 , 1.2 , 1.3 , 1.3999999999999999 , 1.3 , 1.5 , 1.3 , 1.3 , 1.3 , 1.6000000000000001 , 1.8999999999999999 , 1.3999999999999999 , 1.6000000000000001 , 1.3999999999999999 , 1.5 , 1.3999999999999999 , 4.7000000000000002 , 4.5 , 4.9000000000000004 , 4.0 , 4.5999999999999996 , 4.5 , 4.7000000000000002 , 3.2999999999999998 , 4.5999999999999996 , 3.8999999999999999 , 3.5 , 4.2000000000000002 , 4.0 , 4.7000000000000002 , 3.6000000000000001 , 4.4000000000000004 , 4.5 , 4.0999999999999996 , 4.5 , 3.8999999999999999 , 4.7999999999999998 , 4.0 , 4.9000000000000004 , 4.7000000000000002 , 4.2999999999999998 , 4.4000000000000004 , 4.7999999999999998 , 5.0 , 4.5 , 3.5 , 3.7999999999999998 , 3.7000000000000002 , 3.8999999999999999 , 5.0999999999999996 , 4.5 , 4.5 , 4.7000000000000002 , 4.4000000000000004 , 4.0999999999999996 , 4.0 , 4.4000000000000004 , 4.5999999999999996 , 4.0 , 3.2999999999999998 , 4.2000000000000002 , 4.2000000000000002 , 4.2000000000000002 , 4.2999999999999998 , 3.0 , 4.0999999999999996 , 6.0 , 5.0999999999999996 , 5.9000000000000004 , 5.5999999999999996 , 5.7999999999999998 , 6.5999999999999996 , 4.5 , 6.2999999999999998 , 5.7999999999999998 , 6.0999999999999996 , 5.0999999999999996 , 5.2999999999999998 , 5.5 , 5.0 , 5.0999999999999996 , 5.2999999999999998 , 5.5 , 6.7000000000000002 , 6.9000000000000004 , 5.0 , 5.7000000000000002 , 4.9000000000000004 , 6.7000000000000002 , 4.9000000000000004 , 5.7000000000000002 , 6.0 , 4.7999999999999998 , 4.9000000000000004 , 5.5999999999999996 , 5.7999999999999998 , 6.0999999999999996 , 6.4000000000000004 , 5.5999999999999996 , 5.0999999999999996 , 5.5999999999999996 , 6.0999999999999996 , 5.5999999999999996 , 5.5 , 4.7999999999999998 , 5.4000000000000004 , 5.5999999999999996 , 5.0999999999999996 , 5.0999999999999996 , 5.9000000000000004 , 5.7000000000000002 , 5.2000000000000002 , 5.0 , 5.2000000000000002 , 5.4000000000000004 , 5.0999999999999996 ]} df = pd . DataFrame ( raw_data , columns = [ 'Unnamed: 0' , 'Sepal.Length' , 'Sepal.Width' , 'Petal.Length' , 'Petal.Width' , 'Species' ]) 2. Compare the original and recreated dataframes # Look at the top few rows of the original dataframe df . head () Unnamed: 0 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 1 5.1 3.5 1.4 0.2 setosa 1 2 4.9 3.0 1.4 0.2 setosa 2 3 4.7 3.2 1.3 0.2 setosa 3 4 4.6 3.1 1.5 0.2 setosa 4 5 5.0 3.6 1.4 0.2 setosa # Look at the top few rows of the dataframe created with our code df_original . head () Unnamed: 0 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 1 5.1 3.5 1.4 0.2 setosa 1 2 4.9 3.0 1.4 0.2 setosa 2 3 4.7 3.2 1.3 0.2 setosa 3 4 4.6 3.1 1.5 0.2 setosa 4 5 5.0 3.6 1.4 0.2 setosa","tags":"Python","url":"http://chrisalbon.com/python/csv_to_python_code.html","loc":"http://chrisalbon.com/python/csv_to_python_code.html"},{"title":"My Data Science Reading List","text":"Everyone has hobbies. My hobby is in a big red bookcase. Currently my big red bookcase contains around ~200 dead-tree books on statistics, data science, research, and mathematics. I rarely travel anywhere without at least one book from this bookcase and on a lazy Sunday afternoon you will likely see to working my way through one of it's members. Mathematics Basics Basic Math for Social Scientists: Concepts (Quantitative Applications in the Social Sciences) Mathematics for the Nonmathematician (Dover Books on Mathematics) A Mathematics Course for Political and Social Research Introduction to Mathematical Thinking Basic Math for Social Scientists: Problems and Solutions (Quantitative Applications in the Social Sciences) The Language of Mathematics: Making the Invisible Visible The Joy of x: A Guided Tour of Math, from One to Infinity Here's Looking at Euclid: A Surprising Excursion Through the Astonishing World of Math Good Math: A Geek's Guide to the Beauty of Numbers, Logic, and Computation (Pragmatic Programmers) Mathematics 1001: Absolutely Everything That Matters in Mathematics. Essential Mathematics for Political and Social Research (Analytical Methods for Social Research) Doing Math with Python: Use Programming to Explore Algebra, Statistics, Calculus, and More! Mathematics and Python Programming Algebra Linear Algebra (Dover Books on Mathematics) Basic Algebra I: Second Edition (Dover Books on Mathematics) Basic Algebra II: Second Edition (Dover Books on Mathematics) Introduction to Linear Algebra and Differential Equations (Dover Books on Mathematics) Coding the Matrix: Linear Algebra through Applications to Computer Science Calculus Quick Calculus: A Self-Teaching Guide, 2nd Edition Calculus: An Intuitive and Physical Approach (Second Edition) (Dover Books on Mathematics) Essential Calculus with Applications (Dover Books on Mathematics) Introduction to Partial Differential Equations with Applications (Dover Books on Mathematics) Partial Differential Equations for Scientists and Engineers (Dover Books on Mathematics) Ordinary Differential Equations (Dover Books on Mathematics) Probability Introduction to Probability (Chapman & Hall/CRC Texts in Statistical Science) Probability Theory: A Concise Course (Dover Books on Mathematics) Other Geometry: A Comprehensive Course (Dover Books on Mathematics) Introduction to Stochastic Processes (Dover Books on Mathematics) Traditional Statistics Basic Statistics Statistics Discovering Statistics Using R Statistics: The Art and Science of Learning from Data (3rd Edition) Principles of Statistics (Dover Books on Mathematics) All of Statistics: A Concise Course in Statistical Inference (Springer Texts in Statistics) Mathematical Methods in Statistics a Workbook Fundamental Statistics for the Behavioral Sciences Statistics Made Simple Using Basic Statistics in the Behavioral Sciences Statistical Rules of Thumb Analysis of Longitudinal Data (Oxford Statistical Science) Practical Longitudinal Data Analysis (Chapman & Hall/CRC Texts in Statistical Science) The SAGE Handbook of Regression Analysis and Causal Inference Statistical Modeling and Inference for Social Science (Analytical Methods for Social Research) Statistical Models and Causal Inference: A Dialogue with the Social Sciences Statistics Done Wrong: The Woefully Complete Guide Even You Can Learn Statistics: A Guide for Everyone Who Has Ever Been Afraid of Statistics Practical Statistics Simply Explained (Dover Books on Mathematics) Exploratory Data Analysis Exploratory Data Analysis (Quantitative Applications in the Social Sciences) Fundamentals of Exploratory Analysis of Variance Practical Longitudinal Data Analysis (Chapman & Hall/CRC Texts in Statistical Science) Exploratory Data Analysis Fundamentals of Exploratory Analysis of Variance Understanding Robust and Exploratory Data Analysis Regression Multiple Regression in Practice (Quantitative Applications in the Social Sciences) Understanding Regression Assumptions (Quantitative Applications in the Social Sciences) Understanding Regression Analysis: An Introductory Guide (Quantitative Applications in the Social Sciences) Applied Regression: An Introduction (Quantitative Applications in the Social Sciences) Multiple Regression in Practice (Quantitative Applications in the Social Sciences) Tests of Significance (Quantitative Applications in the Social Sciences) Introduction to Multivariate Analysis (Chapman & Hall/CRC Texts in Statistical Science) Multivariate Statistical Analysis: A Conceptual Introduction, 2nd Edition What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics Understanding Regression Assumptions (Quantitative Applications in the Social Sciences) Regression Analysis by Example Applied Logistic Regression (Wiley Series in Probability and Statistics) Statistics for the Social Sciences Introduction to Linear Regression Analysis Basic statistics: Tales of distributions Analysis of Covariance (Quantitative Applications in the Social Sciences) Interpreting and Using Regression (Quantitative Applications in the Social Sciences, No. 29) Data Analysis and Regression: A Second Course in Statistics Understanding Significance Testing (Quantitative Applications in the Social Sciences) Loglinear Models with Latent Variables (Quantitative Applications in the Social Sciences) Statistical Models: Theory and Practice Permutation, Parametric, and Bootstrap Tests of Hypotheses (Springer Series in Statistics) Advanced Regression Interpreting Probability Models: Logit, Probit, and Other Generalized Linear Models (Quantitative Applications in the Social Sciences) Regression Models for Categorical and Limited Dependent Variables (Advanced Quantitative Techniques in the Social Sciences) Analysis of Ordinal Data (Quantitative Applications in the Social Sciences) Analysis of Nominal Data (Quantitative Applications in the Social Sciences) Ordinal Log-Linear Models (Quantitative Applications in the Social Sciences) Analysis of Ordinal Categorical Data (Wiley Series in Probability and Statistics) An Introduction to Categorical Data Analysis (Wiley Series in Probability and Statistics) Regression with Dummy Variables (Quantitative Applications in the Social Sciences) An Introduction to Generalized Linear Models, Third Edition (Chapman & Hall/CRC Texts in Statistical Science) Modeling Count Data A Primer on Linear Models (Chapman & Hall/CRC Texts in Statistical Science) Data Analysis Using Regression and Multilevel/Hierarchical Models Statistical Methods for Categorical Data Analysis, 2nd Edition Data Analysis Using Regression and Multilevel/Hierarchical Models Non-Parametric Nonparametric Statistics: An Introduction (Quantitative Applications in the Social Sciences) Time Series The Analysis of Time Series: An Introduction, Sixth Edition (Chapman & Hall/CRC Texts in Statistical Science) Time Series Analysis Interrupted Time Series Analysis (Quantitative Applications in the Social Sciences) Time Series Analysis for the Social Sciences (Analytical Methods for Social Research) Event History Event History Modeling: A Guide for Social Scientists (Analytical Methods for Social Research) Survival Analysis Modelling Survival Data in Medical Research, Second Edition Other The Life and Times of the Central Limit Theorem (History of Mathematics) Introduction to Factor Analysis: What It Is and How To Do It (Quantitative Applications in the Social Sciences) Statistics on the Table: The History of Statistical Concepts and Methods Understanding The New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis (Multivariate Applications Series) Central Tendency and Variability (Quantitative Applications in the Social Sciences) Probability, Statistics and Truth (Dover Books on Mathematics) Applied Missing Data Analysis (Methodology in the Social Sciences) Thinking Statistically Heteroskedasticity in Regression: Detection and Correction (Quantitative Applications in the Social Sciences) Statistical Analysis with Missing Data The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives (Economics, Cognition, and Society) Statistics As Principled Argument The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics) Probability, Statistics and Truth (Dover Books on Mathematics) Statistics, Data Mining, and Machine Learning in Astronomy: A Practical Python Guide for the Analysis of Survey Data Statistical Distributions Mathematical Methods of Statistics Bayesian Statistics Bayes' Rule: A Tutorial Introduction to Bayesian Analysis Data Analysis: A Bayesian Tutorial Introduction to Bayesian Statistics, 2nd Edition Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) Bayesian Statistics for the Social Sciences (Methodology in the Social Sciences) Bayesian Programming (Chapman & Hall/CRC Machine Learning & Pattern Recognition) Bayesian Reasoning and Machine Learning Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Methods (Addison-Wesley Data & Analytics Series) Bayesian Methods in Health Economics (Chapman & Hall/CRC Biostatistics Series) Bayesian and Frequentist Regression Methods (Springer Series in Statistics) Machine Learning Machine Learning in Action Learning From Data Machine Learning: An Algorithmic Perspective, Second Edition (Chapman & Hall/Crc Machine Learning & Pattern Recognition) Pattern Recognition and Machine Learning 1st Edition Python Machine Learning (Raschka) Python Machine Learning (Bowles) An Introduction to Machine Learning Boosting: Foundations and Algorithms (Adaptive Computation and Machine Learning series) Machine Learning in Python: Essential Techniques for Predictive Analysis Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series) Introduction to Machine Learning (Adaptive Computation and Machine Learning series) Algorithims Introduction to Algorithms, 3rd Edition Data Structures and Algorithms with Python (Undergraduate Topics in Computer Science) Python Algorithms: Mastering Basic Algorithms in the Python Language Annotated Algorithms in Python: with Applications in Physics, Biology, and Finance Data Structure and Algorithmic Thinking with Python: Data Structure and Algorithmic Puzzles Other Data Analysis for Politics and Policy Missing Data (Quantitative Applications in the Social Sciences) A Handbook of Small Data Sets (Chapman & Hall Statistics Texts) The Statistical Analysis of Experimental Data (Dover Books on Mathematics) The Chicago Guide to Writing about Multivariate Analysis (Chicago Guides to Writing, Editing, and Publishing) Visual Explanations: Images and Quantities, Evidence and Narrative Visual Storytelling with D3: An Introduction to Data Visualization in JavaScript (Addison-Wesley Data & Analytics Series) Cluster Analysis (Quantitative Applications in the Social Sciences) Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data The Visual Display of Quantitative Information Matched Sampling for Causal Effects Beautiful Data: The Stories Behind Elegant Data Solutions Sampling Techniques, 3rd Edition Counterfactuals and Causal Inference: Methods and Principles for Social Research (Analytical Methods for Social Research) Introduction to Analysis (Dover Books on Mathematics) An Introduction to Information Theory: Symbols, Signals and Noise (Dover Books on Mathematics) Monte Carlo Simulation and Resampling Methods for Social Science Neural Networks for Pattern Recognition (Advanced Texts in Econometrics) Causality: Models, Reasoning and Inference Natural Experiments in the Social Sciences: A Design-Based Approach (Strategies for Social Inquiry) Game Theory: Concepts and Applications (Quantitative Applications in the Social Sciences) Web Scraping with Python: A Comprehensive Guide to Data Collection Solutions Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications Running Randomized Evaluations: A Practical Guide Python for Data Science For Dummies Counterfactuals and Causal Inference: Methods and Principles for Social Research (Analytical Methods for Social Research) A Practical Introduction to Index Numbers Sampling Data Mining for the Social Sciences: An Introduction Beautiful Data: A History of Vision and Reason since 1945 (Experimental Futures) Designing Social Inquiry: Scientific Inference in Qualitative Research Using Propensity Scores in Quasi-Experimental Designs Propensity Score Analysis: Statistical Methods and Applications (Advanced Quantitative Techniques in the Social Sciences)","tags":"Blog","url":"http://chrisalbon.com/blog/data_science_reading_list.html","loc":"http://chrisalbon.com/blog/data_science_reading_list.html"},{"title":"Data Structure Basics","text":"Lists \"A list is a data structure that holds an ordered collection of items i.e. you can store a sequence of items in a list.\" - A Byte Of Python Lists are mutable. # Create a list of countries, then print the results allies = [ 'USA' , 'UK' , 'France' , 'New Zealand' , 'Australia' , 'Canada' , 'Poland' ]; allies ['USA', 'UK', 'France', 'New Zealand', 'Australia', 'Canada', 'Poland'] # Print the length of the list len ( allies ) 7 # Add an item to the list, then print the results allies . append ( 'China' ); allies ['USA', 'UK', 'France', 'New Zealand', 'Australia', 'Canada', 'Poland', 'China'] # Sort list, then print the results allies . sort (); allies ['Australia', 'Canada', 'China', 'France', 'New Zealand', 'Poland', 'UK', 'USA'] # Reverse sort list, then print the results allies . reverse (); allies ['USA', 'UK', 'Poland', 'New Zealand', 'France', 'China', 'Canada', 'Australia'] # View the first item of the list allies [ 0 ] 'USA' # View the last item of the list allies [ - 1 ] 'Australia' # Delete the item in the list del allies [ 0 ]; allies ['UK', 'Poland', 'New Zealand', 'France', 'China', 'Canada', 'Australia'] # Add a numeric value to a list of strings allies . append ( 3442 ); allies ['UK', 'Poland', 'New Zealand', 'France', 'China', 'Canada', 'Australia', 3442] Tuples \"Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain an heterogeneous sequence of elements that are accessed via unpacking (or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.\" - Python Documentation \"Tuples are heterogeneous data structures (i.e., their entries have different meanings), while lists are homogeneous sequences.\" - StackOverflow Parentheses are optional, but useful. # Create a tuple of state names usa = ( 'Texas' , 'California' , 'Maryland' ); usa ('Texas', 'California', 'Maryland') # Create a tuple of countries # (notice the USA has a state names in the nested tuple) countries = ( 'canada' , 'mexico' , usa ); countries ('canada', 'mexico', ('Texas', 'California', 'Maryland')) # View the third item of the top tuple countries [ 2 ] ('Texas', 'California', 'Maryland') # View the third item of the third tuple countries [ 2 ][ 2 ] 'Maryland' Dictionaries \"A dictionary is like an address-book where you can find the address or contact details of a person by knowing only his/her name i.e. we associate keys (name) with values (details). Note that the key must be unique just like you cannot find out the correct information if you have two persons with the exact same name.\" - A Byte Of Python # Create a dictionary with key:value combos staff = { 'Chris' : 'chris@stater.org' , 'Jake' : 'jake@stater.org' , 'Ashley' : 'ashley@stater.org' , 'Shelly' : 'shelly@stater.org' } # Print the value using the key staff [ 'Chris' ] 'chris@stater.org' # Delete a dictionary entry based on the key del staff [ 'Chris' ]; staff {'Ashley': 'ashley@stater.org', 'Jake': 'jake@stater.org', 'Shelly': 'shelly@stater.org'} # Add an item to the dictionary staff [ 'Guido' ] = 'guido@python.org' ; staff {'Ashley': 'ashley@stater.org', 'Guido': 'guido@python.org', 'Jake': 'jake@stater.org', 'Shelly': 'shelly@stater.org'} Sets Sets are unordered collections of simple objects. # Create a set of BRI countries BRI = set ([ 'brazil' , 'russia' , 'india' ]) # Is India in the set BRI? 'india' in BRI True # Is the US in the set BRI? 'usa' in BRI False # Create a copy of BRI called BRIC BRIC = BRI . copy () # Add China to BRIC BRIC . add ( 'china' ) # Is BRIC a super-set of BRI? BRIC . issuperset ( BRI ) True # Remove Russia from BRI BRI . remove ( 'russia' ) # What items are the union of BRI and BRIC? BRI & BRIC {'brazil', 'india'}","tags":"Python","url":"http://chrisalbon.com/python/data_structure_basics.html","loc":"http://chrisalbon.com/python/data_structure_basics.html"},{"title":"Date And Time Basics","text":"# Import modules from datetime import datetime from datetime import timedelta # Create a variable with the current time now = datetime . now () now datetime.datetime(2014, 5, 11, 20, 5, 11, 688051) # The current year now . year 2014 # The current month now . month 5 # The current day now . day 11 # The current hour now . hour 20 # The current minute now . minute 5 # The difference between two dates delta = datetime ( 2011 , 1 , 7 ) - datetime ( 2011 , 1 , 6 ) delta datetime.timedelta(1) # The difference days delta . days 1 # The difference seconds delta . seconds 0 # Create a time start = datetime ( 2011 , 1 , 7 ) # Add twelve days to the time start + timedelta ( 12 ) datetime.datetime(2011, 1, 19, 0, 0)","tags":"Python","url":"http://chrisalbon.com/python/date_and_time_basics.html","loc":"http://chrisalbon.com/python/date_and_time_basics.html"},{"title":"Dates And Times","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Get Current Date %% sql -- Select the current date SELECT date ( 'now' ); date('now') 2017-01-19 Get Current Date And Time %% sql -- Select the unix time code '1200762133' SELECT datetime ( 'now' , 'unixepoch' ); datetime('now', 'unixepoch') 1970-01-29 10:42:53 Compute A UNIX timestamp into a date and time %% sql -- Select the unix time code '1169229733' SELECT datetime ( 1169229733 , 'unixepoch' ); datetime(1169229733, 'unixepoch') 2007-01-19 18:02:13 Compute A UNIX timestamp into a date and time and convert to the local timezone. %% sql -- Select the unix time code '1171904533' and convert to the machine 's local timezone SELECT datetime ( 1171904533 , 'unixepoch' , 'localtime' ); datetime(1171904533, 'unixepoch', 'localtime') 2007-02-19 10:02:13 Compute The Day Of The Week %% sql -- Select the the day of this week ( 0 = Sunday , 4 = Thursday ) SELECT strftime ( '%w' , 'now' ); strftime('%w','now') 4","tags":"SQL","url":"http://chrisalbon.com/sql/dates_and_times.html","loc":"http://chrisalbon.com/sql/dates_and_times.html"},{"title":"Delete A Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Delete A Table %% sql -- Delete the table called 'criminals' DROP TABLE criminals [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals (sqlite3.OperationalError) no such table: criminals [SQL: \"-- Select everything\\nSELECT *\\n\\n-- From the table 'criminals'\\nFROM criminals\"] Note: We get an error because the table doesn't exist anymore.","tags":"SQL","url":"http://chrisalbon.com/sql/delete_a_table.html","loc":"http://chrisalbon.com/sql/delete_a_table.html"},{"title":"Delete The Contents Of A Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View The Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Delete A Table %% sql -- Delete the contents of the table called 'criminals' DELETE FROM criminals [] View The Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals []","tags":"SQL","url":"http://chrisalbon.com/sql/delete_contents_of_table.html","loc":"http://chrisalbon.com/sql/delete_contents_of_table.html"},{"title":"Demonstrate The Central Limit Theorem","text":"Preliminaries # Import packages import pandas as pd import numpy as np # Set matplotlib as inline % matplotlib inline Create Population Data From Non-Normal Distribution # Create an empty dataframe population = pd . DataFrame () # Create an column that is 10000 random numbers drawn from a uniform distribution population [ 'numbers' ] = np . random . uniform ( 0 , 10000 , size = 10000 ) # Plot a histogram of the score data. # This confirms the data is not a normal distribution. population [ 'numbers' ] . hist ( bins = 100 ) <matplotlib.axes._subplots.AxesSubplot at 0x112c72710> View the True Mean Of Population # View the mean of the numbers population [ 'numbers' ] . mean () 4983.824612472138 Take A Sample Mean, Repeat 1000 Times # Create a list sampled_means = [] # For 1000 times, for i in range ( 0 , 1000 ): # Take a random sample of 100 rows from the population, take the mean of those rows, append to sampled_means sampled_means . append ( population . sample ( n = 100 ) . mean () . values [ 0 ]) Plot The Sample Means Of All 100 Samples # Plot a histogram of sampled_means. # It is clearly normally distributed and centered around 5000 pd . Series ( sampled_means ) . hist ( bins = 100 ) <matplotlib.axes._subplots.AxesSubplot at 0x11516e668> This is the critical chart, remember that the population distribution was uniform, however, this distribution is approaching normality. This is the key point to the central limit theory, and the reason we can assume sample means are not bias. View The Mean Sample Mean # View the mean of the sampled_means pd . Series ( sampled_means ) . mean () 4981.465310909289 Compare To True Mean # Subtract Mean Sample Mean From True Population Mean error = population [ 'numbers' ] . mean () - pd . Series ( sampled_means ) . mean () # Print print ( 'The Mean Sample Mean is only %f different the True Population mean!' % error ) The Mean Sample Mean is only 2.359302 different the True Population mean!","tags":"Statistics","url":"http://chrisalbon.com/statistics/demonstrate_the_central_limit_theorem.html","loc":"http://chrisalbon.com/statistics/demonstrate_the_central_limit_theorem.html"},{"title":"Dictionary Basics","text":"Basics Not sequences, but mappings. That is, stored by key, not relative position. Dictionaries are mutable. Build a dictionary via brackets unef_org = { 'name' : 'UNEF' , 'staff' : 32 , 'url' : 'http://unef.org' } View the variable unef_org {'name': 'UNEF', 'staff': 32, 'url': 'http://unef.org'} Build a dict via keys who_org = {} who_org [ 'name' ] = 'WHO' who_org [ 'staff' ] = '10' who_org [ 'url' ] = 'http://who.org' View the variable who_org {'name': 'WHO', 'staff': '10', 'url': 'http://who.org'} Nesting in dictionaries Build a dictionary via brackets unitas_org = { 'name' : 'UNITAS' , 'staff' : 32 , 'url' : [ 'http://unitas.org' , 'http://unitas.int' ]} View the variable unitas_org {'name': 'UNITAS', 'staff': 32, 'url': ['http://unitas.org', 'http://unitas.int']} Index the nested list Index the second item of the list nested in the url key. unitas_org [ 'url' ][ 1 ] 'http://unitas.int'","tags":"Python","url":"http://chrisalbon.com/python/dictionary_basics.html","loc":"http://chrisalbon.com/python/dictionary_basics.html"},{"title":"Drop A Column","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 Delete Column (Most Common) %%sql -- Alter the table called 'criminals' ALTER TABLE criminals -- From the table 'criminals' DROP COLUMN age Delete Column (SQLite) SQLite (the version of SQL used in this tutorial) does not allow you to drop a column. The workaround is to make a new table that contains only the columns you want to keep, then rename the new table to the original template's name. %% sql -- Create a table called 'criminals_tamps' with the columns we want to not drop CREATE TABLE criminals_temp ( pid , name , sex ); -- Copy the data from the columns we want to keep to the new table INSERT INTO criminals_temp SELECT pid , name , sex FROM criminals ; -- Delete the original table DROP TABLE criminals ; -- Rename the new table to the original table 's name ALTER TABLE criminals_temp RENAME TO criminals ; [] View Table %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals pid name sex 412 James Smith M 234 Bill James M 632 Stacy Miller F 621 Betty Bob F 162 Jaden Ado M 901 Gordon Ado F 512 Bill Byson M 411 Bob Iton M","tags":"SQL","url":"http://chrisalbon.com/sql/drop_a_column.html","loc":"http://chrisalbon.com/sql/drop_a_column.html"},{"title":"Enumerate A List","text":"# Create a list of strings data = [ 'One' , 'Two' , 'Three' , 'Four' , 'Five' ] # For each item in the enumerated variable, data for item in enumerate ( data ): # Print the whole enumerated element print ( item ) # Print only the value (not the index number) print ( item [ 1 ]) ( 0 , 'One' ) One ( 1 , 'Two' ) Two ( 2 , 'Three' ) Three ( 3 , 'Four' ) Four ( 4 , 'Five' ) Five","tags":"Python","url":"http://chrisalbon.com/python/enumerate_a_list.html","loc":"http://chrisalbon.com/python/enumerate_a_list.html"},{"title":"Filter pandas Dataframes","text":"Import modules import pandas as pd Create Dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 View Column df [ 'name' ] Cochice Jason Pima Molly Santa Cruz Tina Maricopa Jake Yuma Amy Name: name, dtype: object View Two Columns df [[ 'name' , 'reports' ]] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports Cochice Jason 4 Pima Molly 24 Santa Cruz Tina 31 Maricopa Jake 2 Yuma Amy 3 View First Two Rows df [: 2 ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 View Rows Where Coverage Is Greater Than 50 df [ df [ 'coverage' ] > 50 ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } coverage name reports year Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 View Rows Where Coverage Is Greater Than 50 And Reports Less Than 4 df [( df [ 'coverage' ] > 50 ) & ( df [ 'reports' ] < 4 )] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } coverage name reports year Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/filter_dataframes.html","loc":"http://chrisalbon.com/python/filter_dataframes.html"},{"title":"Selecting Items In A List With Filters","text":"# Create an list of items denoting the number of soldiers in each regiment, view the list regimentSize = ( 5345 , 6436 , 3453 , 2352 , 5212 , 6232 , 2124 , 3425 , 1200 , 1000 , 1211 ); regimentSize (5345, 6436, 3453, 2352, 5212, 6232, 2124, 3425, 1200, 1000, 1211) One-line Method This line of code does the same thing as the multiline method below, it is just more compact (but also more complicated to understand. # Create a list called smallRegiments that filters regimentSize to # find all items that fulfill the lambda function (which looks for all items under 2500). smallRegiments = list ( filter (( lambda x : x < 2500 ), regimentSize )); smallRegiments [2352, 2124, 1200, 1000, 1211] Multi-line Method The ease with interpreting what is happening, I've broken down the one-line filter method into multiple steps, one per line of code. This appears below. # Create a lambda function that looks for things under 2500 lessThan2500Filter = lambda x : x < 2500 # Filter regimentSize by the lambda function filter filteredRegiments = filter ( lessThan2500Filter , regimentSize ) # Convert the filter results into a list smallRegiments = list ( filteredRegiments ) [2352, 2124, 1200, 1000, 1211] For Loop Equivalent This for loop does the same as both methods above, except it uses a for loop. Create a for loop that go through each item of a list and finds items under 2500 # Create a variable for the results of the loop to be placed smallRegiments_2 = [] # for each item in regimentSize, for x in regimentSize : # look if the item's value is less than 2500 if x < 2500 : # if true, add that item to smallRegiments_2 smallRegiments_2 . append ( x ) # View the smallRegiment_2 variable smallRegiments_2 [2352, 2124, 1200, 1000, 1211]","tags":"Python","url":"http://chrisalbon.com/python/filter_items_in_list_with_filter.html","loc":"http://chrisalbon.com/python/filter_items_in_list_with_filter.html"},{"title":"Flatten Lists Of Lists","text":"# Create a list containing three lists of names list_of_lists = [[ 'Amy' , 'Betty' , 'Cathryn' , 'Dana' ], [ 'Elizabeth' , 'Fay' , 'Gora' ], [ 'Heidi' , 'Jane' , 'Kayley' ]] # For each element in list_of_lists, take each element in the list flattened_list = [ i for row in list_of_lists for i in row ] # View the flattened list flattened_list ['Amy', 'Betty', 'Cathryn', 'Dana', 'Elizabeth', 'Fay', 'Gora', 'Heidi', 'Jane', 'Kayley']","tags":"Python","url":"http://chrisalbon.com/python/flatten_list_of_lists.html","loc":"http://chrisalbon.com/python/flatten_list_of_lists.html"},{"title":"For Loop","text":"The for loop iterates over each item in a sequences. # One at a time, assign each value of the sequence to i and, for i in [ 432 , 342 , 928 , 920 ]: # multiply i by 10 and store the product in a new variable, x create a new variable, x, x = i * 10 # print the value of x print ( x ) # after the entire sequence processes, else : # print this print ( 'All done!' ) 4320 3420 9280 9200 All done!","tags":"Python","url":"http://chrisalbon.com/python/for_loops.html","loc":"http://chrisalbon.com/python/for_loops.html"},{"title":"Function Basics","text":"Create Function Called print_max def print_max ( x , y ): # if a is larger than b if x > y : # then print this print ( x , 'is maximum' ) # if a is equal to b elif x == y : # print this print ( x , 'is equal to' , y ) # otherwise else : # print this print ( y , 'is maximum' ) Run Function With Two Arguments print_max ( 3 , 4 ) 4 is maximum Note: By default, variables created within functions are local to the function. But you can create a global function that IS defined outside the function. Create Variable x = 50 Create Function Called Func # Create function def func (): # Create a global variable called x global x # Print this print ( 'x is' , x ) # Set x to 2. x = 2 # Print this print ( 'Changed global x to' , x ) Run func() func () x is 50 Changed global x to 2 Print x x 2 Create Function Say() Displaying x with default value of 1 # Create function def say ( x , times = 1 , times2 = 3 ): print ( x * times , x * times2 ) # Run the function say() with the default values say ( '!' ) # Run the function say() with the non-default values of 5 and 10 say ( '!' , 5 , 10 ) ! !!! !!!!! !!!!!!!!!! VarArgs Parameters (i.e. unlimited number of parameters) * denotes that all positonal arguments from that point to next arg are used ** dnotes that all keyword arguments from that point to the next arg are used # Create a function called total() with three parameters def total ( initial = 5 , * numbers , ** keywords ): # Create a variable called count that takes it's value from initial count = initial # for each item in numbers for number in numbers : # add count to that number count += number # for each item in keywords for key in keywords : # add count to keyword's value count += keywords [ key ] # return counts return count # Run function total ( 10 , 1 , 2 , 3 , vegetables = 50 , fruits = 100 ) 166","tags":"Python","url":"http://chrisalbon.com/python/function_basics.html","loc":"http://chrisalbon.com/python/function_basics.html"},{"title":"Generating Random Numbers With Numpy","text":"Import Numpy import numpy as np Generate A Random Number From The Normal Distribution np . random . normal () 0.5661104974399703 Generate Four Random Numbers From The Normal Distribution np . random . normal ( size = 4 ) array([-1.03175853, 1.2867365 , -0.23560103, -1.05225393]) Generate Four Random Numbers From The Uniform Distribution np . random . uniform ( size = 4 ) array([ 0.00193123, 0.51932356, 0.87656884, 0.33684494]) Generate Four Random Integers Between 1 and 100 np . random . randint ( low = 1 , high = 100 , size = 4 ) array([96, 25, 94, 77])","tags":"Python","url":"http://chrisalbon.com/python/generating_random_numbers_with_numpy.html","loc":"http://chrisalbon.com/python/generating_random_numbers_with_numpy.html"},{"title":"Geocoding And Reverse Geocoding","text":"Geocoding (converting a physical address or location into latitude/longitude) and reverse geocoding (converting a lat/long to a physical address or location) are common tasks when working with geo-data. Python offers a number of packages to make the task incredibly easy. In the tutorial below, I use pygeocoder, a wrapper for Google's geo-API, to both geocode and reverse geocode. Preliminaries First we want to load the packages we will want to use in the script. Specifically, I am loading pygeocoder for its geo-functionality, pandas for its dataframe structures, and numpy for its missing value (np.nan) functionality. # Load packages from pygeocoder import Geocoder import pandas as pd import numpy as np Create some simulated geo data Geo-data comes in a wide variety of forms, in this case we have a Python dictionary of five latitude and longitude strings, with each coordinate in a coordinate pair separated by a comma. # Create a dictionary of raw data data = { 'Site 1' : '31.336968, -109.560959' , 'Site 2' : '31.347745, -108.229963' , 'Site 3' : '32.277621, -107.734724' , 'Site 4' : '31.655494, -106.420484' , 'Site 5' : '30.295053, -104.014528' } While technically unnecessary, because I originally come from R, I am a big fan of dataframes, so let us turn the dictionary of simulated data into a dataframe. # Convert the dictionary into a pandas dataframe df = pd . DataFrame . from_dict ( data , orient = 'index' ) # View the dataframe df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 0 Site 4 31.655494, -106.420484 Site 3 32.277621, -107.734724 Site 1 31.336968, -109.560959 Site 5 30.295053, -104.014528 Site 2 31.347745, -108.229963 You can see now that we have a a dataframe with five rows, with each now containing a string of latitude and longitude. Before we can work with the data, we'll need to 1) seperate the strings into latitude and longitude and 2) convert them into floats. The function below does just that. # Create two lists for the loop results to be placed lat = [] lon = [] # For each row in a varible, for row in df [ 0 ]: # Try to, try : # Split the row by comma, convert to float, and append # everything before the comma to lat lat . append ( float ( row . split ( ',' )[ 0 ])) # Split the row by comma, convert to float, and append # everything after the comma to lon lon . append ( float ( row . split ( ',' )[ 1 ])) # But if you get an error except : # append a missing value to lat lat . append ( np . NaN ) # append a missing value to lon lon . append ( np . NaN ) # Create two new columns from lat and lon df [ 'latitude' ] = lat df [ 'longitude' ] = lon Let's take a took a what we have now. # View the dataframe df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 0 latitude longitude Site 4 31.655494, -106.420484 31.655494 -106.420484 Site 3 32.277621, -107.734724 32.277621 -107.734724 Site 1 31.336968, -109.560959 31.336968 -109.560959 Site 5 30.295053, -104.014528 30.295053 -104.014528 Site 2 31.347745, -108.229963 31.347745 -108.229963 Awesome. This is exactly what we want to see, one column of floats for latitude and one column of floats for longitude. Reverse Geocoding To reverse geocode, we feed a specific latitude and longitude pair, in this case the first row (indexed as '0') into pygeocoder's reverse_geocoder function. # Convert longitude and latitude to a location results = Geocoder . reverse_geocode ( df [ 'latitude' ][ 0 ], df [ 'longitude' ][ 0 ]) Now we can take can start pulling out the data that we want. # Print the lat/long results . coordinates (31.6556534, -106.4204309) # Print the city results . city 'Ciudad Juárez' # Print the country results . country 'Mexico' # Print the street address (if applicable) results . street_address # Print the admin1 level results . administrative_area_level_1 'Chihuahua' Geocoding For geocoding, we need to submit a string containing an address or location (such as a city) into the geocode function. However, not all strings are formatted in a way that Google's geo-API can make sense of them. We can text if an input is valid by using the .geocode().valid_address function. # Verify that an address is valid (i.e. in Google's system) Geocoder . geocode ( \"4207 N Washington Ave, Douglas, AZ 85607\" ) . valid_address True Because the output was True, we now know that this is a valid address and thus can print the latitude and longitude coordinates. # Print the lat/long results . coordinates (31.6556534, -106.4204309) But even more interesting, once the address is processed by the Google geo API, we can parse it and easily separate street numbers, street names, etc. # Find the lat/long of a certain address result = Geocoder . geocode ( \"7250 South Tucson Boulevard, Tucson, AZ 85756\" ) # Print the street number result . street_number '7250' # Print the street name result . route 'South Tucson Boulevard' And there you have it. Python makes this entire process easy and inserting it into an analysis only takes a few minutes. Good luck!","tags":"Python","url":"http://chrisalbon.com/python/geocoding_and_reverse_geocoding.html","loc":"http://chrisalbon.com/python/geocoding_and_reverse_geocoding.html"},{"title":"GitHub Cheatsheet","text":"Find The Version Of Git git --version Create A New Git Repository Go to the fold of the project. Run git init Clone An Existing Git Repository Cloning is the process of pulling down a copy of a repository stored on a server. Go to the parent folder of where you want to repository's folder to be in. git clone [url to repository's git file] [name of folder / repository you want] Check The Status Of A Git Repository git status Tell Git To Track A File git add readme.md Make A Commit git commit --m \"First commit\" Tell Git To Track A Whole Folder git add chapter2/ Tell Git To Track (And Stage) All Files And Subfolders In A Directory git add -A View All Branches git branch Create A New Branch git branch new_model Switch To A Branch git checkout new_model Create A New Branch And Switch To It git checkout -b new_ux Merge One Branch Into Another Switch to the branch you want to pull changes into: git checkout master Pull changes from another branch into your branch: git merge new_ux Set A Remote Github Repository Go to GitHub.com and create a new repository. Set that repository's url as the origin repo: git remote add origin https://github.com/chrisalbon/git_tutorial.git Push Master Branch To A Github Repository The -u sets the origin as the default for this branch git push -u origin master Pull Down From A Branch From A GitHub Repository To Local Repository git pull origin master Pull Down All Branches From GitHub git fetch origin View All Remote Branches git branch --remote View Log git log View Unstagged Changes To Files git diff Unstage A File git reset filename Undo Last Commit, Move Commits Changes To Staging git reset --soft HEAD&#94; Undo Last Commit, Remove All Changes In Your Working Directory git reset --hard HEAD&#94; Clone A Remote Repository Locally git clone url Show Changes From A Particular Commit git show --pretty=\"format:\" <commitID> Revert A Commit By Creating A New Commit With Opposite Changes git revert <commitID>","tags":"Cloud Computing","url":"http://chrisalbon.com/cloud-computing/github_cheatsheet.html","loc":"http://chrisalbon.com/cloud-computing/github_cheatsheet.html"},{"title":"if and if else","text":"Create a variable with the status of the conflict. 1 if the conflict is active 0 if the conflict is not active unknown if the status of the conflict is unknwon conflict_active = 1 If the conflict is active print a statement if conflict_active == 1 : print ( 'The conflict is active.' ) The conflict is active. If the conflict is active print a statement, if not, print a different statement if conflict_active == 1 : print ( 'The conflict is active.' ) else : print ( 'The conflict is not active.' ) The conflict is active. If the conflict is active print a statement, if not, print a different statement, if unknown, state a third statement. if conflict_active == 1 : print ( 'The conflict is active.' ) elif conflict_active == 'unknown' : print ( 'The status of the conflict is unknown' ) else : print ( 'The conflict is not active.' ) The conflict is active.","tags":"Python","url":"http://chrisalbon.com/python/if_and_if_else_statements.html","loc":"http://chrisalbon.com/python/if_and_if_else_statements.html"},{"title":"If Else On Any Or All Elements","text":"Preliminaries # import pandas as pd import pandas as pd Create a simulated dataset # Create an example dataframe data = { 'score' : [ 1 , 2 , 3 , 4 , 5 ]} df = pd . DataFrame ( data ) df score 0 1 1 2 2 3 3 4 4 5 Does any cell equal 3? # If any element in df.score equals three, if ( df . score == 3 ) . any (): # Print this print ( 'Does any cells equal 3? Yes!' ) # Otherwise, else : # Print this print ( 'Does any cells equal 3? No!' ) Does any cells equal 3? Yes! Do all cells equal 3? # If all elements in df.score equal three, if ( df . score == 3 ) . all (): # Print this print ( 'Do all cells equal 3? Yes!' ) # Otherwise else : # Print this print ( 'Do all cells equal 3? No!' ) Do all cells equal 3? No!","tags":"Python","url":"http://chrisalbon.com/python/ifelse_on_any_or_all_elements.html","loc":"http://chrisalbon.com/python/ifelse_on_any_or_all_elements.html"},{"title":"Indexing And Slicing Numpy Arrays","text":"Slicing Arrays Explanation Of Broadcasting Unlike many other data types, slicing an array into a new variable means that any chances to that new variable are broadcasted to the original variable. Put other way, a slice is a hotlink to the original array variable, not a seperate and independent copy of it. # Import Modules import numpy as np # Create an array of battle casualties from the first to the last battle battleDeaths = np . array ([ 1245 , 2732 , 3853 , 4824 , 5292 , 6184 , 7282 , 81393 , 932 , 10834 ]) # Divide the array of battle deaths into start, middle, and end of the war warStart = battleDeaths [ 0 : 3 ]; print ( 'Death from battles at the start of war:' , warStart ) warMiddle = battleDeaths [ 3 : 7 ]; print ( 'Death from battles at the middle of war:' , warMiddle ) warEnd = battleDeaths [ 7 : 10 ]; print ( 'Death from battles at the end of war:' , warEnd ) Death from battles at the start of war: [1245 2732 3853] Death from battles at the middle of war: [4824 5292 6184 7282] Death from battles at the end of war: [81393 932 10834] # Change the battle death numbers from the first battle warStart [ 0 ] = 11101 # View that change reflected in the warStart slice of the battleDeaths array warStart array([11101, 2732, 3853]) # View that change reflected in (i.e. \"broadcasted to) the original battleDeaths array battleDeaths array([11101, 2732, 3853, 4824, 5292, 6184, 7282, 81393, 932, 10834]) Indexing Arrays Note: This multidimensional array behaves like a dataframe or matrix (i.e. columns and rows) # Create an array of regiment information regimentNames = [ 'Nighthawks' , 'Sky Warriors' , 'Rough Riders' , 'New Birds' ] regimentNumber = [ 1 , 2 , 3 , 4 ] regimentSize = [ 1092 , 2039 , 3011 , 4099 ] regimentCommander = [ 'Mitchell' , 'Blackthorn' , 'Baker' , 'Miller' ] regiments = np . array ([ regimentNames , regimentNumber , regimentSize , regimentCommander ]) regiments array([['Nighthawks', 'Sky Warriors', 'Rough Riders', 'New Birds'], ['1', '2', '3', '4'], ['1092', '2039', '3011', '4099'], ['Mitchell', 'Blackthorn', 'Baker', 'Miller']], dtype='<U12') # View the first column of the matrix regiments [:, 0 ] array(['Nighthawks', '1', '1092', 'Mitchell'], dtype='<U12') # View the second row of the matrix regiments [ 1 ,] array(['1', '2', '3', '4'], dtype='<U12') # View the top-right quarter of the matrix regiments [: 2 , 2 :] array([['Rough Riders', 'New Birds'], ['3', '4']], dtype='<U12')","tags":"Python","url":"http://chrisalbon.com/python/indexing_and_slicing_numpy_arrays.html","loc":"http://chrisalbon.com/python/indexing_and_slicing_numpy_arrays.html"},{"title":"Iterate An Ifelse Over A List","text":"Create some data word_list = [ 'Egypt' , 'Watching' , 'Eleanor' ] vowels = [ 'A' , 'E' , 'I' , 'O' , 'U' ] Create a for loop # for each item in the word_list, for word in word_list : # if any word starts with e, where e is vowels, if any ([ word . startswith ( e ) for e in vowels ]): # then print is valid, print ( 'Is valid' ) # if not, else : # print invalid print ( 'Invalid' ) Is valid Invalid Is valid","tags":"Python","url":"http://chrisalbon.com/python/iterate_ifelse_over_list.html","loc":"http://chrisalbon.com/python/iterate_ifelse_over_list.html"},{"title":"Lambda Functions","text":"In Python it is possible to string lambda functions together. Create a series, called pipeline, that contains three mini functions pipeline = [ lambda x : x ** 2 - 1 + 5 , lambda x : x ** 20 - 2 + 3 , lambda x : x ** 200 - 1 + 4 ] For each item in pipeline, run the lambda function with x = 3 for f in pipeline : print ( f ( 3 )) 13 3486784402 265613988875874769338781322035779626829233452653394495974574961739092490901302182994384699044004","tags":"Python","url":"http://chrisalbon.com/python/lambda_functions.html","loc":"http://chrisalbon.com/python/lambda_functions.html"},{"title":"Logical Operations","text":"Create some simulated variables x = 6 y = 9 z = 12 x or y x or y 6 x and y x and y 9 not x not x False x is equal to y x == y False x is not equal to y x != y True One is less than two 1 < 2 True Two is less than or equal to four 2 <= 4 True Three is equal to five 3 == 5 False Three is not equal to four 3 != 4 True x is less than y which is less than z x < y < z True x is less than y and y is less than z x < y and y < z True","tags":"Python","url":"http://chrisalbon.com/python/logical_operations.html","loc":"http://chrisalbon.com/python/logical_operations.html"},{"title":"Match A Symbol","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '$100' Apply regex # Find all instances of the exact match '$' re . findall ( r '\\$' , text ) ['$']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_symbol.html","loc":"http://chrisalbon.com/regex/match_a_symbol.html"},{"title":"Match A Unicode Character","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Microsoft™.' Apply regex # Find any unicode character for a trademark re . findall ( r '\\u2122' , text ) ['™']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_unicode_character.html","loc":"http://chrisalbon.com/regex/match_a_unicode_character.html"},{"title":"Match A Word","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any word of three letters re . findall ( r '\\b...\\b' , text ) ['The', 'fox', 'the']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_word.html","loc":"http://chrisalbon.com/regex/match_a_word.html"},{"title":"Match Any Character","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find anything with a 'T' and then the next two characters re . findall ( r 'T..' , text ) ['The']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_character.html","loc":"http://chrisalbon.com/regex/match_any_character.html"},{"title":"Match Any Of A List Of Characters","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find all instances of any vowel re . findall ( r '[aeiou]' , text ) ['e', 'u', 'i', 'o', 'o', 'u', 'e', 'o', 'e', 'e', 'a', 'o', 'e', 'a']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_a_list_of_symbols.html","loc":"http://chrisalbon.com/regex/match_any_of_a_list_of_symbols.html"},{"title":"Match Any Of A Series Of Options","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any of fox, snake, or bear re . findall ( r 'fox|snake|bear' , text ) ['fox', 'bear']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_series_of_characters.html","loc":"http://chrisalbon.com/regex/match_any_of_series_of_characters.html"},{"title":"Match Any Of A Series Of Words","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any of fox, snake, or bear re . findall ( r '\\b(fox|snake|bear)\\b' , text ) ['fox', 'bear']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_series_of_words.html","loc":"http://chrisalbon.com/regex/match_any_of_series_of_words.html"},{"title":"Match Dates","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My birthday is 09/15/1983. My brother \\' s birthday is 01/01/01. My other two brothers have birthdays of 9/3/2001 and 09/1/83.' Apply regex # Find any text that fits the regex re . findall ( r '\\b[0-3]?[0-9]/[0-3]?[0-9]/(?:[0-9]{2})?[0-9]{2}\\b' , text ) ['09/15/1983', '01/01/01', '9/3/2001', '09/1/83']","tags":"Regex","url":"http://chrisalbon.com/regex/match_dates.html","loc":"http://chrisalbon.com/regex/match_dates.html"},{"title":"Match Email Addresses","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My email is chris@hotmail.com, thanks! No, I am at bob@data.ninja.' Apply regex # Find all email addresses re . findall ( r '[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9]+' , text ) # Explanation: # This regex has three parts # [a-zA-Z0-9_.+-]+ Matches a word (the username) of any length # @[a-zA-Z0-9-]+ Matches a word (the domain name) of any length # \\.[a-zA-Z0-9-.]+ Matches a word (the TLD) of any length ['chris@hotmail.com', 'bob@data.ninja']","tags":"Regex","url":"http://chrisalbon.com/regex/match_email_addresses.html","loc":"http://chrisalbon.com/regex/match_email_addresses.html"},{"title":"Match Exact Text","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find all instances of the exact match 'The' re . findall ( r 'The' , text ) ['The']","tags":"Regex","url":"http://chrisalbon.com/regex/match_exact_text.html","loc":"http://chrisalbon.com/regex/match_exact_text.html"},{"title":"Match Integers Of Any Length","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '21 scouts and 3 tanks fought against 4,003 protestors.' Apply regex # Find any character block that is a integer of any length re . findall ( r '[1-9](?:\\d{0,2})(?:,\\d{3})*(?:\\.\\d*[1-9])?|0?\\.\\d*[1-9]|0' , text ) ['21', '3', '4,003'] Explanation from Justin Morgan [1-9](?:\\d{0,2}) #A sequence of 1-3 numerals not starting with 0 (?:,\\d{3})* #Any number of three-digit groups, each preceded by a comma (?:\\.\\d*[1-9])? #Optionally, a decimal point followed by any number of digits not ending in 0 | #OR... 0?\\.\\d*[1-9] #Only the decimal portion, optionally preceded by a 0 | #OR... 0 #Zero.","tags":"Regex","url":"http://chrisalbon.com/regex/match_integers_of_any_length.html","loc":"http://chrisalbon.com/regex/match_integers_of_any_length.html"},{"title":"Match Text Between HTML Tags","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '<p>The quick brown fox.</p><p>The lazy brown bear.</p>' Apply regex # Find any text between '<p>' and '</p>' re . findall ( r '<p>(.*?)</p>' , text ) ['The quick brown fox.', 'The lazy brown bear.']","tags":"Regex","url":"http://chrisalbon.com/regex/match_text_between_html_tags.html","loc":"http://chrisalbon.com/regex/match_text_between_html_tags.html"},{"title":"Match Times","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Chris: 12:34am. Steve: 16:30' Apply regex # Find any text that fits the regex re . findall ( r '([0-1]\\d:[0-5]\\d)\\s*(?:AM|PM)?' , text ) ['12:34', '16:30']","tags":"Regex","url":"http://chrisalbon.com/regex/match_times.html","loc":"http://chrisalbon.com/regex/match_times.html"},{"title":"Match URLs","text":"Source: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My blog is http://www.chrisalbon.com and not http://chrisalbon.com' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r '(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?&#94;=%&amp;:/~\\+#]*[\\w\\-\\@?&#94;=%&amp;/~\\+#])?' , text ) [('http', 'www.chrisalbon.com', ''), ('http', 'chrisalbon.com', '')]","tags":"Regex","url":"http://chrisalbon.com/regex/match_urls.html","loc":"http://chrisalbon.com/regex/match_urls.html"},{"title":"Match US Phone Numbers","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My phone number is 415-333-3922. His phone number is 4239389283' Apply regex # Find any text that fits the regex re . findall ( r '\\(?([2-9][0-8][0-9])\\)?[-.●]?([2-9][0-9]{2})[-.●]?([0-9]{4})' , text ) [('415', '333', '3922'), ('423', '938', '9283')]","tags":"Regex","url":"http://chrisalbon.com/regex/match_us_phone_numbers.html","loc":"http://chrisalbon.com/regex/match_us_phone_numbers.html"},{"title":"Match US and UK Spellings","text":"Source: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'It\\s center and not centre.' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r '\\bcent(?:er|re)\\b' , text ) ['center', 'centre']","tags":"Regex","url":"http://chrisalbon.com/regex/match_us_uk_spellings.html","loc":"http://chrisalbon.com/regex/match_us_uk_spellings.html"},{"title":"Match Words With A Certain Ending","text":"Source: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Capitalism, Communism, Neorealism, Liberalism' Apply regex # Find any word ending in 'ism' re . findall ( r '\\b\\w*ism\\b' , text ) # Specific: # \\b - start of the word # \\w* - a word of any length # ism\\b - with 'ism'at the end ['Capitalism', 'Communism', 'Neorealism', 'Liberalism']","tags":"Regex","url":"http://chrisalbon.com/regex/match_words_with_certain_ending.html","loc":"http://chrisalbon.com/regex/match_words_with_certain_ending.html"},{"title":"Match ZIP Codes","text":"Source: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '3829 South Ave Street, Pheonix, AZ 34923' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r '[0-9]{5}(?:-[0-9]{4})?' , text ) ['34923']","tags":"Regex","url":"http://chrisalbon.com/regex/match_zip_codes.html","loc":"http://chrisalbon.com/regex/match_zip_codes.html"},{"title":"Mathematical Operations","text":"Import the math module import math Display the value of pi. math . pi 3.141592653589793 Display the value of e. math . e 2.718281828459045 Sine, cosine, and tangent math . sin ( 2 * math . pi / 180 ) 0.03489949670250097 Exponent 2 ** 4 , pow ( 2 , 4 ) (16, 16) Absolute value abs ( - 20 ) 20 Summation sum (( 1 , 2 , 3 , 4 )) 10 Minimum min ( 3 , 9 , 10 , 12 ) 3 Maximum max ( 3 , 5 , 10 , 15 ) 15 Floor math . floor ( 2.949 ) 2 Truncate (drop decimal digits) math . trunc ( 32.09292 ) 32 Truncate (integer conversion) int ( 3.292838 ) 3 Round to an integrer round ( 2.943 ), round ( 2.499 ) (3, 2) Round by 2 digits round ( 2.569 , 2 ) 2.57","tags":"Python","url":"http://chrisalbon.com/python/math_operations.html","loc":"http://chrisalbon.com/python/math_operations.html"},{"title":"Back To Back Bar Plot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # input data, specifically the second and # third rows, skipping the first column x1 = df . ix [ 1 , 1 :] x2 = df . ix [ 2 , 1 :] # Create the bar labels bar_labels = [ 'Pre Score' , 'Mid Score' , 'Post Score' ] # Create a figure fig = plt . figure ( figsize = ( 8 , 6 )) # Set the y position y_pos = np . arange ( len ( x1 )) y_pos = [ x for x in y_pos ] plt . yticks ( y_pos , bar_labels , fontsize = 10 ) # Create a horizontal bar in the position y_pos plt . barh ( y_pos , # using x1 data x1 , # that is centered align = 'center' , # with alpha 0.4 alpha = 0.4 , # and color green color = '#263F13' ) # Create a horizontal bar in the position y_pos plt . barh ( y_pos , # using NEGATIVE x2 data - x2 , # that is centered align = 'center' , # with alpha 0.4 alpha = 0.4 , # and color green color = '#77A61D' ) # annotation and labels plt . xlabel ( 'Tina \\' s Score: Light Green. Molly \\' s Score: Dark Green' ) t = plt . title ( 'Comparison of Molly and Tina \\' s Score' ) plt . ylim ([ - 1 , len ( x1 ) + 0.1 ]) plt . xlim ([ - max ( x2 ) - 10 , max ( x1 ) + 10 ]) plt . grid () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_back_to_back_bar_plot.html","loc":"http://chrisalbon.com/python/matplotlib_back_to_back_bar_plot.html"},{"title":"Bar Plot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create a list of the mean scores for each variable mean_values = [ df [ 'pre_score' ] . mean (), df [ 'mid_score' ] . mean (), df [ 'post_score' ] . mean ()] # Create a list of variances, which are set at .25 above and below the score variance = [ df [ 'pre_score' ] . mean () * 0.25 , df [ 'pre_score' ] . mean () * 0.25 , df [ 'pre_score' ] . mean () * 0.25 ] # Set the bar labels bar_labels = [ 'Pre Score' , 'Mid Score' , 'Post Score' ] # Create the x position of the bars x_pos = list ( range ( len ( bar_labels ))) # Create the plot bars # In x position plt . bar ( x_pos , # using the data from the mean_values mean_values , # with a y-error lines set at variance yerr = variance , # aligned in the center align = 'center' , # with color color = '#FFC222' , # alpha 0.5 alpha = 0.5 ) # add a grid plt . grid () # set height of the y-axis max_y = max ( zip ( mean_values , variance )) # returns a tuple, here: (3, 5) plt . ylim ([ 0 , ( max_y [ 0 ] + max_y [ 1 ]) * 1.1 ]) # set axes labels and title plt . ylabel ( 'Score' ) plt . xticks ( x_pos , bar_labels ) plt . title ( 'Mean Scores For Each Test' ) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_bar_plot.html","loc":"http://chrisalbon.com/python/matplotlib_bar_plot.html"},{"title":"Group Bar Plot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Setting the positions and width for the bars pos = list ( range ( len ( df [ 'pre_score' ]))) width = 0.25 # Plotting the bars fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # Create a bar with pre_score data, # in position pos, plt . bar ( pos , #using df['pre_score'] data, df [ 'pre_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#EE3224' , # with label the first value in first_name label = df [ 'first_name' ][ 0 ]) # Create a bar with mid_score data, # in position pos + some width buffer, plt . bar ([ p + width for p in pos ], #using df['mid_score'] data, df [ 'mid_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#F78F1E' , # with label the second value in first_name label = df [ 'first_name' ][ 1 ]) # Create a bar with post_score data, # in position pos + some width buffer, plt . bar ([ p + width * 2 for p in pos ], #using df['post_score'] data, df [ 'post_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#FFC222' , # with label the third value in first_name label = df [ 'first_name' ][ 2 ]) # Set the y axis label ax . set_ylabel ( 'Score' ) # Set the chart's title ax . set_title ( 'Test Subject Scores' ) # Set the position of the x ticks ax . set_xticks ([ p + 1.5 * width for p in pos ]) # Set the labels for the x ticks ax . set_xticklabels ( df [ 'first_name' ]) # Setting the x-axis and y-axis limits plt . xlim ( min ( pos ) - width , max ( pos ) + width * 4 ) plt . ylim ([ 0 , max ( df [ 'pre_score' ] + df [ 'mid_score' ] + df [ 'post_score' ])] ) # Adding the legend and showing the plot plt . legend ([ 'Pre Score' , 'Mid Score' , 'Post Score' ], loc = 'upper left' ) plt . grid () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_grouped_bar_plot.html","loc":"http://chrisalbon.com/python/matplotlib_grouped_bar_plot.html"},{"title":"Histograms In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np import math # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create dataframe df = pd . read_csv ( 'https://www.dropbox.com/s/52cb7kcflr8qm2u/5kings_battles_v1.csv?dl=1' ) df . head () name year battle_number attacker_king defender_king attacker_1 attacker_2 attacker_3 attacker_4 defender_1 defender_2 defender_3 defender_4 attacker_outcome battle_type major_death major_capture attacker_size defender_size attacker_commander defender_commander summer location region note 0 Battle of the Golden Tooth 298 1 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 1 0 15000 4000 Jaime Lannister Clement Piper, Vance 1 Golden Tooth The Westerlands NaN 1 Battle at the Mummer's Ford 298 2 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Baratheon NaN NaN NaN win ambush 1 0 NaN 120 Gregor Clegane Beric Dondarrion 1 Mummer's Ford The Riverlands NaN 2 Battle of Riverrun 298 3 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 0 1 15000 10000 Jaime Lannister, Andros Brax Edmure Tully, Tytos Blackwood 1 Riverrun The Riverlands NaN 3 Battle of the Green Fork 298 4 Robb Stark Joffrey/Tommen Baratheon Stark NaN NaN NaN Lannister NaN NaN NaN loss pitched battle 1 1 18000 20000 Roose Bolton, Wylis Manderly, Medger Cerwyn, H... Tywin Lannister, Gregor Clegane, Kevan Lannist... 1 Green Fork The Riverlands NaN 4 Battle of the Whispering Wood 298 5 Robb Stark Joffrey/Tommen Baratheon Stark Tully NaN NaN Lannister NaN NaN NaN win ambush 1 1 1875 6000 Robb Stark, Brynden Tully Jaime Lannister 1 Whispering Wood The Riverlands NaN Make plot with bins of fixed size # Make two variables of the attacker and defender size, but leaving out # cases when there are over 10000 attackers data1 = df [ 'attacker_size' ][ df [ 'attacker_size' ] < 90000 ] data2 = df [ 'defender_size' ][ df [ 'attacker_size' ] < 90000 ] # Create bins of 2000 each bins = np . arange ( data1 . min (), data2 . max (), 2000 ) # fixed bin size # Plot a histogram of attacker size plt . hist ( data1 , bins = bins , alpha = 0.5 , color = '#EDD834' , label = 'Attacker' ) # Plot a histogram of defender size plt . hist ( data2 , bins = bins , alpha = 0.5 , color = '#887E43' , label = 'Defender' ) # Set the x and y boundaries of the figure plt . ylim ([ 0 , 10 ]) # Set the title and labels plt . title ( 'Histogram of Attacker and Defender Size' ) plt . xlabel ( 'Number of troops' ) plt . ylabel ( 'Number of battles' ) plt . legend ( loc = 'upper right' ) plt . show () Make plot with fixed number of bins # Make two variables of the attacker and defender size, but leaving out # cases when there are over 10000 attackers data1 = df [ 'attacker_size' ][ df [ 'attacker_size' ] < 90000 ] data2 = df [ 'defender_size' ][ df [ 'attacker_size' ] < 90000 ] # Create 10 bins with the minimum # being the smallest value of data1 and data2 bins = np . linspace ( min ( data1 + data2 ), # the max being the highest value max ( data1 + data2 ), # and divided into 10 bins 10 ) # Plot a histogram of attacker size plt . hist ( data1 , # with bins defined as bins = bins , # with alpha alpha = 0.5 , # with color color = '#EDD834' , # labelled attacker label = 'Attacker' ) # Plot a histogram of defender size plt . hist ( data2 , # with bins defined as bins = bins , # with alpha alpha = 0.5 , # with color color = '#887E43' , # labeled defender label = 'Defender' ) # Set the x and y boundaries of the figure plt . ylim ([ 0 , 10 ]) # Set the title and labels plt . title ( 'Histogram of Attacker and Defender Size' ) plt . xlabel ( 'Number of troops' ) plt . ylabel ( 'Number of battles' ) plt . legend ( loc = 'upper right' ) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_histogram.html","loc":"http://chrisalbon.com/python/matplotlib_histogram.html"},{"title":"Stacked Percentage Bar Plot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create a figure with a single subplot f , ax = plt . subplots ( 1 , figsize = ( 10 , 5 )) # Set bar width at 1 bar_width = 1 # positions of the left bar-boundaries bar_l = [ i for i in range ( len ( df [ 'pre_score' ]))] # positions of the x-axis ticks (center of the bars as bar labels) tick_pos = [ i + ( bar_width / 2 ) for i in bar_l ] # Create the total score for each participant totals = [ i + j + k for i , j , k in zip ( df [ 'pre_score' ], df [ 'mid_score' ], df [ 'post_score' ])] # Create the percentage of the total score the pre_score value for each participant was pre_rel = [ i / j * 100 for i , j in zip ( df [ 'pre_score' ], totals )] # Create the percentage of the total score the mid_score value for each participant was mid_rel = [ i / j * 100 for i , j in zip ( df [ 'mid_score' ], totals )] # Create the percentage of the total score the post_score value for each participant was post_rel = [ i / j * 100 for i , j in zip ( df [ 'post_score' ], totals )] # Create a bar chart in position bar_1 ax . bar ( bar_l , # using pre_rel data pre_rel , # labeled label = 'Pre Score' , # with alpha alpha = 0.9 , # with color color = '#019600' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Create a bar chart in position bar_1 ax . bar ( bar_l , # using mid_rel data mid_rel , # with pre_rel bottom = pre_rel , # labeled label = 'Mid Score' , # with alpha alpha = 0.9 , # with color color = '#3C5F5A' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Create a bar chart in position bar_1 ax . bar ( bar_l , # using post_rel data post_rel , # with pre_rel and mid_rel on bottom bottom = [ i + j for i , j in zip ( pre_rel , mid_rel )], # labeled label = 'Post Score' , # with alpha alpha = 0.9 , # with color color = '#219AD8' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Set the ticks to be first names plt . xticks ( tick_pos , df [ 'first_name' ]) ax . set_ylabel ( \"Percentage\" ) ax . set_xlabel ( \"\" ) # Let the borders of the graphic plt . xlim ([ min ( tick_pos ) - bar_width , max ( tick_pos ) + bar_width ]) plt . ylim ( - 10 , 110 ) # rotate axis labels plt . setp ( plt . gca () . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) # shot plot plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html","loc":"http://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html"},{"title":"Pie Chart In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt Create dataframe raw_data = { 'officer_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'jan_arrests' : [ 4 , 24 , 31 , 2 , 3 ], 'feb_arrests' : [ 25 , 94 , 57 , 62 , 70 ], 'march_arrests' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'officer_name' , 'jan_arrests' , 'feb_arrests' , 'march_arrests' ]) df officer_name jan_arrests feb_arrests march_arrests 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 # Create a column with the total arrests for each officer df [ 'total_arrests' ] = df [ 'jan_arrests' ] + df [ 'feb_arrests' ] + df [ 'march_arrests' ] df officer_name jan_arrests feb_arrests march_arrests total_arrests 0 Jason 4 25 5 34 1 Molly 24 94 43 161 2 Tina 31 57 23 111 3 Jake 2 62 23 87 4 Amy 3 70 51 124 Make plot # Create a list of colors (from iWantHue) colors = [ \"#E13F29\" , \"#D69A80\" , \"#D63B59\" , \"#AE5552\" , \"#CB5C3B\" , \"#EB8076\" , \"#96624E\" ] # Create a pie chart plt . pie ( # using data total)arrests df [ 'total_arrests' ], # with the labels being officer names labels = df [ 'officer_name' ], # with no shadows shadow = False , # with colors colors = colors , # with one slide exploded out explode = ( 0 , 0 , 0 , 0 , 0.15 ), # with the start angle at 90% startangle = 90 , # with the percent listed as a fraction autopct = ' %1.1f%% ' , ) # View the plot drop above plt . axis ( 'equal' ) # View the plot plt . tight_layout () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_pie_chart.html","loc":"http://chrisalbon.com/python/matplotlib_pie_chart.html"},{"title":"Making A Matplotlib Scatterplot From A Pandas Dataframe","text":"Based on: StackOverflow . import modules % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'female' : [ 0 , 1 , 1 , 0 , 1 ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'female' , 'preTestScore' , 'postTestScore' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name age female preTestScore postTestScore 0 Jason Miller 42 0 4 25 1 Molly Jacobson 52 1 24 94 2 Tina Ali 36 1 31 57 3 Jake Milner 24 0 2 62 4 Amy Cooze 73 1 3 70 Scatterplot of preTestScore and postTestScore, with the size of each point determined by age plt . scatter ( df . preTestScore , df . postTestScore , s = df . age ) <matplotlib.collections.PathCollection at 0x10ca42b00> Scatterplot of preTestScore and postTestScore with the size = 300 and the color determined by sex plt . scatter ( df . preTestScore , df . postTestScore , s = 300 , c = df . female ) <matplotlib.collections.PathCollection at 0x10cb90a90>","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_scatterplot_from_pandas.html","loc":"http://chrisalbon.com/python/matplotlib_scatterplot_from_pandas.html"},{"title":"Matplotlib, A Simple Example","text":"Tell Jupyter to load matplotlib and display all visuals created inline (that is, on this page) % matplotlib inline Import matplotlib's pyplot module import matplotlib.pyplot as pyplot Create a simple plot pyplot . plot ([ 1.6 , 2.7 ]) [<matplotlib.lines.Line2D at 0x10c4e7978>]","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_simple_example.html","loc":"http://chrisalbon.com/python/matplotlib_simple_example.html"},{"title":"Scatterplot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create dataframe df = pd . read_csv ( 'https://raw.githubusercontent.com/chrisalbon/war_of_the_five_kings_dataset/master/5kings_battles_v1.csv' ) df . head () name year battle_number attacker_king defender_king attacker_1 attacker_2 attacker_3 attacker_4 defender_1 defender_2 defender_3 defender_4 attacker_outcome battle_type major_death major_capture attacker_size defender_size attacker_commander defender_commander summer location region note 0 Battle of the Golden Tooth 298 1 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 1.0 0.0 15000.0 4000.0 Jaime Lannister Clement Piper, Vance 1.0 Golden Tooth The Westerlands NaN 1 Battle at the Mummer's Ford 298 2 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Baratheon NaN NaN NaN win ambush 1.0 0.0 NaN 120.0 Gregor Clegane Beric Dondarrion 1.0 Mummer's Ford The Riverlands NaN 2 Battle of Riverrun 298 3 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 0.0 1.0 15000.0 10000.0 Jaime Lannister, Andros Brax Edmure Tully, Tytos Blackwood 1.0 Riverrun The Riverlands NaN 3 Battle of the Green Fork 298 4 Robb Stark Joffrey/Tommen Baratheon Stark NaN NaN NaN Lannister NaN NaN NaN loss pitched battle 1.0 1.0 18000.0 20000.0 Roose Bolton, Wylis Manderly, Medger Cerwyn, H... Tywin Lannister, Gregor Clegane, Kevan Lannist... 1.0 Green Fork The Riverlands NaN 4 Battle of the Whispering Wood 298 5 Robb Stark Joffrey/Tommen Baratheon Stark Tully NaN NaN Lannister NaN NaN NaN win ambush 1.0 1.0 1875.0 6000.0 Robb Stark, Brynden Tully Jaime Lannister 1.0 Whispering Wood The Riverlands NaN Make plot # Create a figure plt . figure ( figsize = ( 10 , 8 )) # Create a scatterplot of, # attacker size in year 298 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 298 ], # attacker size in year 298 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 298 ], # the marker as marker = 'x' , # the color color = 'b' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 298' ) # attacker size in year 299 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 299 ], # defender size in year 299 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 299 ], # the marker as marker = 'o' , # the color color = 'r' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 299' ) # attacker size in year 300 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 300 ], # defender size in year 300 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 300 ], # the marker as marker = '&#94;' , # the color color = 'g' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 300' ) # Chart title plt . title ( 'Battles Of The War Of The Five Kings' ) # y label plt . ylabel ( 'Defender Size' ) # x label plt . xlabel ( 'Attacker Size' ) # and a legend plt . legend ( loc = 'upper right' ) # set the figure boundaries plt . xlim ([ min ( df [ 'attacker_size' ]) - 1000 , max ( df [ 'attacker_size' ]) + 1000 ]) plt . ylim ([ min ( df [ 'defender_size' ]) - 1000 , max ( df [ 'defender_size' ]) + 1000 ]) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_simple_scatterplot.html","loc":"http://chrisalbon.com/python/matplotlib_simple_scatterplot.html"},{"title":"Scatterplot In MatPlotLib","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create the general blog and the \"subplots\" i.e. the bars f , ax1 = plt . subplots ( 1 , figsize = ( 10 , 5 )) # Set the bar width bar_width = 0.75 # positions of the left bar-boundaries bar_l = [ i + 1 for i in range ( len ( df [ 'pre_score' ]))] # positions of the x-axis ticks (center of the bars as bar labels) tick_pos = [ i + ( bar_width / 2 ) for i in bar_l ] # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the pre_score data df [ 'pre_score' ], # set the width width = bar_width , # with the label pre score label = 'Pre Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F4561D' ) # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the mid_score data df [ 'mid_score' ], # set the width width = bar_width , # with pre_score on the bottom bottom = df [ 'pre_score' ], # with the label mid score label = 'Mid Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F1911E' ) # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the post_score data df [ 'post_score' ], # set the width width = bar_width , # with pre_score and mid_score on the bottom bottom = [ i + j for i , j in zip ( df [ 'pre_score' ], df [ 'mid_score' ])], # with the label post score label = 'Post Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F1BD1A' ) # set the x ticks with names plt . xticks ( tick_pos , df [ 'first_name' ]) # Set the label and legends ax1 . set_ylabel ( \"Total Score\" ) ax1 . set_xlabel ( \"Test Subject\" ) plt . legend ( loc = 'upper left' ) # Set a buffer around the edge plt . xlim ([ min ( tick_pos ) - bar_width , max ( tick_pos ) + bar_width ]) (0.625, 6.125)","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_stacked_bar_plot.html","loc":"http://chrisalbon.com/python/matplotlib_stacked_bar_plot.html"},{"title":"Merge Tables","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Two Tables, Criminals And Crimes %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); -- Create a table of crimes CREATE TABLE crimes ( cid , crime , city , pid_arrested , cash_stolen ); INSERT INTO crimes VALUES ( 1 , 'fraud' , 'Santa Rosa' , 412 , 40000 ); INSERT INTO crimes VALUES ( 2 , 'burglary' , 'Petaluma' , 234 , 2000 ); INSERT INTO crimes VALUES ( 3 , 'burglary' , 'Santa Rosa' , 632 , 2000 ); INSERT INTO crimes VALUES ( 4 , NULL , NULL , 621 , 3500 ); INSERT INTO crimes VALUES ( 5 , 'burglary' , 'Santa Rosa' , 162 , 1000 ); INSERT INTO crimes VALUES ( 6 , NULL , 'Petaluma' , 901 , 50000 ); INSERT INTO crimes VALUES ( 7 , 'fraud' , 'San Francisco' , 412 , 60000 ); INSERT INTO crimes VALUES ( 8 , 'burglary' , 'Santa Rosa' , 512 , 7000 ); INSERT INTO crimes VALUES ( 9 , 'burglary' , 'San Francisco' , 411 , 3000 ); INSERT INTO crimes VALUES ( 10 , 'robbery' , 'Santa Rosa' , 632 , 2500 ); INSERT INTO crimes VALUES ( 11 , 'robbery' , 'Santa Rosa' , 512 , 3000 ); [] Inner Join Returns all rows whose merge-on id appears in both tables. %% sql -- Select everything SELECT * -- Left table FROM criminals -- Right table INNER JOIN crimes -- Merged on `pid` in the criminals table and `pid_arrested` in the crimes table ON criminals . pid = crimes . pid_arrested ; pid name age sex city minor cid crime city_1 pid_arrested cash_stolen 412 James Smith 15 M Santa Rosa 1 1 fraud Santa Rosa 412 40000 412 James Smith 15 M Santa Rosa 1 7 fraud San Francisco 412 60000 234 Bill James 22 M Santa Rosa 0 2 burglary Petaluma 234 2000 632 Stacy Miller 23 F Santa Rosa 0 3 burglary Santa Rosa 632 2000 632 Stacy Miller 23 F Santa Rosa 0 10 robbery Santa Rosa 632 2500 621 Betty Bob None F Petaluma 1 4 None None 621 3500 162 Jaden Ado 49 M None 0 5 burglary Santa Rosa 162 1000 901 Gordon Ado 32 F Santa Rosa 0 6 None Petaluma 901 50000 512 Bill Byson 21 M Santa Rosa 0 8 burglary Santa Rosa 512 7000 512 Bill Byson 21 M Santa Rosa 0 11 robbery Santa Rosa 512 3000 411 Bob Iton None M San Francisco 0 9 burglary San Francisco 411 3000 Left Join Returns all rows from the left table but only the rows from the right left that match the left table. %% sql -- Select everything SELECT * -- Left table FROM criminals -- Right table LEFT JOIN crimes -- Merged on `pid` in the criminals table and `pid_arrested` in the crimes table ON criminals . pid = crimes . pid_arrested ; pid name age sex city minor cid crime city_1 pid_arrested cash_stolen 412 James Smith 15 M Santa Rosa 1 1 fraud Santa Rosa 412 40000 412 James Smith 15 M Santa Rosa 1 7 fraud San Francisco 412 60000 234 Bill James 22 M Santa Rosa 0 2 burglary Petaluma 234 2000 632 Stacy Miller 23 F Santa Rosa 0 3 burglary Santa Rosa 632 2000 632 Stacy Miller 23 F Santa Rosa 0 10 robbery Santa Rosa 632 2500 621 Betty Bob None F Petaluma 1 4 None None 621 3500 162 Jaden Ado 49 M None 0 5 burglary Santa Rosa 162 1000 901 Gordon Ado 32 F Santa Rosa 0 6 None Petaluma 901 50000 512 Bill Byson 21 M Santa Rosa 0 8 burglary Santa Rosa 512 7000 512 Bill Byson 21 M Santa Rosa 0 11 robbery Santa Rosa 512 3000 411 Bob Iton None M San Francisco 0 9 burglary San Francisco 411 3000 Note: FULL OUTER and RIGHT JOIN are not shown here because they are not supported by the version of SQL (SQLite) used in this tutorial.","tags":"SQL","url":"http://chrisalbon.com/sql/merge_tables.html","loc":"http://chrisalbon.com/sql/merge_tables.html"},{"title":"Nesting Lists","text":"# Create a list of three nested lists, each with three items state_regions = [[ 'California' , 'Oregon' , 'Washington' ], [ 'Texas' , 'Georgia' , 'Alabama' ], [ 'Maine' , 'Vermont' , 'New York' ]] # View the list state_regions [['California', 'Oregon', 'Washington'], ['Texas', 'Georgia', 'Alabama'], ['Maine', 'Vermont', 'New York']] # Print the second list's third item state_regions [ 1 ][ 2 ] 'Alabama'","tags":"Python","url":"http://chrisalbon.com/python/nesting_lists.html","loc":"http://chrisalbon.com/python/nesting_lists.html"},{"title":"Basic Operations With Numpy Array","text":"# Import modules import numpy as np # Create an array civilian_deaths = np . array ([ 4352 , 233 , 3245 , 256 , 2394 ]) civilian_deaths array([4352, 233, 3245, 256, 2394]) # Mean value of the array civilian_deaths . mean () 2096.0 # Total amount of deaths civilian_deaths . sum () 10480 # Smallest value in the array civilian_deaths . min () 233 # Largest value in the array civilian_deaths . max () 4352","tags":"Python","url":"http://chrisalbon.com/python/numpy_array_basic_operations.html","loc":"http://chrisalbon.com/python/numpy_array_basic_operations.html"},{"title":"Numpy Array Basics","text":"# Import modules import numpy as np # Create a list battle_deaths = [ 3246 , 326 , 2754 , 2547 , 2457 , 3456 ] battle_deaths [3246, 326, 2754, 2547, 2457, 3456] # Create an array from numpy deaths = np . array ( battle_deaths ) deaths array([3246, 326, 2754, 2547, 2457, 3456]) # Create an array of zeros defectors = np . zeros ( 6 ) defectors array([ 0., 0., 0., 0., 0., 0.]) # Create a range from 0 to 100 zero_to_99 = np . arange ( 0 , 100 ) zero_to_99 array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]) # Create 100 ticks between 0 and 1 zero_to_1 = np . linspace ( 0 , 1 , 100 ) zero_to_1 array([ 0. , 0.01010101, 0.02020202, 0.03030303, 0.04040404, 0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909, 0.1010101 , 0.11111111, 0.12121212, 0.13131313, 0.14141414, 0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919, 0.2020202 , 0.21212121, 0.22222222, 0.23232323, 0.24242424, 0.25252525, 0.26262626, 0.27272727, 0.28282828, 0.29292929, 0.3030303 , 0.31313131, 0.32323232, 0.33333333, 0.34343434, 0.35353535, 0.36363636, 0.37373737, 0.38383838, 0.39393939, 0.4040404 , 0.41414141, 0.42424242, 0.43434343, 0.44444444, 0.45454545, 0.46464646, 0.47474747, 0.48484848, 0.49494949, 0.50505051, 0.51515152, 0.52525253, 0.53535354, 0.54545455, 0.55555556, 0.56565657, 0.57575758, 0.58585859, 0.5959596 , 0.60606061, 0.61616162, 0.62626263, 0.63636364, 0.64646465, 0.65656566, 0.66666667, 0.67676768, 0.68686869, 0.6969697 , 0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475, 0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.7979798 , 0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485, 0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.8989899 , 0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495, 0.95959596, 0.96969697, 0.97979798, 0.98989899, 1. ])","tags":"Python","url":"http://chrisalbon.com/python/numpy_array_basics.html","loc":"http://chrisalbon.com/python/numpy_array_basics.html"},{"title":"Indexing And Slicing Numpy Arrays","text":"# Import modules import numpy as np # Create a 2x2 array battle_deaths = [[ 344 , 2345 ], [ 253 , 4345 ]] deaths = np . array ( battle_deaths ) deaths array([[ 344, 2345], [ 253, 4345]]) # Select the top row, second item deaths [ 0 , 1 ] 2345 # Select the second column deaths [:, 1 ] array([2345, 4345]) # Select the second row deaths [ 1 , :] array([ 253, 4345]) # Create an array of civilian deaths civilian_deaths = np . array ([ 4352 , 233 , 3245 , 256 , 2394 ]) civilian_deaths array([4352, 233, 3245, 256, 2394]) # Find the index of battles with less than 500 deaths few_civ_deaths = np . where ( civilian_deaths < 500 ) few_civ_deaths (array([1, 3]),) # Find the number of civilian deaths in battles with less than 500 deaths civ_deaths = civilian_deaths [ few_civ_deaths ] civ_deaths array([233, 256])","tags":"Python","url":"http://chrisalbon.com/python/numpy_indexing_and_slicing.html","loc":"http://chrisalbon.com/python/numpy_indexing_and_slicing.html"},{"title":"Apply Functions By Group In Pandas","text":"Preliminaries # import pandas as pd import pandas as pd Create a simulated dataset # Create an example dataframe data = { 'Platoon' : [ 'A' , 'A' , 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' , 'B' , 'C' , 'C' , 'C' , 'C' , 'C' ], 'Casualties' : [ 1 , 4 , 5 , 7 , 5 , 5 , 6 , 1 , 4 , 5 , 6 , 7 , 4 , 6 , 4 , 6 ]} df = pd . DataFrame ( data ) df Casualties Platoon 0 1 A 1 4 A 2 5 A 3 7 A 4 5 A 5 5 A 6 6 B 7 1 B 8 4 B 9 5 B 10 6 B 11 7 C 12 4 C 13 6 C 14 4 C 15 6 C Apply A Function (Rolling Mean) To The DataFrame, By Group # Group df by df.platoon, then apply a rolling mean lambda function to df.casualties df . groupby ( 'Platoon' )[ 'Casualties' ] . apply ( lambda x : x . rolling ( center = False , window = 2 ) . mean ()) 0 NaN 1 2.5 2 4.5 3 6.0 4 6.0 5 5.0 6 NaN 7 3.5 8 2.5 9 4.5 10 5.5 11 NaN 12 5.5 13 5.0 14 5.0 15 5.0 dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_function_by_group.html","loc":"http://chrisalbon.com/python/pandas_apply_function_by_group.html"},{"title":"Applying Operations Over pandas Dataframes","text":"Import Modules import pandas as pd import numpy as np Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 Create a capitalization lambda function capitalizer = lambda x : x . upper () Apply the capitalizer function over the column 'name' apply() can apply a function along any axis of the dataframe df [ 'name' ] . apply ( capitalizer ) Cochice JASON Pima MOLLY Santa Cruz TINA Maricopa JAKE Yuma AMY Name: name, dtype: object Map the capitalizer lambda function over each element in the series 'name' map() applies an operation over each element of a series df [ 'name' ] . map ( capitalizer ) Cochice JASON Pima MOLLY Santa Cruz TINA Maricopa JAKE Yuma AMY Name: name, dtype: object Apply a square root function to every single cell in the whole data frame applymap() applies a function to every single element in the entire dataframe. # Drop the string variable so that applymap() can run df = df . drop ( 'name' , axis = 1 ) # Return the square root of every cell in the dataframe df . applymap ( np . sqrt ) coverage reports year Cochice 5.000000 2.000000 44.855323 Pima 9.695360 4.898979 44.855323 Santa Cruz 7.549834 5.567764 44.866469 Maricopa 7.874008 1.414214 44.877611 Yuma 8.366600 1.732051 44.877611 Applying A Function Over A Dataframe Create a function that multiplies all non-strings by 100 # create a function called times100 def times100 ( x ): # that, if x is a string, if type ( x ) is str : # just returns it untouched return x # but, if not, return it multiplied by 100 elif x : return 100 * x # and leave everything else else : return Apply the times100 over every cell in the dataframe df . applymap ( times100 ) coverage reports year Cochice 2500 400 201200 Pima 9400 2400 201200 Santa Cruz 5700 3100 201300 Maricopa 6200 200 201400 Yuma 7000 300 201400","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html","loc":"http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html"},{"title":"Apply Operations To Groups In Pandas","text":"Preliminaries # import modules import pandas as pd # Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Create a groupby variable that groups preTestScores by regiment groupby_regiment = df [ 'preTestScore' ] . groupby ( df [ 'regiment' ]) groupby_regiment <pandas.core.groupby.SeriesGroupBy object at 0x113ddb550> \"This grouped variable is now a GroupBy object. It has not actually computed anything yet except for some intermediate data about the group key df['key1']. The idea is that this object has all of the information needed to then apply some operation to each of the groups.\" - Python for Data Analysis View a grouping Use list() to show what a grouping looks like list ( df [ 'preTestScore' ] . groupby ( df [ 'regiment' ])) [('Dragoons', 4 3 5 4 6 24 7 31 Name: preTestScore, dtype: int64), ('Nighthawks', 0 4 1 24 2 31 3 2 Name: preTestScore, dtype: int64), ('Scouts', 8 2 9 3 10 2 11 3 Name: preTestScore, dtype: int64)] Descriptive statistics by group df [ 'preTestScore' ] . groupby ( df [ 'regiment' ]) . describe () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } count mean std min 25% 50% 75% max regiment Dragoons 4.0 15.50 14.153916 3.0 3.75 14.0 25.75 31.0 Nighthawks 4.0 15.25 14.453950 2.0 3.50 14.0 25.75 31.0 Scouts 4.0 2.50 0.577350 2.0 2.00 2.5 3.00 3.0 Mean of each regiment's preTestScore groupby_regiment . mean () regiment Dragoons 15.50 Nighthawks 15.25 Scouts 2.50 Name: preTestScore, dtype: float64 Mean preTestScores grouped by regiment and company df [ 'preTestScore' ] . groupby ([ df [ 'regiment' ], df [ 'company' ]]) . mean () regiment company Dragoons 1st 3.5 2nd 27.5 Nighthawks 1st 14.0 2nd 16.5 Scouts 1st 2.5 2nd 2.5 Name: preTestScore, dtype: float64 Mean preTestScores grouped by regiment and company without heirarchical indexing df [ 'preTestScore' ] . groupby ([ df [ 'regiment' ], df [ 'company' ]]) . mean () . unstack () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } company 1st 2nd regiment Dragoons 3.5 27.5 Nighthawks 14.0 16.5 Scouts 2.5 2.5 Group the entire dataframe by regiment and company df . groupby ([ 'regiment' , 'company' ]) . mean () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } preTestScore postTestScore regiment company Dragoons 1st 3.5 47.5 2nd 27.5 75.5 Nighthawks 1st 14.0 59.5 2nd 16.5 59.5 Scouts 1st 2.5 66.0 2nd 2.5 66.0 Number of observations in each regiment and company df . groupby ([ 'regiment' , 'company' ]) . size () regiment company Dragoons 1st 2 2nd 2 Nighthawks 1st 2 2nd 2 Scouts 1st 2 2nd 2 dtype: int64 Iterate an operations over groups # Group the dataframe by regiment, and for each regiment, for name , group in df . groupby ( 'regiment' ): # print the name of the regiment print ( name ) # print the data of that regiment print ( group ) Dragoons regiment company name preTestScore postTestScore 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 Nighthawks regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 Scouts regiment company name preTestScore postTestScore 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 Group by columns Specifically in this case: group by the data types of the columns (i.e. axis=1) and then use list() to view what that grouping looks like list ( df . groupby ( df . dtypes , axis = 1 )) [(dtype('int64'), preTestScore postTestScore 0 4 25 1 24 94 2 31 57 3 2 62 4 3 70 5 4 25 6 24 94 7 31 57 8 2 62 9 3 70 10 2 62 11 3 70), (dtype('O'), regiment company name 0 Nighthawks 1st Miller 1 Nighthawks 1st Jacobson 2 Nighthawks 2nd Ali 3 Nighthawks 2nd Milner 4 Dragoons 1st Cooze 5 Dragoons 1st Jacon 6 Dragoons 2nd Ryaner 7 Dragoons 2nd Sone 8 Scouts 1st Sloan 9 Scouts 1st Piger 10 Scouts 2nd Riani 11 Scouts 2nd Ali)] In the dataframe \"df\", group by \"regiments, take the mean values of the other variables for those groups, then display them with the prefix_mean df . groupby ( 'regiment' ) . mean () . add_prefix ( 'mean_' ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } mean_preTestScore mean_postTestScore regiment Dragoons 15.50 61.5 Nighthawks 15.25 59.5 Scouts 2.50 66.0 Create a function to get the stats of a group def get_stats ( group ): return { 'min' : group . min (), 'max' : group . max (), 'count' : group . count (), 'mean' : group . mean ()} Create bins and bin up postTestScore by those pins bins = [ 0 , 25 , 50 , 75 , 100 ] group_names = [ 'Low' , 'Okay' , 'Good' , 'Great' ] df [ 'categories' ] = pd . cut ( df [ 'postTestScore' ], bins , labels = group_names ) Apply the get_stats() function to each postTestScore bin df [ 'postTestScore' ] . groupby ( df [ 'categories' ]) . apply ( get_stats ) . unstack () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } count max mean min categories Good 8.0 70.0 63.75 57.0 Great 2.0 94.0 94.00 94.0 Low 2.0 25.0 25.00 25.0 Okay 0.0 NaN NaN NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_operations_to_groups.html","loc":"http://chrisalbon.com/python/pandas_apply_operations_to_groups.html"},{"title":"Convert A Categorical Variable Into Dummy Variables","text":"# import modules import pandas as pd # Create a dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'sex' : [ 'male' , 'female' , 'male' , 'female' , 'female' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'sex' ]) df first_name last_name sex 0 Jason Miller male 1 Molly Jacobson female 2 Tina Ali male 3 Jake Milner female 4 Amy Cooze female # Create a set of dummy variables from the sex variable df_sex = pd . get_dummies ( df [ 'sex' ]) # Join the dummy variables to the main dataframe df_new = pd . concat ([ df , df_sex ], axis = 1 ) df_new first_name last_name sex female male 0 Jason Miller male 0.0 1.0 1 Molly Jacobson female 1.0 0.0 2 Tina Ali male 0.0 1.0 3 Jake Milner female 1.0 0.0 4 Amy Cooze female 1.0 0.0 # Alterative for joining the new columns df_new = df . join ( df_sex ) df_new first_name last_name sex female male 0 Jason Miller male 0.0 1.0 1 Molly Jacobson female 1.0 0.0 2 Tina Ali male 0.0 1.0 3 Jake Milner female 1.0 0.0 4 Amy Cooze female 1.0 0.0","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html","loc":"http://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html"},{"title":"Convert A Numeric Categorical Variable With Patsy","text":"# import modules import pandas as pd import patsy # Create dataframe raw_data = { 'countrycode' : [ 1 , 2 , 3 , 2 , 1 ]} df = pd . DataFrame ( raw_data , columns = [ 'countrycode' ]) df countrycode 0 1 1 2 2 3 3 2 4 1 # Convert the countrycode variable into three binary variables patsy . dmatrix ( 'C(countrycode)-1' , df , return_type = 'dataframe' ) C(countrycode)[1] C(countrycode)[2] C(countrycode)[3] 0 1.0 0.0 0.0 1 0.0 1.0 0.0 2 0.0 0.0 1.0 3 0.0 1.0 0.0 4 1.0 0.0 0.0","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_numeric_categorical_to_numeric_with_patsy.html","loc":"http://chrisalbon.com/python/pandas_convert_numeric_categorical_to_numeric_with_patsy.html"},{"title":"Convert A String Categorical Variable With Patsy","text":"import modules import pandas as pd import patsy Create dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 0 , 0 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 strong 1 1 2 1 weak 2 1 3 0 normal 3 0 1 1 weak 4 0 2 0 strong Convert df['score'] into a categorical variable ready for regression (i.e. set one category as the baseline) # On the 'score' variable in the df dataframe, convert to a categorical variable, and spit out a dataframe patsy . dmatrix ( 'score' , df , return_type = 'dataframe' ) Intercept score[T.strong] score[T.weak] 0 1.0 1.0 0.0 1 1.0 0.0 1.0 2 1.0 0.0 0.0 3 1.0 0.0 1.0 4 1.0 1.0 0.0 Convert df['score'] into a categorical variable without setting one category as baseline This is likely what you will want to do # On the 'score' variable in the df dataframe, convert to a categorical variable, and spit out a dataframe patsy . dmatrix ( 'score - 1' , df , return_type = 'dataframe' ) score[normal] score[strong] score[weak] 0 0.0 1.0 0.0 1 0.0 0.0 1.0 2 1.0 0.0 0.0 3 0.0 0.0 1.0 4 0.0 1.0 0.0 Create a variable that is \"1\" if the variables of patient and treatment are both 1 patsy . dmatrix ( 'patient + treatment + patient:treatment-1' , df , return_type = 'dataframe' ) patient treatment patient:treatment 0 1.0 0.0 0.0 1 1.0 1.0 1.0 2 1.0 0.0 0.0 3 0.0 1.0 0.0 4 0.0 0.0 0.0","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_string_categorical_to_numeric_with_patsy.html","loc":"http://chrisalbon.com/python/pandas_convert_string_categorical_to_numeric_with_patsy.html"},{"title":"Convert A Variable To A Time Variable In Pandas","text":"# Import Preliminaries import pandas as pd # Create a dataset with the index being a set of names raw_data = { 'date' : [ '2014-06-01T01:21:38.004053' , '2014-06-02T01:21:38.004053' , '2014-06-03T01:21:38.004053' ], 'score' : [ 25 , 94 , 57 ]} df = pd . DataFrame ( raw_data , columns = [ 'date' , 'score' ]) df date score 0 2014-06-01T01:21:38.004053 25 1 2014-06-02T01:21:38.004053 94 2 2014-06-03T01:21:38.004053 57 # Transpose the dataset, so that the index (in this case the names) are columns df [ \"date\" ] = pd . to_datetime ( df [ \"date\" ]) df = df . set_index ( df [ \"date\" ]) df date score date 2014-06-01 01:21:38.004053 2014-06-01 01:21:38.004053 25 2014-06-02 01:21:38.004053 2014-06-02 01:21:38.004053 94 2014-06-03 01:21:38.004053 2014-06-03 01:21:38.004053 57","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_to_datetime.html","loc":"http://chrisalbon.com/python/pandas_convert_to_datetime.html"},{"title":"Create a Column Based on a Conditional in pandas","text":"Preliminaries # Import required modules import pandas as pd import numpy as np Make a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , columns = [ 'name' , 'age' , 'preTestScore' , 'postTestScore' ]) df name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 Add a new column for elderly # Create a new column called df.elderly where the value is yes # if df.age is greater than 50 and no if not df [ 'elderly' ] = np . where ( df [ 'age' ] >= 50 , 'yes' , 'no' ) # View the dataframe df name age preTestScore postTestScore elderly 0 Jason 42 4 25 no 1 Molly 52 24 94 yes 2 Tina 36 31 57 no 3 Jake 24 2 62 no 4 Amy 73 3 70 yes","tags":"Python","url":"http://chrisalbon.com/python/pandas_create_column_using_conditional.html","loc":"http://chrisalbon.com/python/pandas_create_column_using_conditional.html"},{"title":"Create A Pandas Column With A For Loop","text":"Preliminaries import pandas as pd import numpy as np Create an example dataframe raw_data = { 'student_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'test_score' : [ 76 , 88 , 84 , 67 , 53 , 96 , 64 , 91 , 77 , 73 , 52 , np . NaN ]} df = pd . DataFrame ( raw_data , columns = [ 'student_name' , 'test_score' ]) Create a function to assign letter grades # Create a list to store the data grades = [] # For each row in the column, for row in df [ 'test_score' ]: # if more than a value, if row > 95 : # Append a letter grade grades . append ( 'A' ) # else, if more than a value, elif row > 90 : # Append a letter grade grades . append ( 'A-' ) # else, if more than a value, elif row > 85 : # Append a letter grade grades . append ( 'B' ) # else, if more than a value, elif row > 80 : # Append a letter grade grades . append ( 'B-' ) # else, if more than a value, elif row > 75 : # Append a letter grade grades . append ( 'C' ) # else, if more than a value, elif row > 70 : # Append a letter grade grades . append ( 'C-' ) # else, if more than a value, elif row > 65 : # Append a letter grade grades . append ( 'D' ) # else, if more than a value, elif row > 60 : # Append a letter grade grades . append ( 'D-' ) # otherwise, else : # Append a failing grade grades . append ( 'Failed' ) # Create a column from the list df [ 'grades' ] = grades # View the new dataframe df student_name test_score grades 0 Miller 76.0 C 1 Jacobson 88.0 B 2 Ali 84.0 B- 3 Milner 67.0 D 4 Cooze 53.0 Failed 5 Jacon 96.0 A 6 Ryaner 64.0 D- 7 Sone 91.0 A- 8 Sloan 77.0 C 9 Piger 73.0 C- 10 Riani 52.0 Failed 11 Ali NaN Failed","tags":"Python","url":"http://chrisalbon.com/python/pandas_create_column_with_loop.html","loc":"http://chrisalbon.com/python/pandas_create_column_with_loop.html"},{"title":"Crosstabs In Pandas","text":"Import pandas import pandas as pd raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ 'infantry' , 'infantry' , 'cavalry' , 'cavalry' , 'infantry' , 'infantry' , 'cavalry' , 'cavalry' , 'infantry' , 'infantry' , 'cavalry' , 'cavalry' ], 'experience' : [ 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'experience' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company experience name preTestScore postTestScore 0 Nighthawks infantry veteran Miller 4 25 1 Nighthawks infantry rookie Jacobson 24 94 2 Nighthawks cavalry veteran Ali 31 57 3 Nighthawks cavalry rookie Milner 2 62 4 Dragoons infantry veteran Cooze 3 70 5 Dragoons infantry rookie Jacon 4 25 6 Dragoons cavalry veteran Ryaner 24 94 7 Dragoons cavalry rookie Sone 31 57 8 Scouts infantry veteran Sloan 2 62 9 Scouts infantry rookie Piger 3 70 10 Scouts cavalry veteran Riani 2 62 11 Scouts cavalry rookie Ali 3 70 Create a crosstab table by company and regiment Counting the number of observations by regiment and category pd . crosstab ( df . regiment , df . company , margins = True ) company cavalry infantry All regiment Dragoons 2 2 4 Nighthawks 2 2 4 Scouts 2 2 4 All 6 6 12 Create a crosstab of the number of rookie and veteran cavalry and infantry soldiers per regiment pd . crosstab ([ df . company , df . experience ], df . regiment , margins = True ) regiment Dragoons Nighthawks Scouts All company experience cavalry rookie 1 1 1 3 veteran 1 1 1 3 infantry rookie 1 1 1 3 veteran 1 1 1 3 All 4 4 4 12","tags":"Python","url":"http://chrisalbon.com/python/pandas_crosstabs.html","loc":"http://chrisalbon.com/python/pandas_crosstabs.html"},{"title":"pandas Data Structures","text":"Import modules import pandas as pd Series 101 Series are one-dimensional arrays (like R's vectors) Create a series of the number of floodingReports floodingReports = pd . Series ([ 5 , 6 , 2 , 9 , 12 ]) floodingReports 0 5 1 6 2 2 3 9 4 12 dtype: int64 Note that the first column of numbers (0 to 4) are the index. Set county names to be the index of the floodingReports series floodingReports = pd . Series ([ 5 , 6 , 2 , 9 , 12 ], index = [ 'Cochise County' , 'Pima County' , 'Santa Cruz County' , 'Maricopa County' , 'Yuma County' ]) floodingReports Cochise County 5 Pima County 6 Santa Cruz County 2 Maricopa County 9 Yuma County 12 dtype: int64 View the number of floodingReports in Cochise County floodingReports [ 'Cochise County' ] 5 View the counties with more than 6 flooding reports floodingReports [ floodingReports > 6 ] Maricopa County 9 Yuma County 12 dtype: int64 Create a pandas series from a dictionary Note: when you do this, the dict's key's will become the series's index # Create a dictionary fireReports_dict = { 'Cochise County' : 12 , 'Pima County' : 342 , 'Santa Cruz County' : 13 , 'Maricopa County' : 42 , 'Yuma County' : 52 } # Convert the dictionary into a pd.Series, and view it fireReports = pd . Series ( fireReports_dict ); fireReports Cochise County 12 Maricopa County 42 Pima County 342 Santa Cruz County 13 Yuma County 52 dtype: int64 Change the index of a series to shorter names fireReports . index = [ \"Cochice\" , \"Pima\" , \"Santa Cruz\" , \"Maricopa\" , \"Yuma\" ] fireReports Cochice 12 Pima 42 Santa Cruz 342 Maricopa 13 Yuma 52 dtype: int64 DataFrame 101 DataFrames are like R's Dataframes Create a dataframe from a dict of equal length lists or numpy arrays data = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data ) df county reports year 0 Cochice 4 2012 1 Pima 24 2012 2 Santa Cruz 31 2013 3 Maricopa 2 2014 4 Yuma 3 2014 Set the order of the columns using the columns attribute dfColumnOrdered = pd . DataFrame ( data , columns = [ 'county' , 'year' , 'reports' ]) dfColumnOrdered county year reports 0 Cochice 2012 4 1 Pima 2012 24 2 Santa Cruz 2013 31 3 Maricopa 2014 2 4 Yuma 2014 3 Add a column dfColumnOrdered [ 'newsCoverage' ] = pd . Series ([ 42.3 , 92.1 , 12.2 , 39.3 , 30.2 ]) dfColumnOrdered county year reports newsCoverage 0 Cochice 2012 4 42.3 1 Pima 2012 24 92.1 2 Santa Cruz 2013 31 12.2 3 Maricopa 2014 2 39.3 4 Yuma 2014 3 30.2 Delete a column del dfColumnOrdered [ 'newsCoverage' ] dfColumnOrdered county year reports 0 Cochice 2012 4 1 Pima 2012 24 2 Santa Cruz 2013 31 3 Maricopa 2014 2 4 Yuma 2014 3 Transpose the dataframe dfColumnOrdered . T 0 1 2 3 4 county Cochice Pima Santa Cruz Maricopa Yuma year 2012 2012 2013 2014 2014 reports 4 24 31 2 3","tags":"Python","url":"http://chrisalbon.com/python/pandas_data_structures.html","loc":"http://chrisalbon.com/python/pandas_data_structures.html"},{"title":"Count Values In Pandas Dataframe","text":"Import the pandas module import pandas as pd Create all the columns of the dataframe as series year = pd . Series ([ 1875 , 1876 , 1877 , 1878 , 1879 , 1880 , 1881 , 1882 , 1883 , 1884 , 1885 , 1886 , 1887 , 1888 , 1889 , 1890 , 1891 , 1892 , 1893 , 1894 ]) guardCorps = pd . Series ([ 0 , 2 , 2 , 1 , 0 , 0 , 1 , 1 , 0 , 3 , 0 , 2 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 ]) corps1 = pd . Series ([ 0 , 0 , 0 , 2 , 0 , 3 , 0 , 2 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 2 , 0 , 3 , 1 , 0 ]) corps2 = pd . Series ([ 0 , 0 , 0 , 2 , 0 , 2 , 0 , 0 , 1 , 1 , 0 , 0 , 2 , 1 , 1 , 0 , 0 , 2 , 0 , 0 ]) corps3 = pd . Series ([ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 0 , 2 , 0 , 0 , 0 , 1 , 0 , 1 , 2 , 1 , 0 , 0 , 0 ]) corps4 = pd . Series ([ 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 ]) corps5 = pd . Series ([ 0 , 0 , 0 , 0 , 2 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 ]) corps6 = pd . Series ([ 0 , 0 , 1 , 0 , 2 , 0 , 0 , 1 , 2 , 0 , 1 , 1 , 3 , 1 , 1 , 1 , 0 , 3 , 0 , 0 ]) corps7 = pd . Series ([ 1 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 2 , 0 , 0 , 2 , 1 , 0 , 2 , 0 ]) corps8 = pd . Series ([ 1 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ]) corps9 = pd . Series ([ 0 , 0 , 0 , 0 , 0 , 2 , 1 , 1 , 1 , 0 , 2 , 1 , 1 , 0 , 1 , 2 , 0 , 1 , 0 , 0 ]) corps10 = pd . Series ([ 0 , 0 , 1 , 1 , 0 , 1 , 0 , 2 , 0 , 2 , 0 , 0 , 0 , 0 , 2 , 1 , 3 , 0 , 1 , 1 ]) corps11 = pd . Series ([ 0 , 0 , 0 , 0 , 2 , 4 , 0 , 1 , 3 , 0 , 1 , 1 , 1 , 1 , 2 , 1 , 3 , 1 , 3 , 1 ]) corps14 = pd . Series ([ 1 , 1 , 2 , 1 , 1 , 3 , 0 , 4 , 0 , 1 , 0 , 3 , 2 , 1 , 0 , 2 , 1 , 1 , 0 , 0 ]) corps15 = pd . Series ([ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 0 , 0 , 0 , 0 ]) Create a dictionary variable that assigns variable names variables = dict ( guardCorps = guardCorps , corps1 = corps1 , corps2 = corps2 , corps3 = corps3 , corps4 = corps4 , corps5 = corps5 , corps6 = corps6 , corps7 = corps7 , corps8 = corps8 , corps9 = corps9 , corps10 = corps10 , corps11 = corps11 , corps14 = corps14 , corps15 = corps15 ) Create a dataframe and set the order of the columns using the columns attribute horsekick = pd . DataFrame ( variables , columns = [ 'guardCorps' , 'corps1' , 'corps2' , 'corps3' , 'corps4' , 'corps5' , 'corps6' , 'corps7' , 'corps8' , 'corps9' , 'corps10' , 'corps11' , 'corps14' , 'corps15' ]) Set the dataframe's index to be year horsekick . index = [ 1875 , 1876 , 1877 , 1878 , 1879 , 1880 , 1881 , 1882 , 1883 , 1884 , 1885 , 1886 , 1887 , 1888 , 1889 , 1890 , 1891 , 1892 , 1893 , 1894 ] View the horsekick dataframe horsekick guardCorps corps1 corps2 corps3 corps4 corps5 corps6 corps7 corps8 corps9 corps10 corps11 corps14 corps15 1875 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1876 2 0 0 0 1 0 0 0 0 0 0 0 1 1 1877 2 0 0 0 0 0 1 1 0 0 1 0 2 0 1878 1 2 2 1 1 0 0 0 0 0 1 0 1 0 1879 0 0 0 1 1 2 2 0 1 0 0 2 1 0 1880 0 3 2 1 1 1 0 0 0 2 1 4 3 0 1881 1 0 0 2 1 0 0 1 0 1 0 0 0 0 1882 1 2 0 0 0 0 1 0 1 1 2 1 4 1 1883 0 0 1 2 0 1 2 1 0 1 0 3 0 0 1884 3 0 1 0 0 0 0 1 0 0 2 0 1 1 1885 0 0 0 0 0 0 1 0 0 2 0 1 0 1 1886 2 1 0 0 1 1 1 0 0 1 0 1 3 0 1887 1 1 2 1 0 0 3 2 1 1 0 1 2 0 1888 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1889 0 0 1 1 0 1 1 0 0 1 2 2 0 2 1890 1 2 0 2 0 1 1 2 0 2 1 1 2 2 1891 0 0 0 1 1 1 0 1 1 0 3 3 1 0 1892 1 3 2 0 1 1 3 0 1 1 0 1 1 0 1893 0 1 0 0 0 1 0 2 0 0 1 3 0 0 1894 1 0 0 0 0 0 0 0 1 0 1 1 0 0 Count the number of times each number of deaths occurs in each regiment result = horsekick . apply ( pd . value_counts ) . fillna ( 0 ); result guardCorps corps1 corps2 corps3 corps4 corps5 corps6 corps7 corps8 corps9 corps10 corps11 corps14 corps15 0 9.0 11.0 12.0 11.0 12.0 10.0 9.0 11.0 13.0 10.0 10.0 6 6 14.0 1 7.0 4.0 4.0 6.0 8.0 9.0 7.0 6.0 7.0 7.0 6.0 8 8 4.0 2 3.0 3.0 4.0 3.0 0.0 1.0 2.0 3.0 0.0 3.0 3.0 2 3 2.0 3 1.0 2.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 1.0 3 2 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1 0.0 Count the number of times each monthly death total appears in guardCorps pd . value_counts ( horsekick [ 'guardCorps' ] . values , sort = False ) 0 9 1 7 2 3 3 1 dtype: int64 List all the unique values in guardCorps horsekick [ 'guardCorps' ] . unique () array([0, 2, 1, 3])","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_count_values.html","loc":"http://chrisalbon.com/python/pandas_dataframe_count_values.html"},{"title":"Descriptive Statistics For pandas Dataframe","text":"Import modules import pandas as pd Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , columns = [ 'name' , 'age' , 'preTestScore' , 'postTestScore' ]) df name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 5 rows × 4 columns The sum of all the ages df [ 'age' ] . sum () 227 Mean preTestScore df [ 'preTestScore' ] . mean () 12.800000000000001 Cumulative sum of preTestScores, moving from the rows from the top df [ 'preTestScore' ] . cumsum () 0 4 1 28 2 59 3 61 4 64 Name: preTestScore, dtype: int64 Summary statistics on preTestScore df [ 'preTestScore' ] . describe () count 5.000000 mean 12.800000 std 13.663821 min 2.000000 25% 3.000000 50% 4.000000 75% 24.000000 max 31.000000 Name: preTestScore, dtype: float64 Count the number of non-NA values df [ 'preTestScore' ] . count () 5 Minimum value of preTestScore df [ 'preTestScore' ] . min () 2 Maximum value of preTestScore df [ 'preTestScore' ] . max () 31 Median value of preTestScore df [ 'preTestScore' ] . median () 4.0 Sample variance of preTestScore values df [ 'preTestScore' ] . var () 186.69999999999999 Sample standard deviation of preTestScore values df [ 'preTestScore' ] . std () 13.663820841916802 Skewness of preTestScore values df [ 'preTestScore' ] . skew () 0.74334524573267591 Kurtosis of preTestScore values df [ 'preTestScore' ] . kurt () -2.4673543738411525 Correlation Matrix Of Values df . corr () age preTestScore postTestScore age 1.000000 -0.105651 0.328852 preTestScore -0.105651 1.000000 0.378039 postTestScore 0.328852 0.378039 1.000000 3 rows × 3 columns Covariance Matrix Of Values df . cov () age preTestScore postTestScore age 340.80 -26.65 151.20 preTestScore -26.65 186.70 128.65 postTestScore 151.20 128.65 620.30 3 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_descriptive_stats.html","loc":"http://chrisalbon.com/python/pandas_dataframe_descriptive_stats.html"},{"title":"Simple Example Dataframes In Pandas","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Create 2nd dataframe raw_data_2 = { 'first_name' : [ 'Sarah' , 'Gueniva' , 'Know' , 'Sara' , 'Cat' ], 'last_name' : [ 'Mornig' , 'Jaker' , 'Alom' , 'Ormon' , 'Koozer' ], 'age' : [ 53 , 26 , 72 , 73 , 24 ], 'preTestScore' : [ 13 , 52 , 72 , 26 , 26 ], 'postTestScore' : [ 82 , 52 , 56 , 234 , 254 ]} df_2 = pd . DataFrame ( raw_data_2 , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df_2 first_name last_name age preTestScore postTestScore 0 Sarah Mornig 53 13 82 1 Gueniva Jaker 26 52 52 2 Know Alom 72 72 56 3 Sara Ormon 73 26 234 4 Cat Koozer 24 26 254 Create 3rd dataframe raw_data_3 = { 'first_name' : [ 'Sarah' , 'Gueniva' , 'Know' , 'Sara' , 'Cat' ], 'last_name' : [ 'Mornig' , 'Jaker' , 'Alom' , 'Ormon' , 'Koozer' ], 'postTestScore_2' : [ 82 , 52 , 56 , 234 , 254 ]} df_3 = pd . DataFrame ( raw_data_3 , columns = [ 'first_name' , 'last_name' , 'postTestScore_2' ]) df_3 first_name last_name postTestScore_2 0 Sarah Mornig 82 1 Gueniva Jaker 52 2 Know Alom 56 3 Sara Ormon 234 4 Cat Koozer 254","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_examples.html","loc":"http://chrisalbon.com/python/pandas_dataframe_examples.html"},{"title":"Loading A CSV Into Pandas","text":"import modules import pandas as pd import numpy as np Create dataframe (that we will be importing) raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , \".\" , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , \".\" , \".\" ], 'postTestScore' : [ \"25,000\" , \"94,000\" , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25,000 1 Molly Jacobson 52 24 94,000 2 Tina . 36 31 57 3 Jake Milner 24 . 62 4 Amy Cooze 73 . 70 Save dataframe as csv in the working director df . to_csv ( 'pandas_dataframe_importing_csv/example.csv' ) Load a csv df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25,000 1 1 Molly Jacobson 52 24 94,000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70 Load a csv with no headers df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , header = None ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 0 1 2 3 4 5 0 NaN first_name last_name age preTestScore postTestScore 1 0.0 Jason Miller 42 4 25,000 2 1.0 Molly Jacobson 52 24 94,000 3 2.0 Tina . 36 31 57 4 3.0 Jake Milner 24 . 62 5 4.0 Amy Cooze 73 . 70 Load a csv while specifying column names df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } UID First Name Last Name Age Pre-Test Score Post-Test Score 0 NaN first_name last_name age preTestScore postTestScore 1 0.0 Jason Miller 42 4 25,000 2 1.0 Molly Jacobson 52 24 94,000 3 2.0 Tina . 36 31 57 4 3.0 Jake Milner 24 . 62 5 4.0 Amy Cooze 73 . 70 Load a csv with setting the index column to UID df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , index_col = 'UID' , names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } First Name Last Name Age Pre-Test Score Post-Test Score UID NaN first_name last_name age preTestScore postTestScore 0.0 Jason Miller 42 4 25,000 1.0 Molly Jacobson 52 24 94,000 2.0 Tina . 36 31 57 3.0 Jake Milner 24 . 62 4.0 Amy Cooze 73 . 70 Load a csv while setting the index columns to First Name and Last Name df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , index_col = [ 'First Name' , 'Last Name' ], names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } UID Age Pre-Test Score Post-Test Score First Name Last Name first_name last_name NaN age preTestScore postTestScore Jason Miller 0.0 42 4 25,000 Molly Jacobson 1.0 52 24 94,000 Tina . 2.0 36 31 57 Jake Milner 3.0 24 . 62 Amy Cooze 4.0 73 . 70 Load a csv while specifying \".\" as missing values df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , na_values = [ '.' ]) pd . isnull ( df ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Unnamed: 0 first_name last_name age preTestScore postTestScore 0 False False False False False False 1 False False False False False False 2 False False True False False False 3 False False False False True False 4 False False False False True False Load a csv while specifying \".\" and \"NA\" as missing values in the Last Name column and \".\" as missing values in Pre-Test Score column sentinels = { 'Last Name' : [ '.' , 'NA' ], 'Pre-Test Score' : [ '.' ]} df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , na_values = sentinels ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25,000 1 1 Molly Jacobson 52 24 94,000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70 Load a csv while skipping the top 3 rows df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , na_values = sentinels , skiprows = 3 ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } 2 Tina . 36 31 57 0 3 Jake Milner 24 . 62 1 4 Amy Cooze 73 . 70 Load a csv while interpreting \",\" in strings around numbers as thousands seperators df = pd . read_csv ( 'pandas_dataframe_importing_csv/example.csv' , thousands = ',' ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25000 1 1 Molly Jacobson 52 24 94000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_importing_csv.html","loc":"http://chrisalbon.com/python/pandas_dataframe_importing_csv.html"},{"title":"Load Excel Spreadsheet As Pandas Dataframe","text":"# import modules import pandas as pd # Import the excel file and call it xls_file xls_file = pd . ExcelFile ( '../data/example.xls' ) xls_file <pandas.io.excel.ExcelFile at 0x111912be0> # View the excel file's sheet names xls_file . sheet_names ['Sheet1'] # Load the xls file's Sheet1 as a dataframe df = xls_file . parse ( 'Sheet1' ) df year deaths_attacker deaths_defender soldiers_attacker soldiers_defender wounded_attacker wounded_defender 0 1945 425 423 2532 37235 41 14 1 1956 242 264 6346 2523 214 1424 2 1964 323 1231 3341 2133 131 131 3 1969 223 23 6732 1245 12 12 4 1971 783 23 12563 2671 123 34 5 1981 436 42 2356 7832 124 124 6 1982 324 124 253 2622 264 1124 7 1992 3321 631 5277 3331 311 1431 8 1999 262 232 2732 2522 132 122 9 2004 843 213 6278 26773 623 2563","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_load_xls.html","loc":"http://chrisalbon.com/python/pandas_dataframe_load_xls.html"},{"title":"Ranking Rows Of Pandas Dataframes","text":"# import modules import pandas as pd # Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 5 rows × 4 columns # Create a new column that is the rank of the value of coverage in ascending order df [ 'coverageRanked' ] = df [ 'coverage' ] . rank ( ascending = 1 ) df coverage name reports year coverageRanked Cochice 25 Jason 4 2012 1 Pima 94 Molly 24 2012 5 Santa Cruz 57 Tina 31 2013 2 Maricopa 62 Jake 2 2014 3 Yuma 70 Amy 3 2014 4 5 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_ranking_rows.html","loc":"http://chrisalbon.com/python/pandas_dataframe_ranking_rows.html"},{"title":"Reindexing pandas Series And Dataframes","text":"Series # Import Modules import pandas as pd import numpy as np # Create a pandas series of the risk of fire in Southern Arizona brushFireRisk = pd . Series ([ 34 , 23 , 12 , 23 ], index = [ 'Bisbee' , 'Douglas' , 'Sierra Vista' , 'Tombstone' ]) brushFireRisk Bisbee 34 Douglas 23 Sierra Vista 12 Tombstone 23 dtype: int64 # Reindex the series and create a new series variable brushFireRiskReindexed = brushFireRisk . reindex ([ 'Tombstone' , 'Douglas' , 'Bisbee' , 'Sierra Vista' , 'Barley' , 'Tucson' ]) brushFireRiskReindexed Tombstone 23.0 Douglas 23.0 Bisbee 34.0 Sierra Vista 12.0 Barley NaN Tucson NaN dtype: float64 # Reindex the series and fill in any missing indexes as 0 brushFireRiskReindexed = brushFireRisk . reindex ([ 'Tombstone' , 'Douglas' , 'Bisbee' , 'Sierra Vista' , 'Barley' , 'Tucson' ], fill_value = 0 ) brushFireRiskReindexed Tombstone 23 Douglas 23 Bisbee 34 Sierra Vista 12 Barley 0 Tucson 0 dtype: int64 DataFrames # Create a dataframe data = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data ) df county reports year 0 Cochice 4 2012 1 Pima 24 2012 2 Santa Cruz 31 2013 3 Maricopa 2 2014 4 Yuma 3 2014 # Change the order (the index) of the rows df . reindex ([ 4 , 3 , 2 , 1 , 0 ]) county reports year 4 Yuma 3 2014 3 Maricopa 2 2014 2 Santa Cruz 31 2013 1 Pima 24 2012 0 Cochice 4 2012 # Change the order (the index) of the columns columnsTitles = [ 'year' , 'reports' , 'county' ] df . reindex ( columns = columnsTitles ) year reports county 0 2012 4 Cochice 1 2012 24 Pima 2 2013 31 Santa Cruz 3 2014 2 Maricopa 4 2014 3 Yuma","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_reindexing.html","loc":"http://chrisalbon.com/python/pandas_dataframe_reindexing.html"},{"title":"Delete Duplicates In Pandas","text":"import modules import pandas as pd Create dataframe with duplicates raw_data = { 'first_name' : [ 'Jason' , 'Jason' , 'Jason' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Miller' , 'Miller' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 42 , 1111111 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 4 , 4 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 25 , 25 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Jason Miller 42 4 25 2 Jason Miller 1111111 4 25 3 Tina Ali 36 31 57 4 Jake Milner 24 2 62 5 Amy Cooze 73 3 70 Identify which observations are duplicates df . duplicated () 0 False 1 True 2 False 3 False 4 False 5 False dtype: bool Drop duplicates df . drop_duplicates () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 2 Jason Miller 1111111 4 25 3 Tina Ali 36 31 57 4 Jake Milner 24 2 62 5 Amy Cooze 73 3 70 Drop duplicates in the first name column, but take the last obs in the duplicated set df . drop_duplicates ([ 'first_name' ], keep = 'last' ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } first_name last_name age preTestScore postTestScore 2 Jason Miller 1111111 4 25 3 Tina Ali 36 31 57 4 Jake Milner 24 2 62 5 Amy Cooze 73 3 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_delete_duplicates.html","loc":"http://chrisalbon.com/python/pandas_delete_duplicates.html"},{"title":"Drop A Column That Contains A Certain String In Pandas","text":"import pandas as pd raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Create a variable that drop columns with column names where the first three letters of the column names was 'pre' cols = [ c for c in df . columns if c . lower ()[: 3 ] != 'pre' ] # Create a df of the columns in the variable cols df = df [ cols ] df regiment company name postTestScore 0 Nighthawks 1st Miller 25 1 Nighthawks 1st Jacobson 94 2 Nighthawks 2nd Ali 57 3 Nighthawks 2nd Milner 62 4 Dragoons 1st Cooze 70 5 Dragoons 1st Jacon 25 6 Dragoons 2nd Ryaner 94 7 Dragoons 2nd Sone 57 8 Scouts 1st Sloan 62 9 Scouts 1st Piger 70 10 Scouts 2nd Riani 62 11 Scouts 2nd Ali 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_drop_column_containing_certain_string.html","loc":"http://chrisalbon.com/python/pandas_drop_column_containing_certain_string.html"},{"title":"Dropping Rows And Columns In pandas Dataframe","text":"Import modules import pandas as pd Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Drop an observation (row) df . drop ([ 'Cochice' , 'Pima' ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Drop a variable (column) Note: axis=1 denotes that we are referring to a column, not a row df . drop ( 'reports' , axis = 1 ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name year Cochice Jason 2012 Pima Molly 2012 Santa Cruz Tina 2013 Maricopa Jake 2014 Yuma Amy 2014 Drop a row if it contains a certain value (in this case, \"Tina\") Specifically: Create a new dataframe called df that includes all rows where the value of a cell in the name column does not equal \"Tina\" df [ df . name != 'Tina' ] .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Maricopa Jake 2 2014 Yuma Amy 3 2014 Drop a row by row number (in this case, row 3) Note that Pandas uses zero based numbering, so 0 is the first row, 1 is the second row, etc. df . drop ( df . index [ 2 ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Maricopa Jake 2 2014 Yuma Amy 3 2014 can be extended to dropping a range df . drop ( df . index [[ 2 , 3 ]]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Yuma Amy 3 2014 or dropping relative to the end of the DF. df . drop ( df . index [ - 2 ]) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Yuma Amy 3 2014 you can select ranges relative to the top or drop relative to the bottom of the DF as well. df [: 3 ] #keep top 3 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 df [: - 3 ] #drop bottom 3 .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012","tags":"Python","url":"http://chrisalbon.com/python/pandas_dropping_column_and_rows.html","loc":"http://chrisalbon.com/python/pandas_dropping_column_and_rows.html"},{"title":"Expand Cells Containing Lists Into Their Own Variables In Pandas","text":"# import pandas import pandas as pd # create a dataset raw_data = { 'score' : [ 1 , 2 , 3 ], 'tags' : [[ 'apple' , 'pear' , 'guava' ],[ 'truck' , 'car' , 'plane' ],[ 'cat' , 'dog' , 'mouse' ]]} df = pd . DataFrame ( raw_data , columns = [ 'score' , 'tags' ]) # view the dataset df score tags 0 1 [apple, pear, guava] 1 2 [truck, car, plane] 2 3 [cat, dog, mouse] # expand df.tags into its own dataframe tags = df [ 'tags' ] . apply ( pd . Series ) # rename each variable is tags tags = tags . rename ( columns = lambda x : 'tag_' + str ( x )) # view the tags dataframe tags tag_0 tag_1 tag_2 0 apple pear guava 1 truck car plane 2 cat dog mouse # join the tags dataframe back to the original dataframe pd . concat ([ df [:], tags [:]], axis = 1 ) score tags tag_0 tag_1 tag_2 0 1 [apple, pear, guava] apple pear guava 1 2 [truck, car, plane] truck car plane 2 3 [cat, dog, mouse] cat dog mouse","tags":"Python","url":"http://chrisalbon.com/python/pandas_expand_cells_containing_lists.html","loc":"http://chrisalbon.com/python/pandas_expand_cells_containing_lists.html"},{"title":"Find Largest Value In A Dataframe Column","text":"# import modules % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np # Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 # Index of the row with the highest value in the preTestScore column df [ 'preTestScore' ] . idxmax () 2","tags":"Python","url":"http://chrisalbon.com/python/pandas_find_largest_value_in_column.html","loc":"http://chrisalbon.com/python/pandas_find_largest_value_in_column.html"},{"title":"Find Unique Values In Pandas Dataframes","text":"import pandas as pd import numpy as np raw_data = { 'regiment' : [ '51st' , '29th' , '2nd' , '19th' , '12th' , '101st' , '90th' , '30th' , '193th' , '1st' , '94th' , '91th' ], 'trucks' : [ 'MAZ-7310' , np . nan , 'MAZ-7310' , 'MAZ-7310' , 'Tatra 810' , 'Tatra 810' , 'Tatra 810' , 'Tatra 810' , 'ZIS-150' , 'Tatra 810' , 'ZIS-150' , 'ZIS-150' ], 'tanks' : [ 'Merkava Mark 4' , 'Merkava Mark 4' , 'Merkava Mark 4' , 'Leopard 2A6M' , 'Leopard 2A6M' , 'Leopard 2A6M' , 'Arjun MBT' , 'Leopard 2A6M' , 'Arjun MBT' , 'Arjun MBT' , 'Arjun MBT' , 'Arjun MBT' ], 'aircraft' : [ 'none' , 'none' , 'none' , 'Harbin Z-9' , 'Harbin Z-9' , 'none' , 'Harbin Z-9' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'trucks' , 'tanks' , 'aircraft' ]) # View the top few rows df . head () regiment trucks tanks aircraft 0 51st MAZ-7310 Merkava Mark 4 none 1 29th NaN Merkava Mark 4 none 2 2nd MAZ-7310 Merkava Mark 4 none 3 19th MAZ-7310 Leopard 2A6M Harbin Z-9 4 12th Tatra 810 Leopard 2A6M Harbin Z-9 # Create a list of unique values by turning the # pandas column into a set list ( set ( df . trucks )) [nan, 'Tatra 810', 'MAZ-7310', 'ZIS-150'] # Create a list of unique values in df.trucks list ( df [ 'trucks' ] . unique ()) ['MAZ-7310', nan, 'Tatra 810', 'ZIS-150']","tags":"Python","url":"http://chrisalbon.com/python/pandas_find_unique_values.html","loc":"http://chrisalbon.com/python/pandas_find_unique_values.html"},{"title":"Group A Time Series With pandas","text":"Import required modules import pandas as pd import numpy as np Create a dataframe df = pd . DataFrame () df [ 'german_army' ] = np . random . randint ( low = 20000 , high = 30000 , size = 100 ) df [ 'allied_army' ] = np . random . randint ( low = 20000 , high = 40000 , size = 100 ) df . index = pd . date_range ( '1/1/2014' , periods = 100 , freq = 'H' ) df . head () german_army allied_army 2014-01-01 00:00:00 28755 33938 2014-01-01 01:00:00 25176 28631 2014-01-01 02:00:00 23261 39685 2014-01-01 03:00:00 28686 27756 2014-01-01 04:00:00 24588 25681 Truncate the dataframe df . truncate ( before = '1/2/2014' , after = '1/3/2014' ) german_army allied_army 2014-01-02 00:00:00 26401 20189 2014-01-02 01:00:00 29958 23934 2014-01-02 02:00:00 24492 39075 2014-01-02 03:00:00 25707 39262 2014-01-02 04:00:00 27129 35961 2014-01-02 05:00:00 27903 25418 2014-01-02 06:00:00 20409 25163 2014-01-02 07:00:00 25736 34794 2014-01-02 08:00:00 24057 27209 2014-01-02 09:00:00 26875 33402 2014-01-02 10:00:00 23963 38575 2014-01-02 11:00:00 27506 31859 2014-01-02 12:00:00 23564 25750 2014-01-02 13:00:00 27958 24365 2014-01-02 14:00:00 24915 38866 2014-01-02 15:00:00 23538 33820 2014-01-02 16:00:00 23361 30080 2014-01-02 17:00:00 27284 22922 2014-01-02 18:00:00 24176 32155 2014-01-02 19:00:00 23924 27763 2014-01-02 20:00:00 23111 32343 2014-01-02 21:00:00 20348 28907 2014-01-02 22:00:00 27136 38634 2014-01-02 23:00:00 28649 29950 2014-01-03 00:00:00 21292 26395 Set the dataframe's index df . index = df . index + pd . DateOffset ( months = 4 , days = 5 ) View the dataframe df . head () german_army allied_army 2014-05-06 00:00:00 28755 33938 2014-05-06 01:00:00 25176 28631 2014-05-06 02:00:00 23261 39685 2014-05-06 03:00:00 28686 27756 2014-05-06 04:00:00 24588 25681 Lead a variable 1 hour df . shift ( 1 ) . head () german_army allied_army 2014-05-06 00:00:00 NaN NaN 2014-05-06 01:00:00 28755.0 33938.0 2014-05-06 02:00:00 25176.0 28631.0 2014-05-06 03:00:00 23261.0 39685.0 2014-05-06 04:00:00 28686.0 27756.0 Lag a variable 1 hour df . shift ( - 1 ) . tail () german_army allied_army 2014-05-09 23:00:00 26903.0 39144.0 2014-05-10 00:00:00 27576.0 39759.0 2014-05-10 01:00:00 25232.0 35246.0 2014-05-10 02:00:00 23391.0 21044.0 2014-05-10 03:00:00 NaN NaN Aggregate into days by summing up the value of each hourly observation df . resample ( 'D' ) . sum () german_army allied_army 2014-05-06 605161 755962 2014-05-07 608100 740396 2014-05-08 589744 700297 2014-05-09 607092 719283 2014-05-10 103102 135193 Aggregate into days by averaging up the value of each hourly observation df . resample ( 'D' ) . mean () german_army allied_army 2014-05-06 25215.041667 31498.416667 2014-05-07 25337.500000 30849.833333 2014-05-08 24572.666667 29179.041667 2014-05-09 25295.500000 29970.125000 2014-05-10 25775.500000 33798.250000 Aggregate into days by taking the min value up the value of each hourly observation df . resample ( 'D' ) . median () german_army allied_army 2014-05-06 24882.0 31310.0 2014-05-07 25311.0 30969.5 2014-05-08 24422.5 28318.0 2014-05-09 24941.5 32082.5 2014-05-10 26067.5 37195.0 Aggregate into days by taking the median value of each day's worth of hourly observation df . resample ( 'D' ) . median () german_army allied_army 2014-05-06 24882.0 31310.0 2014-05-07 25311.0 30969.5 2014-05-08 24422.5 28318.0 2014-05-09 24941.5 32082.5 2014-05-10 26067.5 37195.0 Aggregate into days by taking the first value of each day's worth of hourly observation df . resample ( 'D' ) . first () german_army allied_army 2014-05-06 28755 33938 2014-05-07 26401 20189 2014-05-08 21292 26395 2014-05-09 25764 22613 2014-05-10 26903 39144 Aggregate into days by taking the last value of each day's worth of hourly observation df . resample ( 'D' ) . last () german_army allied_army 2014-05-06 28214 32110 2014-05-07 28649 29950 2014-05-08 28379 32600 2014-05-09 26752 22379 2014-05-10 23391 21044 Aggregate into days by taking the first, last, highest, and lowest value of each day's worth of hourly observation df . resample ( 'D' ) . ohlc () german_army allied_army open high low close open high low close 2014-05-06 28755 29206 20037 28214 33938 39955 23417 32110 2014-05-07 26401 29958 20348 28649 20189 39262 20189 29950 2014-05-08 21292 29786 20296 28379 26395 38197 20404 32600 2014-05-09 25764 29952 20738 26752 22613 39695 20189 22379 2014-05-10 26903 27576 23391 23391 39144 39759 21044 21044","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_by_time.html","loc":"http://chrisalbon.com/python/pandas_group_by_time.html"},{"title":"Grouping Rows In Pandas","text":"# Import modules import pandas as pd # Example dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Create a grouping object. In other words, create an object that # represents that particular grouping. In this case we group # pre-test scores by the regiment. regiment_preScore = df [ 'preTestScore' ] . groupby ( df [ 'regiment' ]) # Display the mean value of the each regiment's pre-test score regiment_preScore . mean () regiment Dragoons 15.50 Nighthawks 15.25 Scouts 2.50 Name: preTestScore, dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_rows_by.html","loc":"http://chrisalbon.com/python/pandas_group_rows_by.html"},{"title":"Hierarchical Data In Pandas","text":"# import modules import pandas as pd # Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Set the hierarchical index but leave the columns inplace df . set_index ([ 'regiment' , 'company' ], drop = False ) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Set the hierarchical index to be by regiment, and then by company df = df . set_index ([ 'regiment' , 'company' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name preTestScore postTestScore regiment company Nighthawks 1st Miller 4 25 1st Jacobson 24 94 2nd Ali 31 57 2nd Milner 2 62 Dragoons 1st Cooze 3 70 1st Jacon 4 25 2nd Ryaner 24 94 2nd Sone 31 57 Scouts 1st Sloan 2 62 1st Piger 3 70 2nd Riani 2 62 2nd Ali 3 70 # View the index df . index MultiIndex(levels=[['Dragoons', 'Nighthawks', 'Scouts'], ['1st', '2nd']], labels=[[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2], [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]], names=['regiment', 'company']) # Swap the levels in the index df . swaplevel ( 'regiment' , 'company' ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name preTestScore postTestScore company regiment 1st Nighthawks Miller 4 25 Nighthawks Jacobson 24 94 2nd Nighthawks Ali 31 57 Nighthawks Milner 2 62 1st Dragoons Cooze 3 70 Dragoons Jacon 4 25 2nd Dragoons Ryaner 24 94 Dragoons Sone 31 57 1st Scouts Sloan 2 62 Scouts Piger 3 70 2nd Scouts Riani 2 62 Scouts Ali 3 70 # Summarize the results by regiment df . sum ( level = 'regiment' ) .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } preTestScore postTestScore regiment Dragoons 62 246 Nighthawks 61 238 Scouts 10 264","tags":"Python","url":"http://chrisalbon.com/python/pandas_hierarchical_data.html","loc":"http://chrisalbon.com/python/pandas_hierarchical_data.html"},{"title":"Importing an HTML table into pandas","text":"# Import the required module import pandas as pd # Create a variable with the url to the website contain the html table url = \"http://www.w3schools.com/html/html_tables.asp\" # Read the html table into pandas data = pd . read_html ( url , header = 0 ) --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-6-1bfd2d51c2cb> in <module>() 1 # Read the html table into pandas ----> 2 data = pd.read_html(url, header=0) /Users/chrisralbon/anaconda/lib/python3.5/site-packages/pandas/io/html.py in read_html(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, tupleize_cols, thousands, encoding) 868 _validate_header_arg(header) 869 return _parse(flavor, io, match, header, index_col, skiprows, --> 870 parse_dates, tupleize_cols, thousands, attrs, encoding) /Users/chrisralbon/anaconda/lib/python3.5/site-packages/pandas/io/html.py in _parse(flavor, io, match, header, index_col, skiprows, parse_dates, tupleize_cols, thousands, attrs, encoding) 720 retained = None 721 for flav in flavor: --> 722 parser = _parser_dispatch(flav) 723 p = parser(io, compiled_match, attrs, encoding) 724 /Users/chrisralbon/anaconda/lib/python3.5/site-packages/pandas/io/html.py in _parser_dispatch(flavor) 664 if flavor in ('bs4', 'html5lib'): 665 if not _HAS_HTML5LIB: --> 666 raise ImportError(\"html5lib not found, please install it\") 667 if not _HAS_BS4: 668 raise ImportError( ImportError: html5lib not found, please install it The resulting data will be a list of dataframe objects # View the data data --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-7-9a6a81bab7f3> in <module>() 1 # View the data ----> 2 data NameError: name 'data' is not defined # Select the first (and in this case only) dataframe object data [ 0 ]","tags":"Data Wrangling","url":"http://chrisalbon.com/data-wrangling/pandas_import_html_table.html","loc":"http://chrisalbon.com/data-wrangling/pandas_import_html_table.html"},{"title":"Index, Select, And Filter pandas Dataframes","text":"Import modules import pandas as pd Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 View a column of the dataframe df [ 'name' ] Cochice Jason Pima Molly Santa Cruz Tina Maricopa Jake Yuma Amy Name: name, dtype: object View two columns of the dataframe df [[ 'name' , 'reports' ]] name reports Cochice Jason 4 Pima Molly 24 Santa Cruz Tina 31 Maricopa Jake 2 Yuma Amy 3 View the first two rows of the dataframe df [: 2 ] coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 View all rows where coverage is more than 50 df [ df [ 'coverage' ] > 50 ] coverage name reports year Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 View a row df . ix [ 'Maricopa' ] coverage 62 name Jake reports 2 year 2014 Name: Maricopa, dtype: object View a column df . ix [:, 'coverage' ] Cochice 25 Pima 94 Santa Cruz 57 Maricopa 62 Yuma 70 Name: coverage, dtype: int64 View the value based on a row and column df . ix [ 'Yuma' , 'coverage' ] 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_index_select_and_filter.html","loc":"http://chrisalbon.com/python/pandas_index_select_and_filter.html"},{"title":"Join And Merge Pandas Dataframe","text":"import modules import pandas as pd from IPython.display import display from IPython.display import Image Create a dataframe raw_data = { 'subject_id' : [ '1' , '2' , '3' , '4' , '5' ], 'first_name' : [ 'Alex' , 'Amy' , 'Allen' , 'Alice' , 'Ayoung' ], 'last_name' : [ 'Anderson' , 'Ackerman' , 'Ali' , 'Aoni' , 'Atiches' ]} df_a = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'first_name' , 'last_name' ]) df_a subject_id first_name last_name 0 1 Alex Anderson 1 2 Amy Ackerman 2 3 Allen Ali 3 4 Alice Aoni 4 5 Ayoung Atiches Create a second dataframe raw_data = { 'subject_id' : [ '4' , '5' , '6' , '7' , '8' ], 'first_name' : [ 'Billy' , 'Brian' , 'Bran' , 'Bryce' , 'Betty' ], 'last_name' : [ 'Bonder' , 'Black' , 'Balwner' , 'Brice' , 'Btisan' ]} df_b = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'first_name' , 'last_name' ]) df_b subject_id first_name last_name 0 4 Billy Bonder 1 5 Brian Black 2 6 Bran Balwner 3 7 Bryce Brice 4 8 Betty Btisan Create a third dataframe raw_data = { 'subject_id' : [ '1' , '2' , '3' , '4' , '5' , '7' , '8' , '9' , '10' , '11' ], 'test_id' : [ 51 , 15 , 15 , 61 , 16 , 14 , 15 , 1 , 61 , 16 ]} df_n = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'test_id' ]) df_n subject_id test_id 0 1 51 1 2 15 2 3 15 3 4 61 4 5 16 5 7 14 6 8 15 7 9 1 8 10 61 9 11 16 Join the two dataframes along rows df_new = pd . concat ([ df_a , df_b ]) df_new subject_id first_name last_name 0 1 Alex Anderson 1 2 Amy Ackerman 2 3 Allen Ali 3 4 Alice Aoni 4 5 Ayoung Atiches 0 4 Billy Bonder 1 5 Brian Black 2 6 Bran Balwner 3 7 Bryce Brice 4 8 Betty Btisan Join the two dataframes along columns pd . concat ([ df_a , df_b ], axis = 1 ) subject_id first_name last_name subject_id first_name last_name 0 1 Alex Anderson 4 Billy Bonder 1 2 Amy Ackerman 5 Brian Black 2 3 Allen Ali 6 Bran Balwner 3 4 Alice Aoni 7 Bryce Brice 4 5 Ayoung Atiches 8 Betty Btisan Merge two dataframes along the subject_id value pd . merge ( df_new , df_n , on = 'subject_id' ) subject_id first_name last_name test_id 0 1 Alex Anderson 51 1 2 Amy Ackerman 15 2 3 Allen Ali 15 3 4 Alice Aoni 61 4 4 Billy Bonder 61 5 5 Ayoung Atiches 16 6 5 Brian Black 16 7 7 Bryce Brice 14 8 8 Betty Btisan 15 Merge two dataframes with both the left and right dataframes using the subject_id key pd . merge ( df_new , df_n , left_on = 'subject_id' , right_on = 'subject_id' ) subject_id first_name last_name test_id 0 1 Alex Anderson 51 1 2 Amy Ackerman 15 2 3 Allen Ali 15 3 4 Alice Aoni 61 4 4 Billy Bonder 61 5 5 Ayoung Atiches 16 6 5 Brian Black 16 7 7 Bryce Brice 14 8 8 Betty Btisan 15 Merge with outer join \"Full outer join produces the set of all records in Table A and Table B, with matching records from both sides where available. If there is no match, the missing side will contain null.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'outer' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black 5 6 NaN NaN Bran Balwner 6 7 NaN NaN Bryce Brice 7 8 NaN NaN Betty Btisan Merge with inner join \"Inner join produces only the set of records that match in both Table A and Table B.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'inner' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 4 Alice Aoni Billy Bonder 1 5 Ayoung Atiches Brian Black Merge with right join pd . merge ( df_a , df_b , on = 'subject_id' , how = 'right' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 4 Alice Aoni Billy Bonder 1 5 Ayoung Atiches Brian Black 2 6 NaN NaN Bran Balwner 3 7 NaN NaN Bryce Brice 4 8 NaN NaN Betty Btisan Merge with left join \"Left outer join produces a complete set of records from Table A, with the matching records (where available) in Table B. If there is no match, the right side will contain null.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'left' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black Merge while adding a suffix to duplicate column names pd . merge ( df_a , df_b , on = 'subject_id' , how = 'left' , suffixes = ( '_left' , '_right' )) subject_id first_name_left last_name_left first_name_right last_name_right 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black Merge based on indexes pd . merge ( df_a , df_b , right_index = True , left_index = True ) subject_id_x first_name_x last_name_x subject_id_y first_name_y last_name_y 0 1 Alex Anderson 4 Billy Bonder 1 2 Amy Ackerman 5 Brian Black 2 3 Allen Ali 6 Bran Balwner 3 4 Alice Aoni 7 Bryce Brice 4 5 Ayoung Atiches 8 Betty Btisan","tags":"Python","url":"http://chrisalbon.com/python/pandas_join_merge_dataframe.html","loc":"http://chrisalbon.com/python/pandas_join_merge_dataframe.html"},{"title":"Using List Comprehensions With Pandas","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 List Comprehensions As a loop # Create a variable next_year = [] # For each row in df.years, for row in df [ 'year' ]: # Add 1 to the row and append it to next_year next_year . append ( row + 1 ) # Create df.next_year df [ 'next_year' ] = next_year # View the dataframe df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year next_year Cochice Jason 4 2012 2013 Pima Molly 24 2012 2013 Santa Cruz Tina 31 2013 2014 Maricopa Jake 2 2014 2015 Yuma Amy 3 2014 2015 As list comprehension # Subtract 1 from row, for each row in df.year df [ 'previous_year' ] = [ row - 1 for row in df [ 'year' ]] df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } name reports year next_year previous_year Cochice Jason 4 2012 2013 2011 Pima Molly 24 2012 2013 2011 Santa Cruz Tina 31 2013 2014 2012 Maricopa Jake 2 2014 2015 2013 Yuma Amy 3 2014 2015 2013","tags":"Python","url":"http://chrisalbon.com/python/pandas_list_comprehension.html","loc":"http://chrisalbon.com/python/pandas_list_comprehension.html"},{"title":"List Unique Values In A Pandas Column","text":"Special thanks to Bob Haffner for pointing out a better way of doing it. Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 List unique values #List unique values in the df['name'] column df . name . unique () array(['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], dtype=object)","tags":"Python","url":"http://chrisalbon.com/python/pandas_list_unique_values_in_column.html","loc":"http://chrisalbon.com/python/pandas_list_unique_values_in_column.html"},{"title":"Pandas: Long To Wide Format","text":"import modules import pandas as pd Create \"long\" dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 6252 , 24243 , 2345 , 2342 , 23525 ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 6252 1 1 2 1 24243 2 1 3 0 2345 3 2 1 1 2342 4 2 2 0 23525 Make a \"wide\" data Now we will create a \"wide\" dataframe with the rows by patient number, the columns being by observation number, and the cell values being the score values. df . pivot ( index = 'patient' , columns = 'obs' , values = 'score' ) obs 1 2 3 patient 1 6252.0 24243.0 2345.0 2 2342.0 23525.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_long_to_wide.html","loc":"http://chrisalbon.com/python/pandas_long_to_wide.html"},{"title":"Lower Case Column Names In Pandas Dataframe","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'NAME' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'YEAR' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'REPORTS' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df NAME REPORTS YEAR Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Lowercase column values # Map the lowering function to all column names df . columns = map ( str . lower , df . columns ) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_lowercase_column_names.html","loc":"http://chrisalbon.com/python/pandas_lowercase_column_names.html"},{"title":"Make New Columns Using Functions","text":"# Import modules import pandas as pd # Example dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 Create one column as a function of two columns # Create a function that takes two inputs, pre and post def pre_post_difference ( pre , post ): # returns the difference between post and pre return post - pre # Create a variable that is the output of the function df [ 'score_change' ] = pre_post_difference ( df [ 'preTestScore' ], df [ 'postTestScore' ]) # View the dataframe df regiment company name preTestScore postTestScore score_change 0 Nighthawks 1st Miller 4 25 21 1 Nighthawks 1st Jacobson 24 94 70 2 Nighthawks 2nd Ali 31 57 26 3 Nighthawks 2nd Milner 2 62 60 4 Dragoons 1st Cooze 3 70 67 5 Dragoons 1st Jacon 4 25 21 6 Dragoons 2nd Ryaner 24 94 70 7 Dragoons 2nd Sone 31 57 26 8 Scouts 1st Sloan 2 62 60 9 Scouts 1st Piger 3 70 67 10 Scouts 2nd Riani 2 62 60 11 Scouts 2nd Ali 3 70 67 Create two columns as a function of one column # Create a function that takes one input, x def score_multipler_2x_and_3x ( x ): # returns two things, x multiplied by 2 and x multiplied by 3 return x * 2 , x * 3 # Create two new variables that take the two outputs of the function df [ 'post_score_x2' ], df [ 'post_score_x3' ] = zip ( * df [ 'postTestScore' ] . map ( score_multipler_2x_and_3x )) df regiment company name preTestScore postTestScore score_change post_score_x2 post_score_x3 0 Nighthawks 1st Miller 4 25 21 50 75 1 Nighthawks 1st Jacobson 24 94 70 188 282 2 Nighthawks 2nd Ali 31 57 26 114 171 3 Nighthawks 2nd Milner 2 62 60 124 186 4 Dragoons 1st Cooze 3 70 67 140 210 5 Dragoons 1st Jacon 4 25 21 50 75 6 Dragoons 2nd Ryaner 24 94 70 188 282 7 Dragoons 2nd Sone 31 57 26 114 171 8 Scouts 1st Sloan 2 62 60 124 186 9 Scouts 1st Piger 3 70 67 140 210 10 Scouts 2nd Riani 2 62 60 124 186 11 Scouts 2nd Ali 3 70 67 140 210","tags":"Python","url":"http://chrisalbon.com/python/pandas_make_new_columns_using_functions.html","loc":"http://chrisalbon.com/python/pandas_make_new_columns_using_functions.html"},{"title":"Map External Values To Dataframe Values in Pandas","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'city' : [ 'San Francisco' , 'Baltimore' , 'Miami' , 'Douglas' , 'Boston' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'city' ]) df first_name last_name age city 0 Jason Miller 42 San Francisco 1 Molly Jacobson 52 Baltimore 2 Tina Ali 36 Miami 3 Jake Milner 24 Douglas 4 Amy Cooze 73 Boston Create a dictionary of values city_to_state = { 'San Francisco' : 'California' , 'Baltimore' : 'Maryland' , 'Miami' : 'Florida' , 'Douglas' : 'Arizona' , 'Boston' : 'Massachusetts' } Map the values of the city_to_state dictionary to the values in the city variable df [ 'state' ] = df [ 'city' ] . map ( city_to_state ) df first_name last_name age city state 0 Jason Miller 42 San Francisco California 1 Molly Jacobson 52 Baltimore Maryland 2 Tina Ali 36 Miami Florida 3 Jake Milner 24 Douglas Arizona 4 Amy Cooze 73 Boston Massachusetts","tags":"Python","url":"http://chrisalbon.com/python/pandas_map_values_to_values.html","loc":"http://chrisalbon.com/python/pandas_map_values_to_values.html"},{"title":"Missing Data In Pandas Dataframes","text":"import modules import pandas as pd import numpy as np Create dataframe with missing values raw_data = { 'first_name' : [ 'Jason' , np . nan , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , np . nan , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , np . nan , 36 , 24 , 73 ], 'sex' : [ 'm' , np . nan , 'f' , 'm' , 'f' ], 'preTestScore' : [ 4 , np . nan , np . nan , 2 , 3 ], 'postTestScore' : [ 25 , np . nan , np . nan , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'sex' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 1 NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop missing observations df_no_missing = df . dropna () df_no_missing first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop rows where all cells in that row is NA df_cleaned = df . dropna ( how = 'all' ) df_cleaned first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Create a new column full of missing values df [ 'location' ] = np . nan df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Drop column if they only contain missing values df . dropna ( axis = 1 , how = 'all' ) first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 1 NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop rows that contain less than five observations This is really mostly useful for time series df . dropna ( thresh = 5 ) first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Fill in missing data with zeros df . fillna ( 0 ) first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 0.0 1 0 0 0.0 0 0.0 0.0 0.0 2 Tina Ali 36.0 f 0.0 0.0 0.0 3 Jake Milner 24.0 m 2.0 62.0 0.0 4 Amy Cooze 73.0 f 3.0 70.0 0.0 Fill in missing in preTestScore with the mean value of preTestScore inplace=True means that the changes are saved to the df right away df [ \"preTestScore\" ] . fillna ( df [ \"preTestScore\" ] . mean (), inplace = True ) df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN 3.0 NaN NaN 2 Tina Ali 36.0 f 3.0 NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Fill in missing in postTestScore with each sex's mean value of postTestScore df [ \"postTestScore\" ] . fillna ( df . groupby ( \"sex\" )[ \"postTestScore\" ] . transform ( \"mean\" ), inplace = True ) df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN 3.0 NaN NaN 2 Tina Ali 36.0 f 3.0 70.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Select some raws but ignore the missing data points # Select the rows of df where age is not NaN and sex is not NaN df [ df [ 'age' ] . notnull () & df [ 'sex' ] . notnull ()] first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 2 Tina Ali 36.0 f 3.0 70.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_missing_data.html","loc":"http://chrisalbon.com/python/pandas_missing_data.html"},{"title":"Moving Averages In Pandas","text":"Import Modules # Import pandas import pandas as pd Create Dataframe # Create data data = { 'score' : [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]} # Create dataframe df = pd . DataFrame ( data ) # View dataframe df .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } score 0 1 1 1 2 1 3 2 4 2 5 2 6 3 7 3 8 3 Calculate Rolling Mean # Calculate the moving average. That is, take # the first two values, average them, # then drop the first and add the third, etc. df . rolling ( window = 2 ) . mean () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } score 0 NaN 1 1.0 2 1.0 3 1.5 4 2.0 5 2.0 6 2.5 7 3.0 8 3.0","tags":"Python","url":"http://chrisalbon.com/python/pandas_moving_average.html","loc":"http://chrisalbon.com/python/pandas_moving_average.html"},{"title":"Normalize A Column In Pandas","text":"Based on: Sandman via StackOverflow . Preliminaries # Import required modules import pandas as pd from sklearn import preprocessing # Set charts to view inline % matplotlib inline Create Unnormalized Data # Create an example dataframe with a column of unnormalized data data = { 'score' : [ 234 , 24 , 14 , 27 , - 74 , 46 , 73 , - 18 , 59 , 160 ]} df = pd . DataFrame ( data ) df score 0 234 1 24 2 14 3 27 4 -74 5 46 6 73 7 -18 8 59 9 160 # View the unnormalized data df [ 'score' ] . plot ( kind = 'bar' ) <matplotlib.axes._subplots.AxesSubplot at 0x113a84eb8> Normalize The Column # Create x, where x the 'scores' column's values as floats x = df [ 'score' ] . values . astype ( float ) # Create a minimum and maximum processor object min_max_scaler = preprocessing . MinMaxScaler () # Create an object to transform the data to fit minmax processor x_scaled = min_max_scaler . fit_transform ( x ) # Run the normalizer on the dataframe df_normalized = pd . DataFrame ( x_scaled ) /Users/chrisralbon/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:324: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample. warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning) /Users/chrisralbon/anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py:359: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample. warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning) # View the dataframe df_normalized 0 0 1.000000 1 0.318182 2 0.285714 3 0.327922 4 0.000000 5 0.389610 6 0.477273 7 0.181818 8 0.431818 9 0.759740 # Plot the dataframe df_normalized . plot ( kind = 'bar' ) <matplotlib.axes._subplots.AxesSubplot at 0x115f11278>","tags":"Python","url":"http://chrisalbon.com/python/pandas_normalize_column.html","loc":"http://chrisalbon.com/python/pandas_normalize_column.html"},{"title":"Pivot Tables In Pandas","text":"import modules import pandas as pd Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'TestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'TestScore' ]) df regiment company TestScore 0 Nighthawks 1st 4 1 Nighthawks 1st 24 2 Nighthawks 2nd 31 3 Nighthawks 2nd 2 4 Dragoons 1st 3 5 Dragoons 1st 4 6 Dragoons 2nd 24 7 Dragoons 2nd 31 8 Scouts 1st 2 9 Scouts 1st 3 10 Scouts 2nd 2 11 Scouts 2nd 3 Create a pivot table of group means, by company and regiment pd . pivot_table ( df , index = [ 'regiment' , 'company' ], aggfunc = 'mean' ) TestScore regiment company Dragoons 1st 3.5 2nd 27.5 Nighthawks 1st 14.0 2nd 16.5 Scouts 1st 2.5 2nd 2.5 Create a pivot table of group score counts, by company and regimensts df . pivot_table ( index = [ 'regiment' , 'company' ], aggfunc = 'count' ) TestScore regiment company Dragoons 1st 2 2nd 2 Nighthawks 1st 2 2nd 2 Scouts 1st 2 2nd 2","tags":"Python","url":"http://chrisalbon.com/python/pandas_pivot_tables.html","loc":"http://chrisalbon.com/python/pandas_pivot_tables.html"},{"title":"Breaking Up A String Into Columns Using Regex In Pandas","text":"Based on this tutorial in nbviewer . Import modules import re import pandas as pd Create a dataframe of raw strings # Create a dataframe with a single column of strings data = { 'raw' : [ 'Arizona 1 2014-12-23 3242.0' , 'Iowa 1 2010-02-23 3453.7' , 'Oregon 0 2014-06-20 2123.0' , 'Maryland 0 2014-03-14 1123.6' , 'Florida 1 2013-01-15 2134.0' , 'Georgia 0 2012-07-14 2345.6' ]} df = pd . DataFrame ( data , columns = [ 'raw' ]) df raw 0 Arizona 1 2014-12-23 3242.0 1 Iowa 1 2010-02-23 3453.7 2 Oregon 0 2014-06-20 2123.0 3 Maryland 0 2014-03-14 1123.6 4 Florida 1 2013-01-15 2134.0 5 Georgia 0 2012-07-14 2345.6 Search a column of strings for a pattern # Which rows of df['raw'] contain 'xxxx-xx-xx'? df [ 'raw' ] . str . contains ( '....-..-..' , regex = True ) 0 True 1 True 2 True 3 True 4 True 5 True Name: raw, dtype: bool Extract the column of single digits # In the column 'raw', extract single digit in the strings df [ 'female' ] = df [ 'raw' ] . str . extract ( '(\\d)' , expand = True ) df [ 'female' ] 0 1 1 1 2 0 3 0 4 1 5 0 Name: female, dtype: object Extract the column of dates # In the column 'raw', extract xxxx-xx-xx in the strings df [ 'date' ] = df [ 'raw' ] . str . extract ( '(....-..-..)' , expand = True ) df [ 'date' ] 0 2014-12-23 1 2010-02-23 2 2014-06-20 3 2014-03-14 4 2013-01-15 5 2012-07-14 Name: date, dtype: object Extract the column of thousands # In the column 'raw', extract ####.## in the strings df [ 'score' ] = df [ 'raw' ] . str . extract ( '(\\d\\d\\d\\d\\.\\d)' , expand = True ) df [ 'score' ] 0 3242.0 1 3453.7 2 2123.0 3 1123.6 4 2134.0 5 2345.6 Name: score, dtype: object Extract the column of words # In the column 'raw', extract the word in the strings df [ 'state' ] = df [ 'raw' ] . str . extract ( '([A-Z]\\w{0,})' , expand = True ) df [ 'state' ] 0 Arizona 1 Iowa 2 Oregon 3 Maryland 4 Florida 5 Georgia Name: state, dtype: object View the final dataframe df raw female date score state 0 Arizona 1 2014-12-23 3242.0 1 2014-12-23 3242.0 Arizona 1 Iowa 1 2010-02-23 3453.7 1 2010-02-23 3453.7 Iowa 2 Oregon 0 2014-06-20 2123.0 0 2014-06-20 2123.0 Oregon 3 Maryland 0 2014-03-14 1123.6 0 2014-03-14 1123.6 Maryland 4 Florida 1 2013-01-15 2134.0 1 2013-01-15 2134.0 Florida 5 Georgia 0 2012-07-14 2345.6 0 2012-07-14 2345.6 Georgia","tags":"Python","url":"http://chrisalbon.com/python/pandas_regex_to_create_columns.html","loc":"http://chrisalbon.com/python/pandas_regex_to_create_columns.html"},{"title":"Rename Multiple Pandas Dataframe Column Names At Once","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'Commander' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'Date' : [ '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' ], 'Score' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df Commander Date Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3 Rename Column Names df . columns = [ 'Leader' , 'Time' , 'Score' ] df Leader Time Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3 df . rename ( columns = { 'Leader' : 'Commander' }, inplace = True ) df Commander Time Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3","tags":"Python","url":"http://chrisalbon.com/python/pandas_rename_multiple_columns.html","loc":"http://chrisalbon.com/python/pandas_rename_multiple_columns.html"},{"title":"Replacing Values In Pandas","text":"import modules import pandas as pd import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ - 999 , - 999 , - 999 , 2 , 1 ], 'postTestScore' : [ 2 , 2 , - 999 , 2 , - 999 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 -999 2 1 Molly Jacobson 52 -999 2 2 Tina Ali 36 -999 -999 3 Jake Milner 24 2 2 4 Amy Cooze 73 1 -999 Replace all values of -999 with NAN df . replace ( - 999 , np . nan ) first_name last_name age preTestScore postTestScore 0 Jason Miller 42 NaN 2.0 1 Molly Jacobson 52 NaN 2.0 2 Tina Ali 36 NaN NaN 3 Jake Milner 24 2.0 2.0 4 Amy Cooze 73 1.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_replace_values.html","loc":"http://chrisalbon.com/python/pandas_replace_values.html"},{"title":"Random Sampling Dataframe","text":"import modules import pandas as pd import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Select a random subset of 2 without replacement df . take ( np . random . permutation ( len ( df ))[: 2 ]) first_name last_name age preTestScore postTestScore 1 Molly Jacobson 52 24 94 4 Amy Cooze 73 3 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_sampling_dataframe.html","loc":"http://chrisalbon.com/python/pandas_sampling_dataframe.html"},{"title":"Saving A Pandas Dataframe As A CSV","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Safe the dataframe called \"df\" as csv Note: I've commented out this line of code so it does not run. Just remove the # to run. # df.to_csv('example.csv')","tags":"Python","url":"http://chrisalbon.com/python/pandas_saving_dataframe_as_csv.html","loc":"http://chrisalbon.com/python/pandas_saving_dataframe_as_csv.html"},{"title":"Search A Pandas Column For A Value","text":"# Import modules import pandas as pd raw_data = { 'first_name' : [ 'Jason' , 'Jason' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Miller' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 42 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 4 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 25 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Jason Miller 42 4 25 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Find where a value exists in a column # View preTestscore where postTestscore is greater than 50 df [ 'preTestScore' ] . where ( df [ 'postTestScore' ] > 50 ) 0 NaN 1 NaN 2 31.0 3 2.0 4 3.0 Name: preTestScore, dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_search_column_for_value.html","loc":"http://chrisalbon.com/python/pandas_search_column_for_value.html"},{"title":"Select Rows With A Certain Value","text":"import pandas as pd # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' ], 'country' : [[ 'Syria' , 'Lebanon' ],[ 'Spain' , 'Morocco' ]]} df = pd . DataFrame ( data ) df country name 0 [Syria, Lebanon] Jason 1 [Spain, Morocco] Molly df [ df [ 'country' ] . map ( lambda country : 'Syria' in country )] country name 0 [Syria, Lebanon] Jason","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_containing_values.html","loc":"http://chrisalbon.com/python/pandas_select_rows_containing_values.html"},{"title":"Select Rows With Multiple Filters","text":"# import pandas as pd import pandas as pd # Create an example dataframe data = { 'name' : [ 'A' , 'B' , 'C' , 'D' , 'E' ], 'score' : [ 1 , 2 , 3 , 4 , 5 ]} df = pd . DataFrame ( data ) df name score 0 A 1 1 B 2 2 C 3 3 D 4 4 E 5 # Select rows of the dataframe where df.score is greater than 1 and less and 5 df [( df [ 'score' ] > 1 ) & ( df [ 'score' ] < 5 )] name score 1 B 2 2 C 3 3 D 4","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_multiple_filters.html","loc":"http://chrisalbon.com/python/pandas_select_rows_multiple_filters.html"},{"title":"Select Rows When Columns Contain Certain Values","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Grab rows based on column values value_list = [ 'Tina' , 'Molly' , 'Jason' ] #Grab DataFrame rows where column has certain values df [ df . name . isin ( value_list )] name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 #Grab DataFrame rows where column doesn't have certain values df [ ~ df . name . isin ( value_list )] name reports year Maricopa Jake 2 2014 Yuma Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_when_column_has_certain_values.html","loc":"http://chrisalbon.com/python/pandas_select_rows_when_column_has_certain_values.html"},{"title":"Selecting Pandas DataFrame Rows Based On Conditions","text":"Preliminaries # Import modules import pandas as pd import numpy as np # Create a dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , np . nan , np . nan , np . nan ], 'nationality' : [ 'USA' , 'USA' , 'France' , 'UK' , 'UK' ], 'age' : [ 42 , 52 , 36 , 24 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'nationality' , 'age' ]) df first_name nationality age 0 Jason USA 42 1 Molly USA 52 2 NaN France 36 3 NaN UK 24 4 NaN UK 70 Method 1: Using Boolean Variables # Create variable with TRUE if nationality is USA american = df [ 'nationality' ] == \"USA\" # Create variable with TRUE if age is greater than 50 elderly = df [ 'age' ] > 50 # Select all casess where nationality is USA and age is greater than 50 df [ american & elderly ] first_name nationality age 1 Molly USA 52 Method 2: Using variable attributes # Select all cases where the first name is not missing and nationality is USA df [ df [ 'first_name' ] . notnull () & ( df [ 'nationality' ] == \"USA\" )] first_name nationality age 0 Jason USA 42 1 Molly USA 52","tags":"Python","url":"http://chrisalbon.com/python/pandas_selecting_rows_on_conditions.html","loc":"http://chrisalbon.com/python/pandas_selecting_rows_on_conditions.html"},{"title":"Sorting Rows In pandas Dataframes","text":"import modules import pandas as pd Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 1 , 2 , 1 , 2 , 3 ], 'coverage' : [ 2 , 2 , 3 , 3 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 2 Jason 1 2012 Pima 2 Molly 2 2012 Santa Cruz 3 Tina 1 2013 Maricopa 3 Jake 2 2014 Yuma 3 Amy 3 2014 Sort the dataframe's rows by reports, in descending order df . sort_values ( by = 'reports' , ascending = 0 ) coverage name reports year Yuma 3 Amy 3 2014 Pima 2 Molly 2 2012 Maricopa 3 Jake 2 2014 Cochice 2 Jason 1 2012 Santa Cruz 3 Tina 1 2013 Sort the dataframe's rows by coverage and then by reports, in ascending order df . sort_values ( by = [ 'coverage' , 'reports' ]) coverage name reports year Cochice 2 Jason 1 2012 Pima 2 Molly 2 2012 Santa Cruz 3 Tina 1 2013 Maricopa 3 Jake 2 2014 Yuma 3 Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_sorting_rows_dataframe.html","loc":"http://chrisalbon.com/python/pandas_sorting_rows_dataframe.html"},{"title":"Split Combined Lat/Long Coordinate Variables Into Seperate Variables In Pandas","text":"Preliminaries import pandas as pd import numpy as np Create an example dataframe raw_data = { 'geo' : [ '40.0024, -105.4102' , '40.0068, -105.266' , '39.9318, -105.2813' , np . nan ]} df = pd . DataFrame ( raw_data , columns = [ 'geo' ]) df geo 0 40.0024, -105.4102 1 40.0068, -105.266 2 39.9318, -105.2813 3 NaN Split the geo variable into seperate lat and lon variables # Create two lists for the loop results to be placed lat = [] lon = [] # For each row in a varible, for row in df [ 'geo' ]: # Try to, try : # Split the row by comma and append # everything before the comma to lat lat . append ( row . split ( ',' )[ 0 ]) # Split the row by comma and append # everything after the comma to lon lon . append ( row . split ( ',' )[ 1 ]) # But if you get an error except : # append a missing value to lat lat . append ( np . NaN ) # append a missing value to lon lon . append ( np . NaN ) # Create two new columns from lat and lon df [ 'latitude' ] = lat df [ 'longitude' ] = lon View the dataframe df geo latitude longitude 0 40.0024, -105.4102 40.0024 -105.4102 1 40.0068, -105.266 40.0068 -105.266 2 39.9318, -105.2813 39.9318 -105.2813 3 NaN NaN NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_split_lat_and_long_into_variables.html","loc":"http://chrisalbon.com/python/pandas_split_lat_and_long_into_variables.html"},{"title":"String Munging In Dataframe","text":"import modules import pandas as pd import numpy as np import re as re Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'email' : [ 'jas203@gmail.com' , 'momomolly@gmail.com' , np . NAN , 'battler@milner.com' , 'Ames1234@yahoo.com' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'email' , 'preTestScore' , 'postTestScore' ]) df first_name last_name email preTestScore postTestScore 0 Jason Miller jas203@gmail.com 4 25 1 Molly Jacobson momomolly@gmail.com 24 94 2 Tina Ali NaN 31 57 3 Jake Milner battler@milner.com 2 62 4 Amy Cooze Ames1234@yahoo.com 3 70 Which strings in the email column contains 'gmail' df [ 'email' ] . str . contains ( 'gmail' ) 0 True 1 True 2 NaN 3 False 4 False Name: email, dtype: object Create a regular expression pattern that breaks apart emails pattern = '([A-Z0-9._%+-]+)@([A-Z0-9.-]+) \\\\ .([A-Z]{2,4})' Find everything in df.email that contains that pattern df [ 'email' ] . str . findall ( pattern , flags = re . IGNORECASE ) 0 [(jas203, gmail, com)] 1 [(momomolly, gmail, com)] 2 NaN 3 [(battler, milner, com)] 4 [(Ames1234, yahoo, com)] Name: email, dtype: object Create a pandas series containing the email elements matches = df [ 'email' ] . str . match ( pattern , flags = re . IGNORECASE ) matches /Users/chrisralbon/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:1: FutureWarning: In future versions of pandas, match will change to always return a bool indexer. if __name__ == '__main__': 0 (jas203, gmail, com) 1 (momomolly, gmail, com) 2 NaN 3 (battler, milner, com) 4 (Ames1234, yahoo, com) Name: email, dtype: object Select the domains of the df.email matches . str [ 1 ] 0 gmail 1 gmail 2 NaN 3 milner 4 yahoo Name: email, dtype: object","tags":"Python","url":"http://chrisalbon.com/python/pandas_string_munging.html","loc":"http://chrisalbon.com/python/pandas_string_munging.html"},{"title":"Pandas Time Series Basics","text":"Import modules from datetime import datetime import pandas as pd % matplotlib inline import matplotlib.pyplot as pyplot Create a dataframe data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'battle_deaths' : [ 34 , 25 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' ]) print ( df ) date battle_deaths 0 2014-05-01 18:47:05.069722 34 1 2014-05-01 18:47:05.119994 25 2 2014-05-02 18:47:05.178768 26 3 2014-05-02 18:47:05.230071 15 4 2014-05-02 18:47:05.230071 15 5 2014-05-02 18:47:05.280592 14 6 2014-05-03 18:47:05.332662 26 7 2014-05-03 18:47:05.385109 25 8 2014-05-04 18:47:05.436523 62 9 2014-05-04 18:47:05.486877 41 Convert df['date'] from string to datetime df [ 'date' ] = pd . to_datetime ( df [ 'date' ]) Set df['date'] as the index and delete the column df . index = df [ 'date' ] del df [ 'date' ] df battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 View all observations that occured in 2014 df [ '2014' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 View all observations that occured in May 2014 df [ '2014-05' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 Observations after May 3rd, 2014 df [ datetime ( 2014 , 5 , 3 ):] battle_deaths date 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 Observations between May 3rd and May 4th df [ '5/3/2014' : '5/4/2014' ] battle_deaths date 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 Truncation observations after May 2nd 2014 df . truncate ( after = '5/3/2014' ) battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 Observations of May 2014 df . ix [ '5-2014' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 Count the number of observations per timestamp df . groupby ( level = 0 ) . count () battle_deaths date 2014-05-01 18:47:05.069722 1 2014-05-01 18:47:05.119994 1 2014-05-02 18:47:05.178768 1 2014-05-02 18:47:05.230071 2 2014-05-02 18:47:05.280592 1 2014-05-03 18:47:05.332662 1 2014-05-03 18:47:05.385109 1 2014-05-04 18:47:05.436523 1 2014-05-04 18:47:05.486877 1 Mean value of battle_deaths per day df . resample ( 'D' ) . mean () battle_deaths date 2014-05-01 29.5 2014-05-02 17.5 2014-05-03 25.5 2014-05-04 51.5 Total value of battle_deaths per day df . resample ( 'D' ) . sum () battle_deaths date 2014-05-01 59 2014-05-02 70 2014-05-03 51 2014-05-04 103 Plot of the total battle deaths per day df . resample ( 'D' ) . sum () . plot () <matplotlib.axes._subplots.AxesSubplot at 0x1148a9860>","tags":"Python","url":"http://chrisalbon.com/python/pandas_time_series_basics.html","loc":"http://chrisalbon.com/python/pandas_time_series_basics.html"},{"title":"Using Seaborn To Visualize A Pandas Dataframe","text":"Preliminaries import pandas as pd % matplotlib inline import random import matplotlib.pyplot as plt import seaborn as sns df = pd . DataFrame () df [ 'x' ] = random . sample ( range ( 1 , 100 ), 25 ) df [ 'y' ] = random . sample ( range ( 1 , 100 ), 25 ) df . head () x y 0 18 25 1 42 67 2 52 77 3 4 34 4 14 69 Scatterplot sns . lmplot ( 'x' , 'y' , data = df , fit_reg = False ) <seaborn.axisgrid.FacetGrid at 0x114563b00> Density Plot sns . kdeplot ( df . y ) <matplotlib.axes._subplots.AxesSubplot at 0x113ea2ef0> sns . kdeplot ( df . y , df . x ) <matplotlib.axes._subplots.AxesSubplot at 0x113d7fef0> sns . distplot ( df . x ) <matplotlib.axes._subplots.AxesSubplot at 0x114294160> Histogram plt . hist ( df . x , alpha =. 3 ) sns . rugplot ( df . x ); Boxplot sns . boxplot ([ df . y , df . x ]) <matplotlib.axes._subplots.AxesSubplot at 0x1142b8b38> Violin Plot sns . violinplot ([ df . y , df . x ]) <matplotlib.axes._subplots.AxesSubplot at 0x114444a58> Heatmap sns . heatmap ([ df . y , df . x ], annot = True , fmt = \"d\" ) <matplotlib.axes._subplots.AxesSubplot at 0x114530c88> Clustermap sns . clustermap ( df ) <seaborn.matrix.ClusterGrid at 0x116f313c8>","tags":"Python","url":"http://chrisalbon.com/python/pandas_with_seaborn.html","loc":"http://chrisalbon.com/python/pandas_with_seaborn.html"},{"title":"Recursive Functions","text":"Simple factorial print ( 5 * 4 * 3 * 2 * 1 ) 120 Recursive function The tell-tale sign of a recursive function is a function that calls itself # Create a function inputing n, that, def factorial ( n ): # if n is less than or equal to 1, if n <= 1 : # return n, return n # if not, return n multiplied by the output # of the factorial function of one less than n return n * factorial ( n - 1 ) # run the function factorial ( 5 ) 120","tags":"Python","url":"http://chrisalbon.com/python/recursive_functions.html","loc":"http://chrisalbon.com/python/recursive_functions.html"},{"title":"Regular Expression By Example","text":"This tutorial is based on: http://www.tutorialspoint.com/python/python_reg_expressions.htm # Import regex import re # Create some data text = 'A flock of 120 quick brown foxes jumped over 30 lazy brown, bears.' &#94; Matches beginning of line. re . findall ( '&#94;A' , text ) ['A'] $ Matches end of line. re . findall ( 'bears.$' , text ) ['bears.'] . Matches any single character except newline. re . findall ( 'f..es' , text ) ['foxes'] [...] Matches any single character in brackets. # Find all vowels re . findall ( '[aeiou]' , text ) ['o', 'o', 'u', 'i', 'o', 'o', 'e', 'u', 'e', 'o', 'e', 'a', 'o', 'e', 'a'] [# &#94;...] Matches any single character not in brackets # Find all characters that are not lower-case vowels re . findall ( '[&#94;aeiou]' , text ) ['A', ' ', 'f', 'l', 'c', 'k', ' ', 'f', ' ', '1', '2', '0', ' ', 'q', 'c', 'k', ' ', 'b', 'r', 'w', 'n', ' ', 'f', 'x', 's', ' ', 'j', 'm', 'p', 'd', ' ', 'v', 'r', ' ', '3', '0', ' ', 'l', 'z', 'y', ' ', 'b', 'r', 'w', 'n', ',', ' ', 'b', 'r', 's', '.'] a | b Matches either a or b. re . findall ( 'a|A' , text ) ['A', 'a', 'a'] (re) Groups regular expressions and remembers matched text. # Find any instance of 'fox' re . findall ( '(foxes)' , text ) ['foxes'] \\w Matches word characters. # Break up string into five character blocks re . findall ( '\\w\\w\\w\\w\\w' , text ) ['flock', 'quick', 'brown', 'foxes', 'jumpe', 'brown', 'bears'] \\W Matches nonword characters. re . findall ( '\\W\\W' , text ) [', '] \\s Matches whitespace. Equivalent to [\\t\\n\\r\\f]. re . findall ( '\\s' , text ) [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \\S Matches nonwhitespace. re . findall ( '\\S\\S' , text ) ['fl', 'oc', 'of', '12', 'qu', 'ic', 'br', 'ow', 'fo', 'xe', 'ju', 'mp', 'ed', 'ov', 'er', '30', 'la', 'zy', 'br', 'ow', 'n,', 'be', 'ar', 's.'] \\d Matches digits. Equivalent to [0-9]. re . findall ( '\\d\\d\\d' , text ) ['120'] \\D Matches nondigits. re . findall ( '\\D\\D\\D\\D\\D' , text ) ['A flo', 'ck of', ' quic', 'k bro', 'wn fo', 'xes j', 'umped', ' over', ' lazy', ' brow', 'n, be'] \\A Matches beginning of string. re . findall ( '\\AA' , text ) ['A'] \\Z Matches end of string. If a newline exists, it matches just before newline. re . findall ( 'bears.\\Z' , text ) ['bears.'] \\b Matches end of string. re . findall ( ' \\b [foxes]' , text ) [] \\n, \\t, etc. Matches newlines, carriage returns, tabs, etc. re . findall ( ' \\n ' , text ) [] [Pp]ython Match \"Python\" or \"python\" re . findall ( '[Ff]oxes' , 'foxes Foxes Doxes' ) ['foxes', 'Foxes'] [0-9] Match any digit; same as [0123456789] re . findall ( '[Ff]oxes' , 'foxes Foxes Doxes' ) ['foxes', 'Foxes'] [a-z] Match any lowercase ASCII letter re . findall ( '[a-z]' , 'foxes Foxes' ) ['f', 'o', 'x', 'e', 's', 'o', 'x', 'e', 's'] [A-Z] Match any uppercase ASCII letter re . findall ( '[A-Z]' , 'foxes Foxes' ) ['F'] [a-zA-Z0-9] Match any of the above re . findall ( '[a-zA-Z0-9]' , 'foxes Foxes' ) ['f', 'o', 'x', 'e', 's', 'F', 'o', 'x', 'e', 's'] [&#94;aeiou] Match anything other than a lowercase vowel re . findall ( '[&#94;aeiou]' , 'foxes Foxes' ) ['f', 'x', 's', ' ', 'F', 'x', 's'] [&#94;0-9] Match anything other than a digit re . findall ( '[&#94;0-9]' , 'foxes Foxes' ) ['f', 'o', 'x', 'e', 's', ' ', 'F', 'o', 'x', 'e', 's'] ruby? Match \"rub\" or \"ruby\": the y is optional re . findall ( 'foxes?' , 'foxes Foxes' ) ['foxes'] ruby* Match \"rub\" plus 0 or more ys re . findall ( 'ox*' , 'foxes Foxes' ) ['ox', 'ox'] ruby+ Match \"rub\" plus 1 or more ys re . findall ( 'ox+' , 'foxes Foxes' ) ['ox', 'ox'] \\d{3} Match exactly 3 digits re . findall ( '\\d{3}' , text ) ['120'] \\d{3,} Match 3 or more digits re . findall ( '\\d{2,}' , text ) ['120', '30'] \\d{3,5} Match 3, 4, or 5 digits re . findall ( '\\d{2,3}' , text ) ['120', '30'] &#94;Python Match \"Python\" at the start of a string or internal line re . findall ( '&#94;A' , text ) ['A'] Python$ Match \"Python\" at the end of a string or line re . findall ( 'bears.$' , text ) ['bears.'] \\APython Match \"Python\" at the start of a string re . findall ( '\\AA' , text ) ['A'] Python\\Z Match \"Python\" at the end of a string re . findall ( 'bears.\\Z' , text ) ['bears.'] Python(?=!) Match \"Python\", if followed by an exclamation point re . findall ( 'bears(?=.)' , text ) ['bears'] Python(?!!) Match \"Python\", if not followed by an exclamation point re . findall ( 'foxes(?!!)' , 'foxes foxes!' ) ['foxes'] python|perl Match \"python\" or \"perl\" re . findall ( 'foxes|foxes!' , 'foxes foxes!' ) ['foxes', 'foxes'] rub(y|le)) Match \"ruby\" or \"ruble\" re . findall ( 'fox(es!)' , 'foxes foxes!' ) ['es!'] Python(!+|\\?) \"Python\" followed by one or more ! or one ? re . findall ( 'foxes(!)' , 'foxes foxes!' ) ['!']","tags":"Python","url":"http://chrisalbon.com/python/regex_by_example.html","loc":"http://chrisalbon.com/python/regex_by_example.html"},{"title":"Regular Expression Basics","text":"Import the regex (re) package import re Import sys import sys Create a simple text string. text = 'The quick brown fox jumped over the lazy black bear.' Create a pattern to match three_letter_word = '\\w{3}' Convert the string into a regex object pattern_re = re . compile ( three_letter_word ); pattern_re re.compile(r'\\w{3}', re.UNICODE) Does a three letter word appear in text? re_search = re . search ( '..own' , text ) If the search query is at all true, if re_search : # Print the search results print ( re_search . group ()) brown re.match re.match() is for matching ONLY the beginning of a string or the whole string For anything else, use re.search Match all three letter words in text re_match = re . match ( '..own' , text ) If re_match is true, print the match, else print \"No Matches\" if re_match : # Print all the matches print ( re_match . group ()) else : # Print this print ( 'No matches' ) No matches re.split Split up the string using \"e\" as the seperator. re_split = re . split ( 'e' , text ); re_split ['Th', ' quick brown fox jump', 'd ov', 'r th', ' lazy black b', 'ar.'] re.sub Replaces occurrences of the regex pattern with something else The \"3\" references to the maximum number of substitutions to make. Substitute the first three instances of \"e\" with \"E\", then print it re_sub = re . sub ( 'e' , 'E' , text , 3 ); print ( re_sub ) ThE quick brown fox jumpEd ovEr the lazy black bear.","tags":"Python","url":"http://chrisalbon.com/python/regular_expressions_basics.html","loc":"http://chrisalbon.com/python/regular_expressions_basics.html"},{"title":"Scheduling Jobs In The Future","text":"# Import required modules import sched import time # setup the scheduler with our time settings s = sched . scheduler ( time . time , time . sleep ) # Create a function we want to run in the future. def print_time (): print ( \"Executive Order 66\" ) # Create a function for the delay def print_some_times (): # Create a scheduled job that will run # the function called 'print_time' # after 10 seconds, and with priority 1. s . enter ( 10 , 1 , print_time ) # Run the scheduler s . run () # Run the function for the delay print_some_times () Executive Order 66","tags":"Python","url":"http://chrisalbon.com/python/schedule_run_in_the_future.html","loc":"http://chrisalbon.com/python/schedule_run_in_the_future.html"},{"title":"Simple Clustering With SciPy","text":"Import modules import numpy as np % matplotlib inline import matplotlib.pyplot as plt from scipy.cluster import vq Create coordinates for battles for each year of the war # create 100 coordinate pairs (i.e. two values), then add 5 to all of them year_1 = np . random . randn ( 100 , 2 ) + 5 # create 30 coordinatee pairs (i.e. two values), then subtract 5 to all of them year_2 = np . random . randn ( 30 , 2 ) - 5 # create 50 coordinatee pairs (i.e. two values) year_3 = np . random . randn ( 50 , 2 ) View the first 3 entries of each year of battles print ( 'year 1 battles:' , year_1 [ 0 : 3 ]) print ( 'year 2 battles:' , year_2 [ 0 : 3 ]) print ( 'year 3 battles:' , year_3 [ 0 : 3 ]) year 1 battles: [[ 5.25720722 4.78051294] [ 4.11980541 6.24062638] [ 4.04612449 5.23819217]] year 2 battles: [[-3.90607071 -5.20880154] [-4.14244415 -4.52520445] [-6.01162308 -5.53489708]] year 3 battles: [[-0.54820297 -0.97483204] [ 0.12813873 0.55198748] [-0.55677223 -0.68900608]] Pool all three years of coordinates # vertically stack year_1, year_2, and year_3 elements battles = np . vstack ([ year_1 , year_2 , year_3 ]) Cluster the battle locations into three groups # calculate the centroid coordinates of each cluster # and the variance of all the clusters centroids , variance = vq . kmeans ( battles , 3 ) View the centroid coordinate for each of the three clusters centroids array([[ 5.02707263, 5.03041508], [-0.05392784, 0.12892838], [-4.88957266, -4.85051116]]) View the variance of the clusters (they all share the same) variance 1.2948126660038406 Seperate the battle data into clusters identified , distance = vq . vq ( battles , centroids ) View the cluster of each battle identified array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32) View the distance of each individual battle from their cluster's centroid distance array([ 0.3397249 , 1.51252941, 1.00271161, 0.7583883 , 0.58103782, 1.81905849, 1.45452846, 1.34523274, 0.69254441, 3.32123157, 1.73900653, 1.01999434, 1.5392708 , 0.64417605, 1.25822142, 1.68913457, 1.09543587, 0.20750281, 2.90778804, 1.62549404, 1.0224336 , 1.05196193, 0.98434964, 0.25634371, 1.19779956, 1.73517217, 2.69339667, 1.32792584, 0.97809768, 1.52654056, 2.20554365, 1.0403091 , 0.93698624, 1.53359041, 0.91717984, 0.3008527 , 0.42901893, 0.95824461, 1.93321831, 1.89139314, 1.49982335, 0.63265951, 1.48579627, 1.04574742, 0.83477916, 2.80489932, 1.50671741, 0.35230994, 1.18607368, 1.36078497, 1.17298152, 0.95961251, 0.95348923, 1.41903574, 1.7816999 , 1.32087763, 0.94807163, 2.22741733, 0.66198152, 0.97404075, 0.24009773, 1.22021557, 1.36298565, 1.77358477, 0.62586652, 1.45234278, 1.87925214, 2.18673534, 0.97113871, 1.0436524 , 1.63491437, 1.43922603, 1.8066756 , 2.55661988, 0.64905457, 0.6939938 , 1.41183181, 2.72140674, 1.70390906, 3.53986459, 1.52044903, 1.98702847, 1.2488108 , 2.61774172, 2.66067284, 0.80078946, 0.79648259, 2.72215296, 1.26904383, 1.16048896, 1.42571458, 1.18519189, 0.46592397, 0.63831379, 0.2294296 , 0.90199062, 0.99296186, 1.79154225, 0.23854105, 1.19095902, 1.0467321 , 0.81487758, 1.31429876, 0.14625493, 1.04421102, 0.72132375, 2.2209666 , 1.00145286, 1.30465026, 1.57217776, 1.31999891, 0.80321763, 2.12942642, 0.81168612, 1.40294667, 0.89994242, 1.70402817, 0.79621269, 1.29554062, 1.87340273, 2.40582742, 2.99089606, 1.01348705, 0.54974364, 0.39367389, 2.28343779, 1.51924388, 0.52095884, 1.54219385, 0.62972955, 1.20937793, 0.46057272, 0.96014023, 0.2287637 , 0.84009151, 1.34393522, 1.5983523 , 0.46066181, 0.49504327, 2.22788557, 1.74688212, 1.99998478, 0.25864751, 1.06955924, 1.68029793, 3.41862662, 1.9273365 , 0.91580509, 0.94390424, 1.42991149, 0.64314749, 0.26250126, 1.09000179, 0.42658645, 0.40866344, 0.47829004, 0.47718204, 0.53641019, 1.42037169, 2.20413065, 1.85270104, 1.9544685 , 1.40727147, 0.85730366, 1.63316935, 1.09642325, 1.36490331, 1.307389 , 1.9727463 , 1.35859479, 2.43699622, 0.80833152, 2.50758584, 0.95216108, 0.16936114, 0.98714981, 0.19962377, 1.13262204, 2.47056129, 2.00154513]) Index the battles data by the cluster to which they belong cluster_1 = battles [ identified == 0 ] cluster_2 = battles [ identified == 1 ] cluster_3 = battles [ identified == 2 ] Print the first three coordinate pairs of each cluster print ( cluster_1 [ 0 : 3 ]) print ( cluster_2 [ 0 : 3 ]) print ( cluster_3 [ 0 : 3 ]) [[ 5.25720722 4.78051294] [ 4.11980541 6.24062638] [ 4.04612449 5.23819217]] [[-0.54820297 -0.97483204] [ 0.12813873 0.55198748] [-0.55677223 -0.68900608]] [[-3.90607071 -5.20880154] [-4.14244415 -4.52520445] [-6.01162308 -5.53489708]] Plot all the battles, color each battle by cluster # create a scatter plot there the x-axis is the first column of battles # the y-axis is the second column of battles, the size is 100, and # the color of each point is determined by the indentified variable plt . scatter ( battles [:, 0 ], battles [:, 1 ], s = 100 , c = identified ) <matplotlib.collections.PathCollection at 0x10d43f588>","tags":"Python","url":"http://chrisalbon.com/python/scipy_simple_clustering.html","loc":"http://chrisalbon.com/python/scipy_simple_clustering.html"},{"title":"Color Palettes in Seaborn","text":"Preliminaries import pandas as pd % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'deaths_regiment_1' : [ 34 , 43 , 14 , 15 , 15 , 14 , 31 , 25 , 62 , 41 ], 'deaths_regiment_2' : [ 52 , 66 , 78 , 15 , 15 , 5 , 25 , 25 , 86 , 1 ], 'deaths_regiment_3' : [ 13 , 73 , 82 , 58 , 52 , 87 , 26 , 5 , 56 , 75 ], 'deaths_regiment_4' : [ 44 , 75 , 26 , 15 , 15 , 14 , 54 , 25 , 24 , 72 ], 'deaths_regiment_5' : [ 25 , 24 , 25 , 15 , 57 , 68 , 21 , 27 , 62 , 5 ], 'deaths_regiment_6' : [ 84 , 84 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 24 ], 'deaths_regiment_7' : [ 46 , 57 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' , 'deaths_regiment_1' , 'deaths_regiment_2' , 'deaths_regiment_3' , 'deaths_regiment_4' , 'deaths_regiment_5' , 'deaths_regiment_6' , 'deaths_regiment_7' ]) df = df . set_index ( df . date ) View some color palettes sns . palplot ( sns . color_palette ( \"deep\" , 10 )) sns . palplot ( sns . color_palette ( \"muted\" , 10 )) sns . palplot ( sns . color_palette ( \"bright\" , 10 )) sns . palplot ( sns . color_palette ( \"dark\" , 10 )) sns . palplot ( sns . color_palette ( \"colorblind\" , 10 )) sns . palplot ( sns . color_palette ( \"Paired\" , 10 )) sns . palplot ( sns . color_palette ( \"BuGn\" , 10 )) sns . palplot ( sns . color_palette ( \"GnBu\" , 10 )) sns . palplot ( sns . color_palette ( \"OrRd\" , 10 )) sns . palplot ( sns . color_palette ( \"PuBu\" , 10 )) sns . palplot ( sns . color_palette ( \"YlGn\" , 10 )) sns . palplot ( sns . color_palette ( \"YlGnBu\" , 10 )) sns . palplot ( sns . color_palette ( \"YlOrBr\" , 10 )) sns . palplot ( sns . color_palette ( \"YlOrRd\" , 10 )) sns . palplot ( sns . color_palette ( \"BrBG\" , 10 )) sns . palplot ( sns . color_palette ( \"PiYG\" , 10 )) sns . palplot ( sns . color_palette ( \"PRGn\" , 10 )) sns . palplot ( sns . color_palette ( \"PuOr\" , 10 )) sns . palplot ( sns . color_palette ( \"RdBu\" , 10 )) sns . palplot ( sns . color_palette ( \"RdGy\" , 10 )) sns . palplot ( sns . color_palette ( \"RdYlBu\" , 10 )) sns . palplot ( sns . color_palette ( \"RdYlGn\" , 10 )) sns . palplot ( sns . color_palette ( \"Spectral\" , 10 )) Create a color palette and set it as the current color palette flatui = [ \"#9b59b6\" , \"#3498db\" , \"#95a5a6\" , \"#e74c3c\" , \"#34495e\" , \"#2ecc71\" ] sns . set_palette ( flatui ) sns . palplot ( sns . color_palette ()) Set the color of a plot sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], color = \"#34495e\" ) <matplotlib.axes._subplots.AxesSubplot at 0x116f5db70>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_color_palettes.html","loc":"http://chrisalbon.com/python/seaborn_color_palettes.html"},{"title":"Creating A Time Series Plot With Seaborn And Pandas","text":"Preliminaries import pandas as pd % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'deaths_regiment_1' : [ 34 , 43 , 14 , 15 , 15 , 14 , 31 , 25 , 62 , 41 ], 'deaths_regiment_2' : [ 52 , 66 , 78 , 15 , 15 , 5 , 25 , 25 , 86 , 1 ], 'deaths_regiment_3' : [ 13 , 73 , 82 , 58 , 52 , 87 , 26 , 5 , 56 , 75 ], 'deaths_regiment_4' : [ 44 , 75 , 26 , 15 , 15 , 14 , 54 , 25 , 24 , 72 ], 'deaths_regiment_5' : [ 25 , 24 , 25 , 15 , 57 , 68 , 21 , 27 , 62 , 5 ], 'deaths_regiment_6' : [ 84 , 84 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 24 ], 'deaths_regiment_7' : [ 46 , 57 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' , 'deaths_regiment_1' , 'deaths_regiment_2' , 'deaths_regiment_3' , 'deaths_regiment_4' , 'deaths_regiment_5' , 'deaths_regiment_6' , 'deaths_regiment_7' ]) df = df . set_index ( df . date ) Time Series Plot sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], color = \"indianred\" ) <matplotlib.axes._subplots.AxesSubplot at 0x1140be780> Time Series Splot With Confidence Interval Lines But No Lines sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], err_style = \"ci_bars\" , interpolate = False ) <matplotlib.axes._subplots.AxesSubplot at 0x116400668>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_pandas_timeseries_plot.html","loc":"http://chrisalbon.com/python/seaborn_pandas_timeseries_plot.html"},{"title":"Creating Scatterplots with Seaborn","text":"Preliminaries import pandas as pd % matplotlib inline import random import matplotlib.pyplot as plt import seaborn as sns Create data # Create empty dataframe df = pd . DataFrame () # Add columns df [ 'x' ] = random . sample ( range ( 1 , 1000 ), 5 ) df [ 'y' ] = random . sample ( range ( 1 , 1000 ), 5 ) df [ 'z' ] = [ 1 , 0 , 0 , 1 , 0 ] df [ 'k' ] = [ 'male' , 'male' , 'male' , 'female' , 'female' ] # View first few rows of data df . head () .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } x y z k 0 466 948 1 male 1 832 481 0 male 2 978 465 0 male 3 510 206 1 female 4 848 357 0 female Scatterplot # Set style of scatterplot sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) # Create scatterplot of dataframe sns . lmplot ( 'x' , # Horizontal axis 'y' , # Vertical axis data = df , # Data source fit_reg = False , # Don't fix a regression line hue = \"z\" , # Set color scatter_kws = { \"marker\" : \"D\" , # Set marker style \"s\" : 100 }) # S marker size # Set title plt . title ( 'Histogram of IQ' ) # Set x-axis label plt . xlabel ( 'Time' ) # Set y-axis label plt . ylabel ( 'Deaths' ) <matplotlib.text.Text at 0x112b7bb70>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_scatterplot.html","loc":"http://chrisalbon.com/python/seaborn_scatterplot.html"},{"title":"Select An Entire Table","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. # Ignore % load_ext sql % sql sqlite : // 'Connected: None@None' Create Data %% sql /* Create A Table Of Criminals */ CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); /* Create A Table Of Crimes */ CREATE TABLE crimes ( cid , crime , city , pid_arrested , cash_stolen ); INSERT INTO crimes VALUES ( 1 , 'fraud' , 'Santa Rosa' , 412 , 40000 ); INSERT INTO crimes VALUES ( 1 , 'burglary' , 'Petaluma' , 234 , 2000 ); INSERT INTO crimes VALUES ( 1 , 'burglary' , 'Santa Rosa' , 632 , 2000 ); INSERT INTO crimes VALUES ( 1 , 'larcony' , 'Petaluma' , 621 , 3500 ); INSERT INTO crimes VALUES ( 1 , 'burglary' , 'Santa Rosa' , 162 , 1000 ); INSERT INTO crimes VALUES ( 1 , 'larcony' , 'Petaluma' , 901 , 50000 ); INSERT INTO crimes VALUES ( 1 , 'fraud' , 'San Francisco' , 412 , 60000 ); INSERT INTO crimes VALUES ( 1 , 'burglary' , 'Santa Rosa' , 512 , 7000 ); INSERT INTO crimes VALUES ( 1 , 'burglary' , 'San Francisco' , 411 , 3000 ); INSERT INTO crimes VALUES ( 1 , 'robbery' , 'Santa Rosa' , 632 , 2500 ); INSERT INTO crimes VALUES ( 1 , 'robbery' , 'Santa Rosa' , 512 , 3000 ); Done. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. Done. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. 1 rows affected. [] View Both Tables %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals Done. pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 162 Jaden Ado 49 M None 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0 411 Bob Iton None M San Francisco 0 %% sql -- Select everything SELECT * -- From the table 'crimes' FROM crimes Done. cid crime city pid_arrested cash_stolen 1 fraud Santa Rosa 412 40000 1 burglary Petaluma 234 2000 1 burglary Santa Rosa 632 2000 1 larcony Petaluma 621 3500 1 burglary Santa Rosa 162 1000 1 larcony Petaluma 901 50000 1 fraud San Francisco 412 60000 1 burglary Santa Rosa 512 7000 1 burglary San Francisco 411 3000 1 robbery Santa Rosa 632 2500 1 robbery Santa Rosa 512 3000","tags":"SQL","url":"http://chrisalbon.com/sql/select_entire_table.html","loc":"http://chrisalbon.com/sql/select_entire_table.html"},{"title":"Select From Multiple Tables Simultaneously","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Two Tables, Criminals And Crimes %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); -- Create a table of crimes CREATE TABLE crimes ( cid , crime , city , pid_arrested , cash_stolen ); INSERT INTO crimes VALUES ( 1 , 'fraud' , 'Santa Rosa' , 412 , 40000 ); INSERT INTO crimes VALUES ( 2 , 'burglary' , 'Petaluma' , 234 , 2000 ); INSERT INTO crimes VALUES ( 3 , 'burglary' , 'Santa Rosa' , 632 , 2000 ); INSERT INTO crimes VALUES ( 4 , NULL , NULL , 621 , 3500 ); INSERT INTO crimes VALUES ( 5 , 'burglary' , 'Santa Rosa' , 162 , 1000 ); INSERT INTO crimes VALUES ( 6 , NULL , 'Petaluma' , 901 , 50000 ); INSERT INTO crimes VALUES ( 7 , 'fraud' , 'San Francisco' , 412 , 60000 ); INSERT INTO crimes VALUES ( 8 , 'burglary' , 'Santa Rosa' , 512 , 7000 ); INSERT INTO crimes VALUES ( 9 , 'burglary' , 'San Francisco' , 411 , 3000 ); INSERT INTO crimes VALUES ( 10 , 'robbery' , 'Santa Rosa' , 632 , 2500 ); INSERT INTO crimes VALUES ( 11 , 'robbery' , 'Santa Rosa' , 512 , 3000 ); [] View All Unique City Names From Both Tables %% sql -- Select city name SELECT city -- From criminals table FROM criminals -- Then combine with UNION -- Select city names SELECT city -- From crimes table FROM crimes ; city None Petaluma San Francisco Santa Rosa View All City Names From Both Tables %% sql -- Select city name SELECT city -- From criminals table FROM criminals -- Then combine with UNION ALL -- Select city names SELECT city -- From crimes table FROM crimes ; city Santa Rosa Santa Rosa Santa Rosa Petaluma None Santa Rosa Santa Rosa San Francisco Santa Rosa Petaluma Santa Rosa None Santa Rosa Petaluma San Francisco Santa Rosa San Francisco Santa Rosa Santa Rosa","tags":"SQL","url":"http://chrisalbon.com/sql/select_from_multiple_tables.html","loc":"http://chrisalbon.com/sql/select_from_multiple_tables.html"},{"title":"Select Rows That Contain An Item In A List","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Select Rows That Contain An Item In A List %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals -- Where the city is any of these cities WHERE city IN ( 'Santa Rosa' , 'Petaluma' ); pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 621 Betty Bob None F Petaluma 1 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0","tags":"SQL","url":"http://chrisalbon.com/sql/select_rows_that_contain_an_item_in_a_list.html","loc":"http://chrisalbon.com/sql/select_rows_that_contain_an_item_in_a_list.html"},{"title":"Select Values Between Two Values","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Select Every Row Where Age Is Between Two Values %% sql -- Select everything SELECT * -- From the table 'criminals' FROM criminals -- Where WHERE age BETWEEN 12 AND 18 pid name age sex city minor 412 James Smith 15 M Santa Rosa 1","tags":"SQL","url":"http://chrisalbon.com/sql/select_values_between_two_values.html","loc":"http://chrisalbon.com/sql/select_values_between_two_values.html"},{"title":"Set The Color Of A Matplotlib Plot","text":"Import numpy and matplotlib.pyplot % matplotlib inline import numpy as np import matplotlib.pyplot as plt Create some simulated data. n = 100 r = 2 * np . random . rand ( n ) theta = 2 * np . pi * np . random . rand ( n ) area = 200 * r ** 2 * np . random . rand ( n ) colors = theta Create a scatterplot using the a colormap. Full list of colormaps: http://wiki.scipy.org/Cookbook/Matplotlib/Show_colormaps c = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdYlGn ) c1 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Blues ) c2 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . BrBG ) c3 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Greens ) c4 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdGy ) c5 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . YlOrRd ) c6 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . autumn ) c7 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . binary ) c8 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . gist_earth ) c9 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . gist_heat ) c10 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . hot ) c11 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . spring ) c12 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . summer ) c12 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . winter ) c13 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . bone ) c14 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . cool ) c15 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . YlGn ) c16 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdBu ) c17 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . PuOr ) c18 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Oranges )","tags":"Python","url":"http://chrisalbon.com/python/set_the_color_of_a_matplotlib.html","loc":"http://chrisalbon.com/python/set_the_color_of_a_matplotlib.html"},{"title":"Store A Query Result","text":"Note: This tutorial was written using Catherine Devlin's SQL in Jupyter Notebooks library . If you have not using a Jupyter Notebook, you can ignore the two lines of code below and any line containing %%sql . Furthermore, this tutorial uses SQLite's flavor of SQL, your version might have some differences in syntax. For more, check out Learning SQL by Alan Beaulieu. # Ignore % load_ext sql % sql sqlite : // % config SqlMagic . feedback = False Create Data %% sql -- Create a table of criminals CREATE TABLE criminals ( pid , name , age , sex , city , minor ); INSERT INTO criminals VALUES ( 412 , 'James Smith' , 15 , 'M' , 'Santa Rosa' , 1 ); INSERT INTO criminals VALUES ( 234 , 'Bill James' , 22 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 632 , 'Stacy Miller' , 23 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 621 , 'Betty Bob' , NULL , 'F' , 'Petaluma' , 1 ); INSERT INTO criminals VALUES ( 162 , 'Jaden Ado' , 49 , 'M' , NULL , 0 ); INSERT INTO criminals VALUES ( 901 , 'Gordon Ado' , 32 , 'F' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 512 , 'Bill Byson' , 21 , 'M' , 'Santa Rosa' , 0 ); INSERT INTO criminals VALUES ( 411 , 'Bob Iton' , NULL , 'M' , 'San Francisco' , 0 ); [] Create A View %% sql -- Create a view called 'Santa Rosa' CREATE VIEW [ Santa Rosa ] AS -- That contains everything SELECT * -- From the table called 'criminals' FROM criminals -- If the city is 'Santa Rosa' WHERE city = 'Santa Rosa' [] View The View (I know, I know, stupid title) %% sql -- Select everything SELECT * -- From the view called [ Santa Rosa ] FROM [ Santa Rosa ] pid name age sex city minor 412 James Smith 15 M Santa Rosa 1 234 Bill James 22 M Santa Rosa 0 632 Stacy Miller 23 F Santa Rosa 0 901 Gordon Ado 32 F Santa Rosa 0 512 Bill Byson 21 M Santa Rosa 0","tags":"SQL","url":"http://chrisalbon.com/sql/store_a_query_result.html","loc":"http://chrisalbon.com/sql/store_a_query_result.html"},{"title":"String Formatting","text":"Import the sys module import sys Print a string with 1 digit and one string. 'This is %d %s bird!' % ( 1 , 'dead' ) 'This is 1 dead bird!' Print a dictionary based string ' %(number)d more %(food)s ' % { 'number' : 1 , 'food' : 'burger' } '1 more burger' Print a string about my laptop. 'My {1[kind]} runs {0.platform}' . format ( sys , { 'kind' : 'laptop' }) 'My laptop runs darwin' String Formatting Codes %s string %r repr string %c character (integer or string) %d decimal %i integer %x hex integer %X same as X but with uppercase %e floating point lowercase %E floating point uppercase %f floating point decimal lowercase %F floating point decimal uppercase %g floating point e or f %G floating point E or F %% literal %","tags":"Python","url":"http://chrisalbon.com/python/string_formatting.html","loc":"http://chrisalbon.com/python/string_formatting.html"},{"title":"String Indexing","text":"Create a string string = 'Strings are defined as ordered collections of characters.' Print the entire string string [:] 'Strings are defined as ordered collections of characters.' Print the first three characters string [ 0 : 3 ] 'Str' Print the first three characters string [: 3 ] 'Str' Print the last three characters string [ - 3 :] 'rs.' Print the third to fifth character string [ 2 : 5 ] 'rin' Print the first to the tenth character, skipping every other character string [ 0 : 10 : 2 ] 'Srnsa' Print the string in reverse string [:: - 1 ] '.sretcarahc fo snoitcelloc deredro sa denifed era sgnirtS'","tags":"Python","url":"http://chrisalbon.com/python/string_indexing.html","loc":"http://chrisalbon.com/python/string_indexing.html"},{"title":"String Operations","text":"Python 3 has three string types str() is for unicode bytes() is for binary data bytesarray() mutable variable of bytes Create some simulated text. string = 'The quick brown fox jumped over the lazy brown bear.' Capitalize the first letter. string_capitalized = string . capitalize () string_capitalized 'The quick brown fox jumped over the lazy brown bear.' Center the string with periods on either side, for a total of 79 characters string_centered = string . center ( 79 , '.' ) string_centered '..............The quick brown fox jumped over the lazy brown bear..............' Count the number of e's between the fifth and last character string_counted = string . count ( 'e' , 4 , len ( string )) string_counted 4 Locate any e's between the fifth and last character string_find = string . find ( 'e' , 4 , len ( string )) string_find 24 Are all characters are alphabet? string_isalpha = string . isalpha () string_isalpha False Are all characters digits? string_isdigit = string . isdigit () string_isdigit False Are all characters lower case? string_islower = string . islower () string_islower False Are all chracters alphanumeric? string_isalnum = string . isalnum () string_isalnum False Are all characters whitespaces? string_isalnum = string . isspace () string_isalnum False Is the string properly titlespaced? string_istitle = string . istitle () string_istitle False Are all the characters uppercase? string_isupper = string . isupper () string_isupper False Return the lengths of string len ( string ) 52 Convert string to lower case string_lower = string . lower () string_lower 'the quick brown fox jumped over the lazy brown bear.' Convert string to lower case string_upper = string . upper () string_upper 'THE QUICK BROWN FOX JUMPED OVER THE LAZY BROWN BEAR.' Convert string to title case string_title = string . title () string_title 'The Quick Brown Fox Jumped Over The Lazy Brown Bear.' Convert string the inverted case string_swapcase = string . swapcase () string_swapcase 'tHE QUICK BROWN FOX JUMPED OVER THE LAZY BROWN BEAR.' Remove all leading whitespaces (i.e. to the left) string_lstrip = string . lstrip () string_lstrip 'The quick brown fox jumped over the lazy brown bear.' Remove all leading and trailing whitespaces (i.e. to the left and right) string_strip = string . strip () string_strip 'The quick brown fox jumped over the lazy brown bear.' Remove all trailing whitespaces (i.e. to the right) string_rstrip = string . rstrip () string_rstrip 'The quick brown fox jumped over the lazy brown bear.' Replace lower case e's with upper case E's, to a maximum of 4 string_replace = string . replace ( 'e' , 'E' , 4 ) string_replace 'ThE quick brown fox jumpEd ovEr thE lazy brown bear.'","tags":"Python","url":"http://chrisalbon.com/python/string_operations.html","loc":"http://chrisalbon.com/python/string_operations.html"},{"title":"Converting Strings To Datetime","text":"Import modules from datetime import datetime from dateutil.parser import parse import pandas as pd Create a string variable with the war start time war_start = '2011-01-03' Convert the string to datetime format datetime . strptime ( war_start , '%Y-%m- %d ' ) datetime.datetime(2011, 1, 3, 0, 0) Create a list of strings as dates attack_dates = [ '7/2/2011' , '8/6/2012' , '11/13/2013' , '5/26/2011' , '5/2/2001' ] Convert attack_dates strings into datetime format [ datetime . strptime ( x , '%m/ %d /%Y' ) for x in attack_dates ] [datetime.datetime(2011, 7, 2, 0, 0), datetime.datetime(2012, 8, 6, 0, 0), datetime.datetime(2013, 11, 13, 0, 0), datetime.datetime(2011, 5, 26, 0, 0), datetime.datetime(2001, 5, 2, 0, 0)] Use parse() to attempt to auto-convert common string formats parse ( war_start ) datetime.datetime(2011, 1, 3, 0, 0) Use parse() on every element of the attack_dates string [ parse ( x ) for x in attack_dates ] [datetime.datetime(2011, 7, 2, 0, 0), datetime.datetime(2012, 8, 6, 0, 0), datetime.datetime(2013, 11, 13, 0, 0), datetime.datetime(2011, 5, 26, 0, 0), datetime.datetime(2001, 5, 2, 0, 0)] Use parse, but designate that the day is first parse ( war_start , dayfirst = True ) datetime.datetime(2011, 3, 1, 0, 0) Create a dataframe data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'value' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'value' ]) print ( df ) date value 0 2014-05-01 18:47:05.069722 1 1 2014-05-01 18:47:05.119994 1 2 2014-05-02 18:47:05.178768 1 3 2014-05-02 18:47:05.230071 1 4 2014-05-02 18:47:05.230071 1 5 2014-05-02 18:47:05.280592 1 6 2014-05-03 18:47:05.332662 1 7 2014-05-03 18:47:05.385109 1 8 2014-05-04 18:47:05.436523 1 9 2014-05-04 18:47:05.486877 1 Convert df['date'] from string to datetime pd . to_datetime ( df [ 'date' ]) 0 2014-05-01 18:47:05.069722 1 2014-05-01 18:47:05.119994 2 2014-05-02 18:47:05.178768 3 2014-05-02 18:47:05.230071 4 2014-05-02 18:47:05.230071 5 2014-05-02 18:47:05.280592 6 2014-05-03 18:47:05.332662 7 2014-05-03 18:47:05.385109 8 2014-05-04 18:47:05.436523 9 2014-05-04 18:47:05.486877 Name: date, dtype: datetime64[ns]","tags":"Python","url":"http://chrisalbon.com/python/strings_to_datetime.html","loc":"http://chrisalbon.com/python/strings_to_datetime.html"},{"title":"Swapping variable values","text":"Setup the originally variables and their values one = 1 two = 2 View the original variables 'one =' , one , 'two =' , two ('one =', 1, 'two =', 2) Swap the values one , two = two , one View the swapped values, notice how the values for each variable have changed 'one =' , one , 'two =' , two ('one =', 2, 'two =', 1)","tags":"Python","url":"http://chrisalbon.com/python/swapping_variable_values.html","loc":"http://chrisalbon.com/python/swapping_variable_values.html"},{"title":"Try, Except, and Finally","text":"Create data # Create some data scores = [ 23 , 453 , 54 , 235 , 74 , 234 ] Try something that doesn't work # Try to: try : # Add a list of integers and a string scores + 'A string of characters.' # If you get an error, set the error as 'e', except Exception as e : # print the error, e print ( 'Error:' , e ) # Then, finally : # print end program print ( 'End Program' ) Error : can only concatenate list ( not \"str\" ) to list End Program Try something that works # Try to: try : # Print scores print ( 'Worked!' , scores ) # If you get an error, set the error as 'e', except Exception as e : # print the error, e print ( 'Error:' , e ) # Then, finally : # print end program print ( 'End program' ) Worked! [23, 453, 54, 235, 74, 234] End program","tags":"Python","url":"http://chrisalbon.com/python/try_except_finally.html","loc":"http://chrisalbon.com/python/try_except_finally.html"},{"title":"while Statement","text":"A while loop loops while a condition is true, stops when the condition becomes false Import the random module import random Create a variable of the true number of deaths of an event deaths = 6 Create a variable that is denotes if the while loop should keep running running = True while running is True while running : # Create a variable that randomly create a integer between 0 and 10. guess = random . randint ( 0 , 10 ) # if guess equals deaths, if guess == deaths : # then print this print ( 'Correct!' ) # and then also change running to False to stop the script running = False # else if guess is lower than deaths elif guess < deaths : # then print this print ( 'No, it is higher.' ) # if guess is none of the above else : # print this print ( 'No, it is lower' ) No, it is higher. No, it is higher. Correct! By the output, you can see that the while script keeping generating guesses and checking them until guess matches deaths, in which case the script stops.","tags":"Python","url":"http://chrisalbon.com/python/while_statements.html","loc":"http://chrisalbon.com/python/while_statements.html"},{"title":"Monitor A Website For Changes With Python","text":"In this snippet, we create a continous loop that, at set times, scrapes a website, checks to see if it contains some text and if so, emails me. Specifically I used this script to find when Venture Beat had published an article about my company. It should be noted that there are more efficient ways of setting scripts to run at certain times, notable cron. However, this is a quick and dirty solution. Note: I've commented out the last few lines of this tutorial, which attempts to send an email. Before running this code, uncomment those lines Preliminaries # Import requests (to download the page) import requests # Import BeautifulSoup (to parse what we download) from bs4 import BeautifulSoup # Import Time (to add a delay between the times the scape runs) import time # Import smtplib (to allow us to email) import smtplib Monitoring Script # This is a pretty simple script. The script downloads the homepage of VentureBeat, and if it finds some text, emails me. # If it does not find some text, it waits 60 seconds and downloads the homepage again. # while this is true (it is true by default), while True : # set the url as VentureBeat, url = \"http://Google.com/\" # set the headers like we are a browser, headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' } # download the homepage response = requests . get ( url , headers = headers ) # parse the downloaded homepage and grab all text, then, soup = BeautifulSoup ( response . text , \"lxml\" ) # if the number of times the word \"Google\" occurs on the page is less than 1, if str ( soup ) . find ( \"Google\" ) == - 1 : # wait 60 seconds, time . sleep ( 60 ) # continue with the script, continue # but if the word \"Google\" occurs any other number of times, else : # create an email message with just a subject line, msg = 'Subject: This is Chris \\' s script talking, CHECK GOOGLE!' # set the 'from' address, fromaddr = 'YOUR_EMAIL_ADDRESS' # set the 'to' addresses, toaddrs = [ 'AN_EMAIL_ADDRESS' , 'A_SECOND_EMAIL_ADDRESS' , 'A_THIRD_EMAIL_ADDRESS' ] # setup the email server, # server = smtplib.SMTP('smtp.gmail.com', 587) # server.starttls() # add my account login name and password, # server.login(\"YOUR_EMAIL_ADDRESS\", \"YOUR_PASSWORD\") # Print the email's contents print ( 'From: ' + fromaddr ) print ( 'To: ' + str ( toaddrs )) print ( 'Message: ' + msg ) # send the email # server.sendmail(fromaddr, toaddrs, msg) # disconnect from the server # server.quit() break From : YOUR_EMAIL_ADDRESS To : [ 'AN_EMAIL_ADDRESS' , 'A_SECOND_EMAIL_ADDRESS' , 'A_THIRD_EMAIL_ADDRESS' ] Message : Subject : This is Chris ' s script talking , CHECK GOOGLE !","tags":"Python","url":"http://chrisalbon.com/python/monitor_a_website.html","loc":"http://chrisalbon.com/python/monitor_a_website.html"},{"title":"Quickly Change A Column Of Strings In Pandas","text":"Often I need or want to change the case of all items in a column of strings (e.g. BRAZIL to Brazil, etc.). There are many ways to accomplish this but I have settled on this one as the easiest and quickest. # Import pandas import pandas as pd # Create a list of first names first_names = pd . Series ([ 'Steve Murrey' , 'Jane Fonda' , 'Sara McGully' , 'Mary Jane' ]) # print the column first_names 0 Steve Murrey 1 Jane Fonda 2 Sara McGully 3 Mary Jane dtype: object # print the column with lower case first_names . str . lower () 0 steve murrey 1 jane fonda 2 sara mcgully 3 mary jane dtype: object # print the column with upper case first_names . str . upper () 0 STEVE MURREY 1 JANE FONDA 2 SARA MCGULLY 3 MARY JANE dtype: object # print the column with title case first_names . str . title () 0 Steve Murrey 1 Jane Fonda 2 Sara Mcgully 3 Mary Jane dtype: object # print the column split across spaces first_names . str . split ( \" \" ) 0 [Steve, Murrey] 1 [Jane, Fonda] 2 [Sara, McGully] 3 [Mary, Jane] dtype: object # print the column with capitalized case first_names . str . capitalize () 0 Steve murrey 1 Jane fonda 2 Sara mcgully 3 Mary jane dtype: object You get the idea. Many more string methods are avaliable here","tags":"Python","url":"http://chrisalbon.com/python/pandas_change_column_of_strings.html","loc":"http://chrisalbon.com/python/pandas_change_column_of_strings.html"},{"title":"Multiline Strings","text":"There are various ways to include multiline strings in JavaScript. Here is the generally preferable way. // Create a series of strings, one per desired line. var multiline = \"Hello Steve\\n\" + \"Rock on.\\n\" + \"From Chris\" ; // Print the string console . log ( multiline ) Hello Steve Rock on. From Chris","tags":"Javascript","url":"http://chrisalbon.com/javascript/multiline_strings.html","loc":"http://chrisalbon.com/javascript/multiline_strings.html"},{"title":"Printing Variables Inside Strings","text":"String formatting in JavaScript works very similar to other languages like Python 3. String concatenation allows for non-string variables to be incorporated into string variables. There are two basic ways of printing variables inside strings. Here is the first: // Create a variable called currentTemp let votingTime = 3 ; // Create a constant (variable) with a string that includes the value from currentTemp const message = \"Voters should go to polling station at \" + votingTime + \"pm\" ; // Print the string console . log ( message ) Voters should go to polling station at 3pm In the first way, you simply wrap the variable in strings. There is, however, a more modern way of achieving the same output: // Create a constant (variable) with a string that includes the value from currentTemp const message = `Voters should go to polling station at ${ votingTime } pm` ; // Print the string console . log ( message ) Voters should go to polling station at 3pm In the example above, instead of wrapping the variable in strings, we called the variable directly from inside a string.","tags":"Javascript","url":"http://chrisalbon.com/javascript/printing_variables_inside_strings.html","loc":"http://chrisalbon.com/javascript/printing_variables_inside_strings.html"},{"title":"Group Data By Time","text":"On March 13, 2016, version 0.18.0 of Pandas was released, with significant changes in how the resampling function operates. This tutorial follows v0.18.0 and will not work for previous versions of pandas. First let's load the modules we care about Preliminaries # Import required packages import pandas as pd import datetime import numpy as np Next, let's create some sample data that we can group by time as an sample. In this example I am creating a dataframe with two columns with 365 rows. One column is a date, the second column is a numeric value. Create Data # Create a datetime variable for today base = datetime . datetime . today () # Create a list variable that creates 365 days of rows of datetime values date_list = [ base - datetime . timedelta ( days = x ) for x in range ( 0 , 365 )] # Create a list variable of 365 numeric values score_list = list ( np . random . randint ( low = 1 , high = 1000 , size = 365 )) # Create an empty dataframe df = pd . DataFrame () # Create a column from the datetime variable df [ 'datetime' ] = date_list # Convert that column into a datetime datatype df [ 'datetime' ] = pd . to_datetime ( df [ 'datetime' ]) # Set the datetime column as the index df . index = df [ 'datetime' ] # Create a column from the numeric score variable df [ 'score' ] = score_list # Let's take a took at the data df . head () datetime score datetime 2016-06-02 09:57:54.793972 2016-06-02 09:57:54.793972 900 2016-06-01 09:57:54.793972 2016-06-01 09:57:54.793972 121 2016-05-31 09:57:54.793972 2016-05-31 09:57:54.793972 547 2016-05-30 09:57:54.793972 2016-05-30 09:57:54.793972 504 2016-05-29 09:57:54.793972 2016-05-29 09:57:54.793972 304 Group Data By Date In pandas, the most common way to group by time is to use the .resample() function. In v0.18.0 this function is two-stage. This means that 'df.resample('M')' creates an object to which we can apply other functions ('mean', 'count', 'sum', etc.) # Group the data by month, and take the mean for each group (i.e. each month) df . resample ( 'M' ) . mean () score datetime 2015-06-30 513.629630 2015-07-31 561.516129 2015-08-31 448.032258 2015-09-30 548.000000 2015-10-31 480.419355 2015-11-30 487.033333 2015-12-31 499.935484 2016-01-31 429.193548 2016-02-29 520.413793 2016-03-31 349.806452 2016-04-30 395.500000 2016-05-31 503.451613 2016-06-30 510.500000 # Group the data by month, and take the sum for each group (i.e. each month) df . resample ( 'M' ) . sum () score datetime 2015-06-30 13868 2015-07-31 17407 2015-08-31 13889 2015-09-30 16440 2015-10-31 14893 2015-11-30 14611 2015-12-31 15498 2016-01-31 13305 2016-02-29 15092 2016-03-31 10844 2016-04-30 11865 2016-05-31 15607 2016-06-30 1021 Grouping Options There are many options for grouping. You can learn more about them in Pandas's timeseries docs , however, I have also listed them below for your convience. | Value | Description |---| |B | business day frequency |C | custom business day frequency (experimental) |D | calendar day frequency |W | weekly frequency |M | month end frequency |BM | business month end frequency |CBM | custom business month end frequency |MS | month start frequency |BMS | business month start frequency |CBMS| custom business month start frequency |Q | quarter end frequency |BQ | business quarter endfrequency |QS | quarter start frequency |BQS | business quarter start frequency |A | year end frequency |BA | business year end frequency |AS | year start frequency |BAS | business year start frequency |BH | business hour frequency |H | hourly frequency |T | minutely frequency |S | secondly frequency |L | milliseonds |U | microseconds |N | nanosecondsa","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_data_by_time.html","loc":"http://chrisalbon.com/python/pandas_group_data_by_time.html"},{"title":"Break A List Into N-Sized Chunks","text":"In this snippet we take a list and break it up into n-size chunks. This is a very common practice when dealing with APIs that have a maximum request size. Credit for this nifty function goes to Ned Batchelder who posted it on StackOverflow . # Create a list of first names first_names = [ 'Steve' , 'Jane' , 'Sara' , 'Mary' , 'Jack' , 'Bob' , 'Bily' , 'Boni' , 'Chris' , 'Sori' , 'Will' , 'Won' , 'Li' ] # Create a function called \"chunks\" with two arguments, l and n: def chunks ( l , n ): # For item i in a range that is a length of l, for i in range ( 0 , len ( l ), n ): # Create an index range for l of n items: yield l [ i : i + n ] # Create a list that from the results of the function chunks: list ( chunks ( first_names , 5 )) [['Steve', 'Jane', 'Sara', 'Mary', 'Jack'], ['Bob', 'Bily', 'Boni', 'Chris', 'Sori'], ['Will', 'Won', 'Li']]","tags":"Python","url":"http://chrisalbon.com/python/break_list_into_chunks_of_equal_size.html","loc":"http://chrisalbon.com/python/break_list_into_chunks_of_equal_size.html"},{"title":"Pearson's Correlation Coefficient","text":"Based on this StackOverflow answer by cbare . Preliminaries import statistics as stats Create Data x = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] y = [ 2 , 1 , 2 , 4.5 , 7 , 6.5 , 6 , 9 , 9.5 ] Calculate Pearson's Correlation Coefficient There are a number of equivalent expression ways to calculate Pearson's correlation coefficient (also called Pearson's r). Here is one. $$r={\\frac {1}{n-1}}\\sum {i=1}&#94;{n}\\left({\\frac {x -{\\bar {x}}}{s_{x}}}\\right)\\left({\\frac {y_{i}-{\\bar {y}}}{s_{y}}}\\right)$$ where $s_{x}$ and $s_{y}$ are the sample standard deviation for $x$ and $y$, and $\\left({\\frac {x_{i}-{\\bar {x}}}{s_{x}}}\\right)$ is the standard score for $x$ and $y$. # Create a function def pearson ( x , y ): # Create n, the number of observations in the data n = len ( x ) # Create lists to store the standard scores standard_score_x = [] standard_score_y = [] # Calculate the mean of x mean_x = stats . mean ( x ) # Calculate the standard deviation of x standard_deviation_x = stats . stdev ( x ) # Calculate the mean of y mean_y = stats . mean ( y ) # Calculate the standard deviation of y standard_deviation_y = stats . stdev ( y ) # For each observation in x for observation in x : # Calculate the standard score of x standard_score_x . append (( observation - mean_x ) / standard_deviation_x ) # For each observation in y for observation in y : # Calculate the standard score of y standard_score_y . append (( observation - mean_y ) / standard_deviation_y ) # Multiple the standard scores together, sum them, then divide by n-1, return that value return ( sum ([ i * j for i , j in zip ( standard_score_x , standard_score_y )])) / ( n - 1 ) # Show Pearson's Correlation Coefficient pearson ( x , y ) 0.9412443251336238","tags":"Statistics","url":"http://chrisalbon.com/statistics/pearsons_correlation_coefficient.html","loc":"http://chrisalbon.com/statistics/pearsons_correlation_coefficient.html"},{"title":"Probability Mass Functions","text":"Preliminaries # Load libraries import matplotlib.pyplot as plt Create Data # Create some random integer data data = [ 3 , 2 , 3 , 4 , 2 , 3 , 5 , 2 , 2 , 3 , 3 , 5 , 2 , 2 , 5 , 6 , 2 , 2 , 2 , 3 , 6 , 6 , 2 , 4 , 3 , 2 , 3 ] Create A Count Of Values # Create a dictionary to store the counts count = {} # For each value in the data for observation in data : # An a key, value pair, with the observation being the key # and the value being +1 count [ observation ] = count . get ( observation , 0 ) + 1 Normalize The Count To Between 0 and 1 # Calculate the number of observations n = len ( data ) # Create a dictionary probability_mass_function = {} # For each unique value, for unique_value , count in count . items (): # Normalize the count by dividing by the length of data, add to the PMC dictionary probability_mass_function [ unique_value ] = count / n Visualize The PMF # Plot the probability mass function plt . bar ( list ( probability_mass_function . keys ()), probability_mass_function . values (), color = 'g' ) plt . show ()","tags":"Statistics","url":"http://chrisalbon.com/statistics/probability_mass_functions.html","loc":"http://chrisalbon.com/statistics/probability_mass_functions.html"},{"title":"Spearman's Rank Correlation","text":"Preliminaries import numpy as np import pandas as pd import scipy.stats Create Data # Create two lists of random values x = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] y = [ 2 , 1 , 2 , 4.5 , 7 , 6.5 , 6 , 9 , 9.5 ] Calculate Spearman's Rank Correlation Spearman's rank correlation is the Pearson's correlation coefficient of the ranked version of the variables. # Create a function that takes in x's and y's def spearmans_rank_correlation ( xs , ys ): # Calculate the rank of x's xranks = pd . Series ( xs ) . rank () # Caclulate the ranking of the y's yranks = pd . Series ( ys ) . rank () # Calculate Pearson's correlation coefficient on the ranked versions of the data return scipy . stats . pearsonr ( xranks , yranks ) # Run the function spearmans_rank_correlation ( x , y )[ 0 ] 0.90377360145618091 Calculate Spearman's Correlation Using SciPy # Just to check our results, here it Spearman's using Scipy scipy . stats . spearmanr ( x , y )[ 0 ] 0.90377360145618102","tags":"Statistics","url":"http://chrisalbon.com/statistics/spearmans_rank_correlation.html","loc":"http://chrisalbon.com/statistics/spearmans_rank_correlation.html"},{"title":"T-Tests","text":"Preliminaries from scipy import stats import numpy as np Create Data # Create a list of 20 observations drawn from a random distribution # with mean 1 and a standard deviation of 1.5 x = np . random . normal ( 1 , 1.5 , 20 ) # Create a list of 20 observations drawn from a random distribution # with mean 0 and a standard deviation of 1.5 y = np . random . normal ( 0 , 1.5 , 20 ) One Sample Two-Sided T-Test Imagine the one sample T-test and drawing a (normally shaped) hill centered at 1 and \"spread\" out with a standard deviation of 1.5 , then placing a flag at 0 and looking at where on the hill the flag is location. Is it near the top? Far away from the hill? If the flag is near the very bottom of the hill or farther, then the t-test p-score will be below 0.05 . # Run a t-test to test if the mean of x is statistically significantly different than 0 pvalue = stats . ttest_1samp ( x , 0 )[ 1 ] # View the p-value pvalue 0.00010976647757800537 Two Variable Unpaired Two-Sided T-Test With Equal Variances Imagine the one sample T-test and drawing two (normally shaped) hills centered at their means and their 'flattness' (individual spread) based on the standard deviation. The T-test looks at how much the two hills are overlapping. Are they basically on top of each other? Do just the bottoms of the hill just barely touch? If the tails of the hills are just barely overlapping or are not overlapping at all, the t-test p-score will be below 0.05. stats . ttest_ind ( x , y )[ 1 ] 0.00035082056802728071 Two Variable Unpaired Two-Sided T-Test With Unequal Variances stats . ttest_ind ( x , y , equal_var = False )[ 1 ] 0.00035089238660076095 Two Variable Paired Two-Sided T-Test Paired T-tests are used when we are taking repeated samples and want to take into account the fact that the two distributions we are testing are paired. stats . ttest_rel ( x , y )[ 1 ] 0.00034222792790150386","tags":"Statistics","url":"http://chrisalbon.com/statistics/t-tests.html","loc":"http://chrisalbon.com/statistics/t-tests.html"},{"title":"Variance And Standard Deviation","text":"Preliminary # Import data import math Create Data # Create list of values data = [ 3 , 2 , 3 , 4 , 2 , 3 , 5 , 2 , 2 , 33 , 3 , 5 , 2 , 2 , 5 , 6 , 62 , 2 , 2 , 3 , 6 , 6 , 2 , 23 , 3 , 2 , 3 ] Calculate Population Variance Variance is a measurement of the spread of a data's distribution. The higher the variance, the more \"spread out\" the data points are. Variance, commonly denoted as $S&#94;{2}$, is calculated like this: $$ \\text{Population Variance} = S_n&#94;{2} = \\frac{1}{n}\\sum_{i=1}&#94;{n}(x_i-\\bar{x})&#94;{2}$$ $$ \\text{Sample Variance} = S_{n-1}&#94;{2} = \\frac{1}{n-1}\\sum_{i=1}&#94;{n}(x_i-\\bar{x})&#94;{2}$$ Where $n$ is the number of observations, $\\bar{x}$ is the mean of the observations, and $x_i-\\bar{x}$ is an individual observation's from the mean of the data. Note that if we were estimating the variance of a population based on a sample from that population, we should use the second equation, replacing $n$ with $n-1$. # Calculate n n = len ( data ) # Calculate the mean mean = sum ( data ) / len ( data ) # Create a list of all deviations from the mean all_deviations_from_mean_squared = [] # For each observation in the data for observation in data : # Calculate the deviation from the mean deviation_from_mean = ( observation - mean ) # Square it deviation_from_mean_squared = deviation_from_mean ** 2 # Add the result to our list all_deviations_from_mean_squared . append ( deviation_from_mean_squared ) # Sum all the squared deviations in our list sum_of_deviations_from_mean_squared = sum ( all_deviations_from_mean_squared ) # Divide by n population_variance = sum_of_deviations_from_mean_squared / n # Show variance population_variance 160.78463648834017 Calculate Population Standard Deviation Standard deviation is just the square root of the variance. # Find the square root of the population variance population_standard_deviation = math . sqrt ( population_variance ) # Print the populaton standard deviation population_standard_deviation 12.68008818929664","tags":"Statistics","url":"http://chrisalbon.com/statistics/variance_and_standard_deviation.html","loc":"http://chrisalbon.com/statistics/variance_and_standard_deviation.html"},{"title":"What I Learned Tracking My Time At Techstars","text":"In the fall of 2015, Popily , a data exploration company I cofounded with two friends was offered a slot in the 2016 class of Techstars Cloud . Like most people in tech, I had heard about Techstars, but in truth I barely knew anything specific, particularly about the day-to-day of the program. Was Techstars a permanent hackathon fueled by Soylent and Adderall? Was it three months of guest speakers and sponsored happy hours? I watched Techstars's promotional videos , but the only impression I could glean was that all the founders worked 36 hours per day while having ample spare time to ride bikes around downtown Boulder. I also found some posts about people's experiences, but overall I was in the dark. So, when we joined Techstars Cloud in November, I did what I was trained to do: gather data. For three months, from November 2nd, 2015 to January 31st, 2016, I tracked how I spent every 15 minutes of every day and categorized each into one of seven activities: Non-Technical Work: Email, writing, diagramming, project management, PivotalTracker , etc. Technical Work: Coding, designing, data analysis, etc. Discussion: Team meetings, speaking events, meeting with mentors or investors, Techstars happy hours, etc. Sleep Travel: Driving or flying (when I couldn't do any work) Exercise: Running (which basically never happened) Personal: Time with family, cooking, hobbies, reading, housework etc. The full data is available on Github , however in this post I used Popily.com to explore the data and understand how I spent my time during Techstars and offer a few lessons learned along the way. I hope you find it useful. Every Day At Techstars Cloud Lesson 1: Techstars Is For Going All-In If there was one description of my time at Techstars it was that I worked, a lot -- during evenings, lunches, and holidays; in offices, cars, and AirBnBs; alone, with my team, and while holding my daughter. The combination of the environment and the looming Demo Day made the work all-consuming. I was responsible for the product, so it is unsurprising that the largest number of my hours was spent on technical tasks. In truth, even non-technical tasks were almost always related to product development: UX testing, QAing, or PivotalTracker. My laser focus on the product was sometimes at odds with the myriad of guest speakers and mentor meetings that, while interesting and useful, could suck me in and stall development. As a company we started using two strategies to keep the product moving forward. First, most of the meetings and events were handled by my cofounders, which left me time to focus to a greater degree on the product. Second, when we had a product update or launch goal, I spent weeks away from the Techstars offices so I could get the most work done on the product as possible. On average, during Techstars I worked 91.6 hours per week. I worked the most hours during the first week of the program, where I averaged 17.4 hours of work per day. This number will be unsurprising to anyone who has going though Techstars. The first three weeks of Techstars are called Mentor Madness and can essentially be described as a full day of back to back 20 minute meetings with every fancy and impressive founder you can imagine. You walk into a room, give a five minute pitch to anyone from the CEO of a boutique ad agency to a founder of a technology giant and hear their take about you, your company, your product, your market, whatever. After 20 minutes you walk into the next room and repeat the entire process again. Total Hours Spent On Tasks Mentor Madness was a grueling experience for all of us, not only because it took up so much of our energy (it takes a lot of focus to be engaged after seven almost-back-to-back meetings), but more importantly it made us have to defend, explain, and face so many fundamental assumptions we implicitly and explicitly made about our company, product, and strategy. In one meeting a mentor would argue that the go-to-market strategy is flawed and 20 minutes later the next mentor is arguing that we need to change our company's name. Those weeks were brutal. However, it was also probably one of the most important few weeks of our company. Because at the end of all those meetings, our day would just begin; from the last meeting to late at night my cofounders and I would consider, discuss, and debate a hundred points that were brought in the day. Why did the mentor hate the pricing model? Was she right or just old-fashioned? Should we change our name? Who is our customer? All the meetings and all the challenging questions forced us to discuss things which might otherwise be assumed or left unsaid. For those first few weeks I would get up early, spend an hour emailing, discuss some point or another about strategy or product, take meetings with mentors, discuss more over lunch, take more meetings, discuss over dinner and into the evening, shower, sleep, and start the whole thing over again. Two days during Mentor Madness contained a massive 13 and 14 hours of discussions. Those conversations alone made Techstars worthwhile. Unsurprisingly, the week with the least work was Christmas when I was traveling with my family and would only be able to sneak in a few hours of work per day after everyone fell asleep. Lesson 2: The Start Of Techstars Is About Strategy, The End Is About Execution Our company was not alone is spending the first weeks of the program focusing on planning and strategy. I think most of the companies in our class spent the first few weeks either going back to the drawing board or mapping out a plan ahead. However, in December, the mid-point of the program there was a general shift from planning to execution. Looking at hours spent on between technical and non-technical tasks, Techstars could be divided into two periods, before Christmas and after Christmas. In the first half of the program it felt like I spent every moment I wasn't in meetings writing, updating, or managing user stories. We had a few contract developers and I agonized over the ordering of user stories in PivotalTracker as to maximize the value we would get from every hour of their time. However, during the second half of the problem the planning took a back seat to building. Hours Spent On Technical And Non-Technical Work During Techstars Between Christmas and our launch on January 19th was probably my happiest in the program, because it was a wonderful coding crunchtime. I would get up, select the next user story in the queue, complete it, QA, and repeat. It was -- frankly -- just fun. All the discussions around marketing, fundraising, or ten thousand other things were put aside (i.e. handled by my cofounders), leaving me free to build. This switch from planning to building is beautifully seen in the data: in the first half of the program I spent almost all my time on planning and strategy and in the second half I spent almost all my time building. While non-technical and technical work peaked around the beginning and the end of the program, meetings, networking, and discussions continued throughout. As I said previously, the Mentor Madness of the first three weeks took up a massive amount of time at the start of the program, but after those weeks my cofounders and I actively tried to give each other the space to get stuff done. Our primary tactic was to have regular short(ish) \"State Of The Union calls almost every day, leaving the major discussions to multi-day planning sessions every few weeks. These discussion can be easily seen as peaks in the chart below. This system might not work well for everyone, but it gave us the right balance of time to work out problems and time to get shit done. Hours Spent At Discussions, Meetings, or Networking During Techstars Did all those hours matter? Completely and absolutely. Over the course of the program the product went from a tricycle to a Harley Davidson. There is more to do, but the leaps and bounds made during Techstars has been incredible. Lesson 3: Sleep Is Important! Kinda. As soon as I started tracking my time, I knew the first question everyone was going to ask: \"how much did you sleep?\" and because of the power of data I can give you a real answer: 5.5 hours of sleep per night. I will admit I was pretty pleased when I saw that number because while everyone at Techstars works their asses off 4.5 to 6.5 hours of sleep was my normal amount prior to Techstars. That said, on those few nights I received far less sleep than I needed my productivity suffered the next day. Looking at the data below, it is also notable to see that the amount of sleep I got every night remained relatively stable throughout the program. The only two deviations are the two times I made the twelve hour drive from my house to the Techstars offices overnight and the moderate dip in sleep I got in early January when I was enjoying my late night coding sessions. Hours Of Sleep During Techstars Lesson 4: There Is No Work-Life Balance I would love to say there was great work-life balance, but in reality was not any sort of work-life balance. There was regular fun events (e.g. happy hours, runs, etc.) during the program, but that isn't work-life balance, that is fun work events. The truth is that all the benefits of Techstars, the rapid advance of our product, the boost in users, the improved strategy, and everything else came at a price -- that for three months you lived your work and everything else from family to kids to friends took a backseat. During Techstars my average daily \"free time\" (i.e. not working or sleeping) was 2.7 hours. That is 2.7 hours per day for everything, from Christmas mornings to birthday parties to doctors appointments to cooking dinner. I am not complaining. I like working, and I am in a place in my life that I can let work take over for a while, but make no mistake: it did take over. I'd argue that to really get value from Techstars, it probably has to take over. As a more concrete example to the point above, of the three major holidays that took place during Techstars Cloud two of them were just a regular work day for me. Hours Spent On Activities On Thanksgiving, Xmas, and NYE Final Thoughts On Techstars It has been incredibly valuable to every aspect of Popily , from product to strategy. The advice been useful, the connections have been valuable, and Demo Day (February 11th) will be awesome. However, at the end of the day none of that was why I enjoyed Techstars. The real reason is that for three months I worked with people I enjoy for long hours on hard problems -- and everything else took a back seat. That might not appeal to many people, but it appeals to me. In truth, not once during Techstars did I feel like I was \"working\". Rather, I was with my friends, trying to do something genuinely hard -- and I can't ask for more than that.","tags":"Blog","url":"http://chrisalbon.com/blog/what-i-learned-tracking-my-time-at-techstars.html","loc":"http://chrisalbon.com/blog/what-i-learned-tracking-my-time-at-techstars.html"},{"title":"Bubble Sort","text":"Want to learn more? Check out Data Structures and Algorithms in Python Create A Sequence unsorted_list = [ 8 , 5 , 3 , 6 , 2 , 1 , 9 , 4 , 7 ] unsorted_list [8, 5, 3, 6, 2, 1, 9, 4, 7] Create A Bubble Sort Function # Define a function that takes an unsorted sequence def bubble_sort ( unsorted_list ): # Create a new list containing the values from the inputed list sequence = unsorted_list [:] # For each value of the sequence (epochs), for i , _ in enumerate ( sequence ): # For each value of the sequence, for i , _ in enumerate ( sequence ): # Try try : # If a value is greater than the value that follows it if sequence [ i ] > sequence [ i + 1 ]: # Swap the values in the sequence sequence [ i ], sequence [ i + 1 ] = sequence [ i + 1 ], sequence [ i ] # If you raise an index error, you are at the end of the sequence, except IndexError : # So ignore the error and continue with iteration continue # Print the sequence afer each epoch print ( sequence ) # Run the function bubble_sort ( unsorted_list ) [5, 3, 6, 2, 1, 8, 4, 7, 9] [3, 5, 2, 1, 6, 4, 7, 8, 9] [3, 2, 1, 5, 4, 6, 7, 8, 9] [2, 1, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9] [1, 2, 3, 4, 5, 6, 7, 8, 9]","tags":"Algorithms","url":"http://chrisalbon.com/algorithms/bubble_sort.html","loc":"http://chrisalbon.com/algorithms/bubble_sort.html"},{"title":"Insertion Sort","text":"Create A Sequence alist = [ 8 , 5 , 3 , 6 , 2 , 1 , 9 , 4 , 7 ] alist [8, 5, 3, 6, 2, 1, 9, 4, 7] Create A Selection Sort Algorithm # Define an function that takes a list def insertion_sort ( alist ): # Create a sequence from the argument list sequence = alist [:] # Get the length of the list n = len ( sequence ) # For 1 through the length for the sequence: for i in range ( 1 , n ): # save the value of the card value = sequence [ i ] # save the current position of the card position = i # while the card is not the first card and is smaller than the card to it's left: while position > 0 and value < sequence [ position - 1 ]: # the card overwrites the card to the left sequence [ position ] = sequence [ position - 1 ] # And we move on to the next position position -= 1 # When we have found the right position (meaning the while loop is false) # put the card in its correct spot in the deck sequence [ position ] = value # View the deck so far print ( sequence ) # Run the sort insertion_sort ( alist ) [5, 8, 3, 6, 2, 1, 9, 4, 7] [3, 5, 8, 6, 2, 1, 9, 4, 7] [3, 5, 6, 8, 2, 1, 9, 4, 7] [2, 3, 5, 6, 8, 1, 9, 4, 7] [1, 2, 3, 5, 6, 8, 9, 4, 7] [1, 2, 3, 5, 6, 8, 9, 4, 7] [1, 2, 3, 4, 5, 6, 8, 9, 7] [1, 2, 3, 4, 5, 6, 7, 8, 9]","tags":"Algorithms","url":"http://chrisalbon.com/algorithms/insertion_sort.html","loc":"http://chrisalbon.com/algorithms/insertion_sort.html"},{"title":"Selection Sort","text":"This might not be the most efficient implementation of the selection sort algorithm. However, it is the one that closely matches how the algorithm is explained: Pick up the first card (if this was a deck of cards). Compare the card in your hand to each other card in turn If a smaller card is found, swap the cards (so the smaller card is now in your hand). When you get to the last card, put the card in your hand into a separate pile. Repeat steps 1-4 until there are no more cards in the original deck Create A Sequence alist = [ 8 , 5 , 3 , 6 , 2 , 1 , 9 , 4 , 7 ] alist [8, 5, 3, 6, 2, 1, 9, 4, 7] Create A Selection Sort Algorithm # Define a function that takes an unsorted list, def selection_sort ( alist ): # Create a new list containing the values from the inputed list unsorted_sequence = alist [:] # Create a list where we will place the sorted values sorted_sequence = [] # While there are still values in the unsorted sequence: while len ( unsorted_sequence ) > 0 : # For each value in the unsorted sequence, for i , _ in enumerate ( unsorted_sequence ): # Assume it is the smallest value smallest_value_index = i # Compare it which each other value in the unsorted list for i , _ in enumerate ( unsorted_sequence ): # If a smaller value is found if unsorted_sequence [ smallest_value_index ] > unsorted_sequence [ i ]: # Swap the two values so the new value is the one we think is the smallest smallest_value_index , i = i , smallest_value_index # When we get to the end of the sequence, remove the smallest valued card smallest_value = unsorted_sequence . pop ( smallest_value_index ) # And add it to our sequence of sorted values sorted_sequence . append ( smallest_value ) # Print the sorted sequence print ( 'Sorted sequence so far:' , sorted_sequence ) # Run the selection sort selection_sort ( alist ) Sorted sequence so far: [1] Sorted sequence so far: [1, 2] Sorted sequence so far: [1, 2, 3] Sorted sequence so far: [1, 2, 3, 4] Sorted sequence so far: [1, 2, 3, 4, 5] Sorted sequence so far: [1, 2, 3, 4, 5, 6] Sorted sequence so far: [1, 2, 3, 4, 5, 6, 7] Sorted sequence so far: [1, 2, 3, 4, 5, 6, 7, 8] Sorted sequence so far: [1, 2, 3, 4, 5, 6, 7, 8, 9]","tags":"Algorithms","url":"http://chrisalbon.com/algorithms/selection_sort.html","loc":"http://chrisalbon.com/algorithms/selection_sort.html"},{"title":"any(), all(), max(), min(), sum()","text":"Interesting in learning more? Check out Fluent Python Create Data data = [ 34 , 54 , 50 , 20 , 20 ] any() # Return true if any element is True any ( data ) True all() # Return true if all elements are True all ( data ) True max() # Return max value max ( data ) 54 min() # Return the min value min ( data ) 20 sum() # Return the total value sum ( data ) 178","tags":"Python","url":"http://chrisalbon.com/python/any_all_max_min_sum.html","loc":"http://chrisalbon.com/python/any_all_max_min_sum.html"},{"title":"argmin and argmax","text":"argmin and argmax are the inputs, x 's, to a function, f , that creates the smallest and largest outputs, f(x) . Preliminaries import numpy as np import pandas as pd np . random . seed ( 1 ) Define A Function, f(x) # Define a function that, def f ( x ): # Outputs x multiplied by a random number drawn from a normal distribution return x * np . random . normal ( size = 1 )[ 0 ] Create Some Values Of x # Create some values of x xs = [ 1 , 2 , 3 , 4 , 5 , 6 ] Find The Argmin Of f(x) #Define argmin that def argmin ( f , xs ): # Applies f on all the x's data = [ f ( x ) for x in xs ] # Finds index of the smallest output of f(x) index_of_min = data . index ( min ( data )) # Returns the x that produced that output return xs [ index_of_min ] # Run the argmin function argmin ( f , xs ) 6 Check Our Results print ( 'x' , '|' , 'f(x)' ) print ( '--------------' ) for x in xs : print ( x , '|' , f ( x )) x | f(x) -------------- 1 | 1.74481176422 2 | -1.52241380179 3 | 0.957117288171 4 | -0.99748150191 5 | 7.31053968522 6 | -12.360844257","tags":"Mathematics","url":"http://chrisalbon.com/mathematics/argmin_and_argmax.html","loc":"http://chrisalbon.com/mathematics/argmin_and_argmax.html"},{"title":"Concurrent Processing","text":"Interesting in learning more? Check out Fluent Python Preliminaries from concurrent import futures Create Data data = range ( 100 ) Create Function # Create some function that takes a value def some_function ( value ): # And outputs it raised to its own power return value ** value Run The Function On The Data Concurrently # With a pool of workers with futures . ProcessPoolExecutor () as executor : # Map the function to the data result = executor . map ( some_function , data ) View Results # List the first 5 outputs list ( result )[ 0 : 5 ] [1, 1, 4, 27, 256]","tags":"Python","url":"http://chrisalbon.com/python/concurrent_processing.html","loc":"http://chrisalbon.com/python/concurrent_processing.html"},{"title":"Create Counts Of Items","text":"Interesting in learning more? Check out Fluent Python Preliminaries from collections import Counter Create A Counter # Create a counter of the fruits eaten today fruit_eaten = Counter ([ 'Apple' , 'Apple' , 'Apple' , 'Banana' , 'Pear' , 'Pineapple' ]) # View counter fruit_eaten Counter({'Apple': 3, 'Banana': 1, 'Pear': 1, 'Pineapple': 1}) Update The Count For An Element # Update the count for 'Pineapple' (because you just ate an pineapple) fruit_eaten . update ([ 'Pineapple' ]) # View the counter fruit_eaten Counter({'Apple': 3, 'Banana': 1, 'Pear': 1, 'Pineapple': 2}) View The Items With The Highest Counts # View the items with the top 3 counts fruit_eaten . most_common ( 3 ) [('Apple', 3), ('Pineapple', 2), ('Banana', 1)]","tags":"Python","url":"http://chrisalbon.com/python/creating_counts_of_items.html","loc":"http://chrisalbon.com/python/creating_counts_of_items.html"},{"title":"Function Annotation Examples","text":"Interesting in learning more? Check out Fluent Python Create A Function With Annotations ''' Create a function. The argument 'text' is the string to print with the default value 'default string' and the argument The argument 'n' is an integer of times to print with the default value of 1. The function should return a string. ''' def print_text ( text : 'string to print' = 'default string' , n : 'integer, times to print' = 1 ) -> str : return text * n Run The Function # Run the function with arguments print_text ( 'string' , 4 ) 'stringstringstringstring' # Run the function with default arguments print_text () 'default string'","tags":"Python","url":"http://chrisalbon.com/python/function_annotations_examples.html","loc":"http://chrisalbon.com/python/function_annotations_examples.html"},{"title":"Functions Vs. Generators","text":"Interesting in learning more? Check out Fluent Python Create A Function # Create a function that def function ( names ): # For each name in a list of names for name in names : # Returns the name return name # Create a variable of that function students = function ([ 'Abe' , 'Bob' , 'Christina' , 'Derek' , 'Eleanor' ]) # Run the function students 'Abe' Now we have a problem, we were only returned the name of the first student. Why? Because the function only ran the for name in names iteration once! Create A Generator A generator is a function, but instead of returning the return , instead returns an iterator. The generator below is exactly the same as the function above except I have replaced return with yield (which defines whether a function with a regular function or a generator function). # Create a generator that def generator ( names ): # For each name in a list of names for name in names : # Yields a generator object yield name # Same as above, create a variable for the generator students = generator ([ 'Abe' , 'Bob' , 'Christina' , 'Derek' , 'Eleanor' ]) Everything has been the same so far, but now things get interesting. Above when we ran students when it was a function, it returned one name. However, now that students refers to a generator, it yields a generator object of names! # Run the generator students <generator object generator at 0x104837a40> What can we do this a generator object? A lot! As a generator students will can each student in the list of students: # Return the next student next ( students ) 'Abe' # Return the next student next ( students ) 'Bob' # Return the next student next ( students ) 'Christina' It is interesting to note that if we use list(students) we can see all the students still remaining in the generator object's iteration: # List all remaining students in the generator list ( students ) ['Derek', 'Eleanor']","tags":"Python","url":"http://chrisalbon.com/python/functions_vs_generators.html","loc":"http://chrisalbon.com/python/functions_vs_generators.html"},{"title":"Generator Expressions","text":"Interesting in learning more? Check out Fluent Python # Create a list of students students = [ 'Abe' , 'Bob' , 'Christina' , 'Derek' , 'Eleanor' ] # Create a generator expression that yields lower-case versions of the student's names lowercase_names = ( student . lower () for student in students ) # View the generator object lowercase_names <generator object <genexpr> at 0x104837518> # Get the next name lower-cased next ( lowercase_names ) 'abe' # Get the next name lower-cased next ( lowercase_names ) 'bob' # Get the remaining names lower-cased list ( lowercase_names ) ['christina', 'derek', 'eleanor']","tags":"Python","url":"http://chrisalbon.com/python/generator_expressions.html","loc":"http://chrisalbon.com/python/generator_expressions.html"},{"title":"How To Use Default Dicts","text":"Interesting in learning more? Check out Fluent Python Preliminaries import collections Create A DefaultDict Default Dicts work just like regular dictionaries, except a key is called that doesn't have a value, a default value (note: value, not key) is supplied. # Create a defaultdict with the default value of 0 (int's default value is 0) arrests = collections . defaultdict ( int ) Add A New Key With A Value # Add an entry of a person with 10 arrests arrests [ 'Sarah Miller' ] = 10 # View dictionary arrests defaultdict(int, {'Sarah Miller': 10}) Add A New Key Without A Value # Add an entry of a person with no value for arrests, # thus the default value is used arrests [ 'Bill James' ] 0 # View dictionary arrests defaultdict(int, {'Bill James': 0, 'Sarah Miller': 10})","tags":"Python","url":"http://chrisalbon.com/python/how_to_use_default_dicts.html","loc":"http://chrisalbon.com/python/how_to_use_default_dicts.html"},{"title":"Mocking Functions","text":"Interesting in learning more? Here are some good books on unit testing in Python: Python Testing: Beginner's Guide and Python Testing Cookbook . Preliminaries import unittest import mock from math import exp The Scenario Imagine we have a function that takes in some external API or database and we want to test that function, but with fake (or mocked) inputs. The Python mock library lets us do that. For this tutorial pretend that math.exp is some expensive operation (e.g. database query, API call, etc) that costs \\$10,000 every time we use it. To test it without paying \\$10,000, we can create mock_function which imitates the behavior of math.exp and allows us to test it. Create The Mock Function # Create a function, def mock_function ( x ): # That returns a string. return 'This is not exp, but rather mock_function.' Create A Unit Test # Create a test case, class TestRandom ( unittest . TestCase ): # where math.exp (__main__.exp is because we imported the exp module from math) # math.exp is mocked (replaced) by mock_function, @mock.patch ( '__main__.exp' , side_effect = mock_function ) # now create a unit test that would only be true IF the exp(4) was being mocked # (so we can prove that math.exp is actually being mocked) def test_math_exp ( self , mock_function ): # assert that math.exp(4) is actually a string, which would only be the case # if math.exp was being mocked by mock_function assert exp ( 4 ) == 'This is not exp, but rather mock_function.' Run Unit Test unittest . main ( argv = [ 'ignored' , '-v' ], exit = False ) test_math_exp (__main__.TestRandom) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK <unittest.main.TestProgram at 0x104945358>","tags":"Python","url":"http://chrisalbon.com/python/mocking_functions.html","loc":"http://chrisalbon.com/python/mocking_functions.html"},{"title":"Parallel Processing","text":"This tutorial is inspired by Chris Kiehl's great post on multiprocessing . Preliminaries from multiprocessing import Pool from multiprocessing.dummy import Pool as ThreadPool Create Some Data # Create a list of some data data = range ( 29999 ) Create An Operation To Execute On The Data # Create a function that takes a data point def some_function ( datum ): # and returns the datum raised to the power of itself return datum ** datum Traditional Approach %% time # Create an empty for the results results = [] # For each value in the data for datum in data : # Append the output of the function when applied to that datum results . append ( some_function ( datum )) CPU times: user 2min 2s, sys: 1.7 s, total: 2min 4s Wall time: 2min 8s Parallelism Approach # Create a pool of workers equaling cores on the machine pool = ThreadPool () %% time # Apply (map) some_function to the data using the pool of workers results = pool . map ( some_function , data ) # Close the pool pool . close () # Combine the results of the workers pool . join () CPU times: user 1min 56s, sys: 1.59 s, total: 1min 57s Wall time: 1min 57s","tags":"Python","url":"http://chrisalbon.com/python/parallel_processing.html","loc":"http://chrisalbon.com/python/parallel_processing.html"},{"title":"Partial Function Applications","text":"Partial function application allows us to create \"functions\" from other functions with pre-filled arguments. This can be very useful when we want to pipe the output of one function into a function requiring two functions. Preliminaries from functools import partial Create A Function def multiply ( x , y ): return x * y Create A Function With Y Pre-Filled double = partial ( multiply , y = 2 ) Run The Partial Function double ( 3 ) 6","tags":"Python","url":"http://chrisalbon.com/python/partial_functions_applications.html","loc":"http://chrisalbon.com/python/partial_functions_applications.html"},{"title":"Queues And Stacks","text":"Interesting in learning more? Check out Fluent Python Preliminaries from collections import deque Make A Queue # Option 1: Make a queue queue = deque ( range ( 10 )) # Option 2: Make a queue that, if full, discards any item at the # opposite end to where you added an item. queue = deque ( range ( 10 ), maxlen = 10 ) Manipulate Queue # Append an item to the right queue . append ( 'A' ) # View queue queue deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 'A']) # Append an item to the left queue . appendleft ( 'A' ) # View queue queue deque(['A', 1, 2, 3, 4, 5, 6, 7, 8, 9]) # Count occurances of item queue . count ( 'A' ) # View queue queue deque(['A', 1, 2, 3, 4, 5, 6, 7, 8, 9]) # Remove and return right-most item queue . pop () # View queue queue deque(['A', 1, 2, 3, 4, 5, 6, 7, 8]) # Remove and return left-most item queue . popleft () # View queue queue deque([1, 2, 3, 4, 5, 6, 7, 8]) # Insert item to the right of an item queue . insert ( 2 , 'A' ) # View queue queue deque([1, 2, 'A', 3, 4, 5, 6, 7, 8]) # Reverse the queue queue . reverse () # View queue queue deque([8, 7, 6, 5, 4, 3, 'A', 2, 1]) # Delete all items queue . clear () # View queue queue deque([])","tags":"Python","url":"http://chrisalbon.com/python/queues_and_stacks.html","loc":"http://chrisalbon.com/python/queues_and_stacks.html"},{"title":"repr vs. str","text":"Interesting in learning more? Check out Fluent Python Preliminaries import datetime Create A Simple Object class Regiment ( object ): def __init__ ( self , date = datetime . datetime . now ()): self . date = date def __repr__ ( self ): return date def __str__ ( self ): return str ( date ) __repr__ is for the developer. It is string representation of the object and the code needed to reproduce the object. __str__ is the output for the end user. It prints what the user wants to see.","tags":"Python","url":"http://chrisalbon.com/python/repr_vs_str.html","loc":"http://chrisalbon.com/python/repr_vs_str.html"},{"title":"Select Random Item From A Lists","text":"Interesting in learning more? Check out Fluent Python Preliminaries from random import choice Create List # Make a list of crew members crew_members = [ 'Steve' , 'Stacy' , 'Miller' , 'Chris' , 'Bill' , 'Jack' ] Select Random Item From List # Choose a random crew member choice ( crew_members ) 'Stacy'","tags":"Python","url":"http://chrisalbon.com/python/select_random_item_from_list.html","loc":"http://chrisalbon.com/python/select_random_item_from_list.html"},{"title":"Simple Unit Test","text":"Interesting in learning more? Here are some good books on unit testing in Python: Python Testing: Beginner's Guide and Python Testing Cookbook . Preliminaries import unittest import sys Create Function To Be Tested def multiply ( x , y ): return x * y Create Test Note: It is standard practice to name a unit test test_ + <function being tested> . This naming standard allows for automated test using some libraries. # Create a test case class TestMultiply ( unittest . TestCase ): # Create the unit test def test_multiply_two_integers_together ( self ): # Test if 4 equals the output of multiply(2,2) self . assertEqual ( 4 , multiply ( 2 , 2 )) Run Test # Run the unit test (and don't shut down the Jupyter Notebook) unittest . main ( argv = [ 'ignored' , '-v' ], exit = False ) test_multiply_two_integers_together (__main__.TestMultiply) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.001s OK <unittest.main.TestProgram at 0x104919128>","tags":"Python","url":"http://chrisalbon.com/python/simple_unit_test.html","loc":"http://chrisalbon.com/python/simple_unit_test.html"},{"title":"Test Code Speed","text":"Interesting in learning more? Here are some good books on unit testing in Python: Python Testing: Beginner's Guide and Python Testing Cookbook . Preliminaries import cProfile Create A Slow Function def slow_function (): total = 0.0 for i , _ in enumerate ( range ( 10000 )): for j , _ in enumerate ( range ( 1 , 10000 )): total += ( i * j ) return total Test The Speed Of The Function cProfile . run ( 'slow_function()' , sort = 'time' ) 4 function calls in 13.291 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 13.291 13.291 13.291 13.291 <ipython-input-2-64fc1cd43878>:1(slow_function) 1 0.000 0.000 13.291 13.291 {built-in method builtins.exec} 1 0.000 0.000 13.291 13.291 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} How To Read cProfile's Output ncalls: Number of calls to the function. tottime: Total time. percall: Time per call. cumtime: Total time in function and sub-functions. percall: Time to call. :lineno(function): Name of the operation. Alternative In Jupyter Notebook: %% timeit slow_function () 1 loop, best of 3: 12.9 s per loop","tags":"Python","url":"http://chrisalbon.com/python/test_code_speed.html","loc":"http://chrisalbon.com/python/test_code_speed.html"},{"title":"Test If Output Is Close To A Value","text":"Interesting in learning more? Here are some good books on unit testing in Python: Python Testing: Beginner's Guide and Python Testing Cookbook . Preliminaries import unittest import sys Create Function To Be Tested def add ( x , y ): return x + y Create Test # Create a test case class TestAdd ( unittest . TestCase ): # Create the unit test def test_add_two_floats_roughly_equals_11 ( self ): # Test if add(4.48293848, 6.5023845) return roughly (to 1 place) 11 (actual product: 10.98532298) self . assertAlmostEqual ( 11 , add ( 4.48293848 , 6.5023845 ), places = 1 ) Run Test # Run the unit test (and don't shut down the Jupyter Notebook) unittest . main ( argv = [ 'ignored' , '-v' ], exit = False ) test_add_two_floats_roughly_equals_11 (__main__.TestAdd) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.001s OK <unittest.main.TestProgram at 0x1049191d0>","tags":"Python","url":"http://chrisalbon.com/python/test_if_an_output_is_close_to_a_value.html","loc":"http://chrisalbon.com/python/test_if_an_output_is_close_to_a_value.html"},{"title":"Testable Documentation","text":"Interesting in learning more? Here are some good books on unit testing in Python: Python Testing: Beginner's Guide and Python Testing Cookbook . Preliminaries import doctest Create A Function To Test Note that our test cases are inside the function's documentation. Each test case is marked by a >>> and the expect output is the line below. def summation ( a , b ): \"\"\" Takes two inputs and outputs their sum. Tests: >>> summation(5, 4) 9 >>> summation(4, 3) 7 >>> summation('foo','bar') 'foobar' >>> summation(3,'d') Traceback (most recent call last): ... TypeError: unsupported operand type(s) for +: 'int' and 'str' \"\"\" return a + b Notice that in the last test, we are making sure the function outputs the correct error. Test Function doctest . testmod ( verbose = True ) Trying : summation ( 5 , 4 ) Expecting : 9 ok Trying : summation ( 4 , 3 ) Expecting : 7 ok Trying : summation ( 'foo' , 'bar' ) Expecting : 'foobar' ok Trying : summation ( 3 , 'd' ) Expecting : Traceback ( most recent call last ): ... TypeError : unsupported operand type ( s ) for +: 'int' and 'str' ok 1 items had no tests : __main__ 1 items passed all tests : 4 tests in __main__ . summation 4 tests in 2 items . 4 passed and 0 failed . Test passed . TestResults ( failed = 0 , attempted = 4 )","tags":"Python","url":"http://chrisalbon.com/python/testable_documentation.html","loc":"http://chrisalbon.com/python/testable_documentation.html"},{"title":"Unpacking A Tuple","text":"Interesting in learning more? Check out Fluent Python Create List Of Tuples # Create a list of tuples where the first and second element of each # super is the first last names, respectively soldiers = [( 'Steve' , 'Miller' ), ( 'Stacy' , 'Markov' ), ( 'Sonya' , 'Matthews' ), ( 'Sally' , 'Mako' )] Unpack Tuples # For the second element for each tuple in soldiers, for _ , last_name in soldiers : # print the second element print ( last_name ) Miller Markov Matthews Mako","tags":"Python","url":"http://chrisalbon.com/python/unpacking_a_tuple.html","loc":"http://chrisalbon.com/python/unpacking_a_tuple.html"},{"title":"Unpacking Function Arguments","text":"Interesting in learning more? Check out Fluent Python Create Argument Objects # Create a dictionary of arguments argument_dict = { 'a' : 'Alpha' , 'b' : 'Bravo' } # Create a list of arguments argument_list = [ 'Alpha' , 'Bravo' ] Create A Simple Function # Create a function that takes two inputs def simple_function ( a , b ): # and prints them combined return a + b Run the Function With Unpacked Arguments # Run the function with the unpacked argument dictionary simple_function ( ** argument_dict ) 'AlphaBravo' # Run the function with the unpacked argument list simple_function ( * argument_list ) 'AlphaBravo'","tags":"Python","url":"http://chrisalbon.com/python/unpacking_function_arguments.html","loc":"http://chrisalbon.com/python/unpacking_function_arguments.html"},{"title":"Using Named Tuples To Store Data","text":"Interesting in learning more? Check out Fluent Python Preliminaries from collections import namedtuple Create A Named Tuple Vehicle = namedtuple ( 'Vehicle' , 'make model wheels manual' ) Create An Entry forrester = Vehicle ( 'Forrester' , 'Subaru' , 4 , True ) View The Data In Entry forrester . model 'Subaru' forrester . wheels 4","tags":"Python","url":"http://chrisalbon.com/python/using_named_tuples_to_store_data.html","loc":"http://chrisalbon.com/python/using_named_tuples_to_store_data.html"}]}